{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "26d9f234-58de-4fc9-9f4e-443ea8265844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:28.796742400Z",
     "start_time": "2023-11-12T16:12:28.038868100Z"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea0ba8bc12eaef6f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:29.455918500Z",
     "start_time": "2023-11-12T16:12:28.052867900Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from catboost import CatBoostRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850e2dfc3bb16aa",
   "metadata": {},
   "source": [
    "Setting rules for libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8081b47cf627a139",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:31.863234300Z",
     "start_time": "2023-11-12T16:12:28.069869300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Warnings\n",
    "\n",
    "# Pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f768895a70a9bd57",
   "metadata": {},
   "source": [
    "# Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cf20b5c6b0ced7a4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:32.045826700Z",
     "start_time": "2023-11-12T16:12:28.089907900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Features to remove\n",
    "DATE_CALC: str = \"date_calc\"\n",
    "DATE_FORECAST: str = \"date_forecast\"\n",
    "ABSOLUTE_HUMIDITY: str = 'absolute_humidity_2m:gm3'\n",
    "AIR_DENSITY: str = 'air_density_2m:kgm3'\n",
    "CEILING_HEIGHT: str = 'ceiling_height_agl:m'\n",
    "CLEAR_SKY_ENERGY: str = 'clear_sky_energy_1h:J'\n",
    "CLEAR_SKY_RAD: str = 'clear_sky_rad:W'\n",
    "CLOUD_BASE: str = 'cloud_base_agl:m'\n",
    "DEW_OR_RIME: str = 'dew_or_rime:idx'\n",
    "DEW_POINT: str = 'dew_point_2m:K'\n",
    "DIFFUSE_RAD: str = 'diffuse_rad:W'\n",
    "DIFFUSE_RAD_1H: str = 'diffuse_rad_1h:J'\n",
    "DIRECT_RAD: str = 'direct_rad:W'\n",
    "DIRECT_RAD_1H: str = 'direct_rad_1h:J'\n",
    "EFFECTIVE_CLOUD_COVER: str = 'effective_cloud_cover:p'\n",
    "ELEVATION: str = 'elevation:m'\n",
    "FRESH_SNOW_12H: str = 'fresh_snow_12h:cm'\n",
    "FRESH_SNOW_1H: str = 'fresh_snow_1h:cm'\n",
    "FRESH_SNOW_24H: str = 'fresh_snow_24h:cm'\n",
    "FRESH_SNOW_3H: str = 'fresh_snow_3h:cm'\n",
    "FRESH_SNOW_6H: str = 'fresh_snow_6h:cm'\n",
    "IS_DAY: str = 'is_day:idx'\n",
    "IS_IN_SHADOW: str = 'is_in_shadow:idx'\n",
    "MSL_PRESSURE: str = 'msl_pressure:hPa'\n",
    "PRECIP_5MIN: str = 'precip_5min:mm'\n",
    "PRECIP_TYPE_5MIN: str = 'precip_type_5min:idx'\n",
    "PRESSURE_100M: str = 'pressure_100m:hPa'\n",
    "PRESSURE_50M: str = 'pressure_50m:hPa'\n",
    "PROB_RIME: str = 'prob_rime:p'\n",
    "RAIN_WATER: str = 'rain_water:kgm2'\n",
    "RELATIVE_HUMIDITY: str = 'relative_humidity_1000hPa:p'\n",
    "SFC_PRESSURE: str = 'sfc_pressure:hPa'\n",
    "SNOW_DENSITY: str = 'snow_density:kgm3'\n",
    "SNOW_DEPTH: str = 'snow_depth:cm'\n",
    "SNOW_DRIFT: str = 'snow_drift:idx'\n",
    "SNOW_MELT_10MIN: str = 'snow_melt_10min:mm'\n",
    "SNOW_WATER: str = 'snow_water:kgm2'\n",
    "SUN_AZIMUTH: str = 'sun_azimuth:d'\n",
    "SUN_ELEVATION: str = 'sun_elevation:d'\n",
    "SUPER_COOLED_LIQUID_WATER: str = 'super_cooled_liquid_water:kgm2'\n",
    "T_1000HPA: str = 't_1000hPa:K'\n",
    "TOTAL_CLOUD_COVER: str = 'total_cloud_cover:p'\n",
    "VISIBILITY: str = 'visibility:m'\n",
    "WIND_SPEED_10M: str = 'wind_speed_10m:ms'\n",
    "WIND_SPEED_U_10M: str = 'wind_speed_u_10m:ms'\n",
    "WIND_SPEED_V_10M: str = 'wind_speed_v_10m:ms'\n",
    "WIND_SPEED_W_1000HPA: str = 'wind_speed_w_1000hPa:ms'\n",
    "IS_ESTIMATED = 'is_estimated'\n",
    "\n",
    "TIME = 'time'\n",
    "PV_MEASUREMENT = 'pv_measurement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e5747184b6faafe",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:32.227296900Z",
     "start_time": "2023-11-12T16:12:28.100098Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def read_parquet(filepath: str) -> pd.DataFrame:\n",
    "    dataframe: pd.DataFrame = pd.read_parquet(filepath)\n",
    "    if DATE_CALC in dataframe.columns:\n",
    "        dataframe = dataframe.drop(columns=[DATE_CALC])\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def concat_observed_and_estimated_train_data(\n",
    "        X_observed: pd.DataFrame,\n",
    "        X_estimated: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    X_observed: pd.DataFrame = X_observed.copy()\n",
    "    X_estimated: pd.DataFrame = X_estimated.copy()\n",
    "\n",
    "    assert X_observed.shape[1] == X_estimated.shape[1]\n",
    "\n",
    "    concatenated_dataframe: pd.DataFrame = pd.concat(\n",
    "        objs=[X_observed, X_estimated],\n",
    "        axis=0\n",
    "    ).sort_values(\n",
    "        by=DATE_FORECAST,\n",
    "        ascending=True\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return concatenated_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2f38c501e227ecc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:32.524890200Z",
     "start_time": "2023-11-12T16:12:28.117097300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data transfer object for location data\n",
    "class LocationData:\n",
    "    combined_data: pd.DataFrame\n",
    "\n",
    "    def __init__(self, train_data, target_data, test_data) -> None:\n",
    "        self.train_data = train_data\n",
    "        self.target_data = target_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "        self.original_train_data = train_data.copy()\n",
    "        self.original_target_data = target_data.copy()\n",
    "        self.original_test_data = test_data.copy()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Train: {self.train_data.shape} [No. NaN: {self.train_data.isna().sum().sum()}], Target: {self.target_data.shape} [No. NaN: {self.target_data.isna().sum().sum()}], Test: {self.test_data.shape} [No. NaN: {self.test_data.isna().sum().sum()}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "60d61a1645354f7c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:34.189338800Z",
     "start_time": "2023-11-12T16:12:28.131147400Z"
    }
   },
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/A/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/A/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/A/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/A/X_test_estimated.parquet\")\n",
    "\n",
    "observed_train['is_estimated'] = 0\n",
    "estimated_train['is_estimated'] = 1\n",
    "test_data['is_estimated'] = 1\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d6374820ca84a630",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:34.247044100Z",
     "start_time": "2023-11-12T16:12:28.240129200Z"
    }
   },
   "outputs": [],
   "source": [
    "A = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "58333e566cc3633f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:34.249074300Z",
     "start_time": "2023-11-12T16:12:28.257142900Z"
    }
   },
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/B/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/B/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/B/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/B/X_test_estimated.parquet\")\n",
    "\n",
    "observed_train['is_estimated'] = 0\n",
    "estimated_train['is_estimated'] = 1\n",
    "test_data['is_estimated'] = 1\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")\n",
    "\n",
    "B = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "edbd3f707ed3d2f7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:34.249074300Z",
     "start_time": "2023-11-12T16:12:28.413920300Z"
    }
   },
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/C/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/C/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/C/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/C/X_test_estimated.parquet\")\n",
    "\n",
    "observed_train['is_estimated'] = 0\n",
    "estimated_train['is_estimated'] = 1\n",
    "test_data['is_estimated'] = 1\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a3ab86d84357bed",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:34.249074300Z",
     "start_time": "2023-11-12T16:12:28.554759100Z"
    }
   },
   "outputs": [],
   "source": [
    "C = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b3d3d8fec6811b18",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:34.250042Z",
     "start_time": "2023-11-12T16:12:28.588219600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Train: (136245, 47) [No. NaN: 168040], Target: (34085, 2) [No. NaN: 0], Test: (2880, 47) [No. NaN: 3971]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (134505, 47) [No. NaN: 158811], Target: (32848, 2) [No. NaN: 4], Test: (2880, 47) [No. NaN: 3912]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (134401, 47) [No. NaN: 157326], Target: (32155, 2) [No. NaN: 6060], Test: (2880, 47) [No. NaN: 4104]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0124801bff563",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Defining functions for preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f4e6db271eea0",
   "metadata": {},
   "source": [
    "Function to remove features and replace `NaN` values with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "926a2ddc44c56339",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:34.250042Z",
     "start_time": "2023-11-12T16:12:28.651317700Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_feature(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    dataframe = dataframe.drop(columns=[*features])\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def replace_nan_with_zero(dataframe: pd.DataFrame, feature: str) -> pd.DataFrame:\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe[[feature]] = dataframe[[feature]].fillna(0)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f994238640695aa",
   "metadata": {},
   "source": [
    "Function to resample data by categorizing them\n",
    "- Features to average\n",
    "- Features to pick a value\n",
    "- Features to change redefine `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "76f0e24c1ba0fe4b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:34.250042Z",
     "start_time": "2023-11-12T16:12:28.676316600Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_by_selection(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "    if reduced_dataframe.isna().sum().sum() > 0:\n",
    "        print(\"[ERROR] Sample by selection got dataframe with NaN values\")\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.last()\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_by_mean(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.mean()\n",
    "    \n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_by_sum(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.sum()\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_cloud_base(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    cloud_base_frame = dataframe[[CLOUD_BASE]]\n",
    "\n",
    "    # Filling NaN values with -2_000\n",
    "    filled_frame = cloud_base_frame.copy()\n",
    "    filled_frame = filled_frame.fillna(-100_000)\n",
    "\n",
    "    # Resampling to 1H\n",
    "    resampler = filled_frame.resample('H')\n",
    "    cloud_base_frame = resampler.mean()\n",
    "\n",
    "    # Replacing negative values with -666\n",
    "    cloud_bases = cloud_base_frame[CLOUD_BASE].to_numpy()\n",
    "    negative_indexes = np.where(cloud_bases < 0)[0]\n",
    "    cloud_bases[negative_indexes] = -666\n",
    "    cloud_base_frame[CLOUD_BASE] = cloud_bases\n",
    "\n",
    "    # Dropping NaN values that were introduced by resampling\n",
    "    reduced_dataframe = cloud_base_frame.copy()\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def merge_frames(*dataframes: pd.DataFrame) -> pd.DataFrame:\n",
    "    indexes = [df.index for df in dataframes]\n",
    "\n",
    "    # Finding intersecting indexes in list of indexes, the indexes are datetime\n",
    "    intersecting_indexes = list(set(indexes[0]).intersection(*indexes))\n",
    "    filtered_dataframes = [df.loc[intersecting_indexes] for df in dataframes]\n",
    "\n",
    "    return pd.concat(filtered_dataframes, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4830f59eace7ae",
   "metadata": {},
   "source": [
    "Functions to synchronze dates in weather data and target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "98926ee1a8a03451",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:34.250042Z",
     "start_time": "2023-11-12T16:12:28.699208400Z"
    }
   },
   "outputs": [],
   "source": [
    "def synchronize_timestamps(train_data: pd.DataFrame, target_data: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # IndexError if index in both train data and target data is DateTimeIndex\n",
    "    if not isinstance(train_data.index, pd.DatetimeIndex):\n",
    "        raise IndexError(\"Train data does not have DateTimeIndex\")\n",
    "\n",
    "    if not isinstance(target_data.index, pd.DatetimeIndex):\n",
    "        raise IndexError(\"Target data does not have DateTimeIndex\")\n",
    "\n",
    "    train_data.sort_index(inplace=True)\n",
    "    target_data.sort_index(inplace=True)\n",
    "\n",
    "    train_dates = train_data.index.values\n",
    "    target_dates = target_data.index.values\n",
    "\n",
    "    # Finding intersecting dates and filtering dataframes\n",
    "    intersecting_dates = np.intersect1d(train_dates, target_dates)\n",
    "    train_data = train_data.loc[intersecting_dates]\n",
    "    target_data = target_data.loc[intersecting_dates]\n",
    "\n",
    "    return train_data, target_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dateset contains many constant values for some features, any row with constant values is removed. If there is are at least 24 rows with constant values, the rows are removed.\n",
    "- -666 in cloud base should not be removed as this is added in earlier "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f0c01434829e1ae"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "472023231528654c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:34.250042Z",
     "start_time": "2023-11-12T16:12:28.711201500Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_constant_measurements(\n",
    "        train_data: pd.DataFrame,\n",
    "        target_data: pd.DataFrame,\n",
    "        THRESHOLD: int = 24\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    indexes = []\n",
    "    count = 0\n",
    "    previous_value = None\n",
    "    start_index = None\n",
    "\n",
    "    # Setting date indexes as columns and resetting index\n",
    "    train_data[DATE_FORECAST] = train_data.index.values\n",
    "    target_data[TIME] = target_data.index.values\n",
    "\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    target_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Assuming y is a DataFrame and has a column for pv_measurement values\n",
    "    pv_measurements = target_data[PV_MEASUREMENT].to_numpy()\n",
    "\n",
    "    # Identifying indexes of consecutive constant values\n",
    "    for index, value in enumerate(pv_measurements):\n",
    "        if value == previous_value:\n",
    "            count += 1\n",
    "            if count == 1:\n",
    "                start_index = index - 1\n",
    "            if count >= THRESHOLD and value > -0.1:\n",
    "                indexes.extend(range(start_index, index + 1))\n",
    "        else:\n",
    "            count = 0\n",
    "        previous_value = value\n",
    "\n",
    "    # Assuming y has a column for dates and X has a similar column to filter on\n",
    "    dates = target_data.loc[indexes, TIME].unique()\n",
    "\n",
    "    filtered_train_data = train_data[~train_data[DATE_FORECAST].isin(dates)]\n",
    "    filtered_target_data = target_data[~target_data[TIME].isin(dates)]\n",
    "\n",
    "    # Switching back to date as index\n",
    "    filtered_train_data = filtered_train_data.set_index(DATE_FORECAST)\n",
    "    filtered_target_data = filtered_target_data.set_index(TIME)\n",
    "\n",
    "    return filtered_train_data, filtered_target_data\n",
    "\n",
    "\n",
    "def drop_nan_values_from_target_data(train_data: pd.DataFrame, target_data: pd.DataFrame) -> tuple[\n",
    "    pd.DataFrame, pd.DataFrame]:\n",
    "    target_data.dropna(axis=0, how='any', inplace=True)\n",
    "    train_data, target_data = synchronize_timestamps(train_data, target_data)\n",
    "    return train_data, target_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948b908f0f59cef",
   "metadata": {},
   "source": [
    "## Adding lag features, this has to come here because of the resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c1317434ce4b07e5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:34.250042Z",
     "start_time": "2023-11-12T16:12:28.732214500Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_lag_features(dataframe: pd.DataFrame, features: list[str], lags: list[int]) -> tuple[pd.DataFrame, list[str]]:\n",
    "    dataframe = dataframe.copy()\n",
    "    lag_feature_names: list[str] = []\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            lag_feature_name = f\"{feature}_lag_{lag}\"\n",
    "            dataframe[lag_feature_name] = dataframe[feature].shift(lag)\n",
    "            lag_feature_names.append(lag_feature_name)\n",
    "\n",
    "    return dataframe, lag_feature_names\n",
    "\n",
    "\n",
    "def add_lead_features(dataframe: pd.DataFrame, features: list[str], lags: list[int]) -> tuple[pd.DataFrame, list[str]]:\n",
    "    dataframe = dataframe.copy()\n",
    "    lead_feature_names: list[str] = []\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            lead_feature_name = f\"{feature}_lead_{lag}\"\n",
    "            dataframe[lead_feature_name] = dataframe[feature].shift(-lag)\n",
    "            lead_feature_names.append(lead_feature_name)\n",
    "\n",
    "    return dataframe, lead_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4514f22c17b74b8e",
   "metadata": {},
   "source": [
    "Pipeline for preprocessing that is common for both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "92c2733c9a43a644",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:34.251042Z",
     "start_time": "2023-11-12T16:12:28.754212400Z"
    }
   },
   "outputs": [],
   "source": [
    "def common_pre_processing(dataframe: pd.DataFrame, is_cat_boost: bool) -> pd.DataFrame:\n",
    "    features_to_drop = [SNOW_DRIFT, CEILING_HEIGHT, SNOW_DRIFT]\n",
    "    \n",
    "    if is_cat_boost: \n",
    "        dataframe = remove_feature(dataframe, *features_to_drop)\n",
    "\n",
    "    dataframe = dataframe.set_index(DATE_FORECAST)\n",
    "    feature_to_lag_and_lead: list[str] = [DIRECT_RAD]\n",
    "\n",
    "    # Adding lag and lead features\n",
    "    lag_features = []\n",
    "    lead_features = []\n",
    "\n",
    "    if is_cat_boost:\n",
    "        dataframe, lag_features = add_lag_features(dataframe, feature_to_lag_and_lead, [1, 2, 3])\n",
    "        dataframe, lead_features = add_lead_features(dataframe, feature_to_lag_and_lead, [1, 2, 3])\n",
    "\n",
    "    # Resample data to hours\n",
    "    selection_features = [\n",
    "        CLEAR_SKY_ENERGY,  # Maybe move\n",
    "        DIRECT_RAD_1H,  # MAYBE MOVE\n",
    "        DIFFUSE_RAD_1H,  # Maybe move\n",
    "        FRESH_SNOW_1H,\n",
    "        FRESH_SNOW_3H,\n",
    "        FRESH_SNOW_6H,\n",
    "        FRESH_SNOW_12H,\n",
    "        FRESH_SNOW_24H,\n",
    "        DEW_OR_RIME,\n",
    "        IS_DAY,\n",
    "        IS_IN_SHADOW,\n",
    "        PRECIP_TYPE_5MIN,\n",
    "        ELEVATION,\n",
    "        IS_ESTIMATED,\n",
    "        *lag_features,\n",
    "        *lead_features,\n",
    "    ]\n",
    "\n",
    "    mean_features = [\n",
    "        ABSOLUTE_HUMIDITY,\n",
    "        CLEAR_SKY_RAD,\n",
    "        DIFFUSE_RAD,\n",
    "        DIRECT_RAD,\n",
    "        AIR_DENSITY,\n",
    "        DEW_POINT,\n",
    "        MSL_PRESSURE,\n",
    "        PRESSURE_100M,\n",
    "        PRESSURE_50M,\n",
    "        SFC_PRESSURE,\n",
    "        RELATIVE_HUMIDITY,\n",
    "        T_1000HPA,\n",
    "        SUN_AZIMUTH,\n",
    "        SUN_ELEVATION,\n",
    "        EFFECTIVE_CLOUD_COVER,\n",
    "        TOTAL_CLOUD_COVER,\n",
    "        VISIBILITY,\n",
    "        WIND_SPEED_10M,\n",
    "        WIND_SPEED_U_10M,\n",
    "        WIND_SPEED_V_10M,\n",
    "        WIND_SPEED_W_1000HPA,\n",
    "        SNOW_MELT_10MIN,\n",
    "        PROB_RIME,\n",
    "        SUPER_COOLED_LIQUID_WATER,\n",
    "        SNOW_DEPTH, \n",
    "    ]\n",
    "    \n",
    "    sum_features = [\n",
    "        SNOW_WATER,\n",
    "        PRECIP_5MIN,\n",
    "        RAIN_WATER,\n",
    "    ]\n",
    "    \n",
    "    if not is_cat_boost:\n",
    "        for feature in features_to_drop:\n",
    "            mean_features.append(feature)\n",
    "\n",
    "    selection_df = sample_by_selection(dataframe, *selection_features)\n",
    "    mean_df = sample_by_mean(dataframe, *mean_features)\n",
    "    sum_df = sample_by_sum(dataframe, *sum_features)\n",
    "    ceiling_height_df = sample_cloud_base(dataframe)\n",
    "    dataframe = merge_frames(selection_df, mean_df, ceiling_height_df, sum_df)\n",
    "\n",
    "    if is_cat_boost:\n",
    "        dataframe = dataframe.dropna(axis=0, how='any')\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db91786254d2a576",
   "metadata": {},
   "source": [
    "### Preprocessing pipelines for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9556d94345fac3ba",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:34.251042Z",
     "start_time": "2023-11-12T16:12:28.780214500Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process_train_data(\n",
    "        train_data: pd.DataFrame, \n",
    "        target_data: pd.DataFrame, \n",
    "        is_cat_boost: bool\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_data = common_pre_processing(dataframe=train_data, is_cat_boost=is_cat_boost)\n",
    "\n",
    "    # Setting time as index for target data\n",
    "    target_data[TIME] = pd.to_datetime(target_data[TIME])\n",
    "    target_data = target_data.set_index(TIME)\n",
    "\n",
    "    # Preprocessing that affects both train and target data\n",
    "    train_data, target_data = synchronize_timestamps(train_data=train_data, target_data=target_data)\n",
    "    train_data, target_data = remove_constant_measurements(train_data=train_data, target_data=target_data)\n",
    "    train_data, target_data = drop_nan_values_from_target_data(train_data=train_data, target_data=target_data)\n",
    "\n",
    "    return train_data, target_data\n",
    "\n",
    "\n",
    "def pre_process_test_data(test_data: pd.DataFrame, is_cat_boost: bool) -> pd.DataFrame:\n",
    "    if is_cat_boost:\n",
    "        test_data = common_pre_processing(dataframe=test_data, is_cat_boost=is_cat_boost)\n",
    "    else:\n",
    "        test_data = test_data.set_index(DATE_FORECAST)\n",
    "        test_data = test_data.groupby(test_data.index.floor('H')).mean()\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79beb4",
   "metadata": {},
   "source": [
    "# AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(\n",
    "    train_data=A.train_data,\n",
    "    target_data=A.target_data,\n",
    "    is_cat_boost=False\n",
    ")\n",
    "\n",
    "test_data = pre_process_test_data(test_data=A.test_data, is_cat_boost=False)\n",
    "A.train_data = train_data\n",
    "A.target_data = target_data\n",
    "A.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:34.944417Z",
     "start_time": "2023-11-12T16:12:28.795734700Z"
    }
   },
   "id": "6ad5e575852aadfb"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(\n",
    "    train_data=B.train_data,\n",
    "    target_data=B.target_data,\n",
    "    is_cat_boost=False\n",
    ")\n",
    "test_data = pre_process_test_data(test_data=B.test_data, is_cat_boost=False)\n",
    "B.train_data = train_data\n",
    "B.target_data = target_data\n",
    "B.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:34.978498400Z",
     "start_time": "2023-11-12T16:12:29.816897900Z"
    }
   },
   "id": "a4a11892d7383702"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(\n",
    "    train_data=C.train_data,\n",
    "    target_data=C.target_data,\n",
    "    is_cat_boost=False\n",
    ")\n",
    "test_data = pre_process_test_data(test_data=C.test_data, is_cat_boost=False)\n",
    "C.train_data = train_data\n",
    "C.target_data = target_data\n",
    "C.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:35.183545Z",
     "start_time": "2023-11-12T16:12:31.566376Z"
    }
   },
   "id": "d7936de9ca1f1338"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "Train: (34042, 46) [No. NaN: 7159], Target: (34042, 1) [No. NaN: 0], Test: (720, 46) [No. NaN: 978]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (25899, 46) [No. NaN: 5166], Target: (25899, 1) [No. NaN: 0], Test: (720, 46) [No. NaN: 965]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (21135, 46) [No. NaN: 5427], Target: (21135, 1) [No. NaN: 0], Test: (720, 46) [No. NaN: 1010]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:35.185526300Z",
     "start_time": "2023-11-12T16:12:32.966048300Z"
    }
   },
   "id": "53a3241e6b3792ef"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a20a9cca-6d35-4e5a-aba2-ef43b90af3e8",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:35.185526300Z",
     "start_time": "2023-11-12T16:12:32.996321900Z"
    }
   },
   "outputs": [],
   "source": [
    "MINUTES: int = 1\n",
    "time_limit = int(60 * MINUTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "33e3fd13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:35.186525100Z",
     "start_time": "2023-11-12T16:12:33.017321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merged files required by the AutoGluon model\n",
    "def merge_train_and_target(location: LocationData) -> pd.DataFrame:\n",
    "    train_data = location.train_data.copy()\n",
    "    target_data = location.target_data.copy()\n",
    "\n",
    "    return pd.merge(left=train_data, right=target_data, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "\n",
    "def prepare_data_for_ag(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    for col in dataframe.columns:\n",
    "        if dataframe[col].dtype == 'object':\n",
    "            dataframe[col] = dataframe[col].astype(str)\n",
    "        else:\n",
    "            dataframe[col] = pd.to_numeric(dataframe[col], errors='coerce')\n",
    "    \"\"\"\n",
    "\n",
    "    if pd.api.types.is_datetime64_any_dtype(dataframe.index):\n",
    "        dataframe = dataframe.sort_index()  # Sort by date if the index is a datetime\n",
    "        dataframe = dataframe.reset_index()  # Reset index\n",
    "\n",
    "    if DATE_FORECAST in dataframe.columns:\n",
    "        dataframe = dataframe.drop(columns=[DATE_FORECAST])\n",
    "\n",
    "    if TIME in dataframe.columns:\n",
    "        dataframe = dataframe.drop(columns=[TIME])\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cc33d2a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:35.186525100Z",
     "start_time": "2023-11-12T16:12:33.027323200Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_A = prepare_data_for_ag(merge_train_and_target(A))\n",
    "combined_B = prepare_data_for_ag(merge_train_and_target(B))\n",
    "combined_C = prepare_data_for_ag(merge_train_and_target(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "26d37c15-7870-48b9-8bf9-cb85db0caa68",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:35.186525100Z",
     "start_time": "2023-11-12T16:12:33.109124700Z"
    }
   },
   "outputs": [],
   "source": [
    "tuning_data_A = combined_A[combined_A['is_estimated'] == 1].tail(10100)\n",
    "tuning_data_B = combined_B[combined_B['is_estimated'] == 1].tail(950)\n",
    "tuning_data_C = combined_C[combined_C['is_estimated'] == 1].tail(870)\n",
    "\n",
    "combined_A = combined_A[(~combined_A.index.isin(tuning_data_A.index))]\n",
    "combined_B = combined_B[(~combined_B.index.isin(tuning_data_B.index))]\n",
    "combined_C = combined_C[(~combined_C.index.isin(tuning_data_C.index))]\n",
    "\n",
    "test_A = prepare_data_for_ag(A.test_data)\n",
    "test_B = prepare_data_for_ag(B.test_data)\n",
    "test_C = prepare_data_for_ag(C.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c187ace-8822-4a31-b9cb-474fb0a01c75",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-11-12T15:50:42.111814500Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor_A = TabularPredictor(\n",
    "    label='pv_measurement',\n",
    "    eval_metric=\"mean_absolute_error\"\n",
    ").fit(\n",
    "    train_data=combined_A,\n",
    "    tuning_data=tuning_data_A,\n",
    "    use_bag_holdout=True,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=2,\n",
    "    num_stack_levels=2,\n",
    "    time_limit=time_limit,\n",
    "    presets=\"best_quality\",\n",
    "    excluded_model_types=['KNN']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961a928-3931-465c-924a-d6030ca0a0f3",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-11-12T15:50:42.117813100Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor_B = TabularPredictor(\n",
    "    label='pv_measurement',\n",
    "    eval_metric=\"mean_absolute_error\"\n",
    ").fit(\n",
    "    train_data=combined_B,\n",
    "    tuning_data=tuning_data_B,\n",
    "    use_bag_holdout=True,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=2,\n",
    "    num_stack_levels=2,\n",
    "    time_limit=time_limit,\n",
    "    presets=\"best_quality\",\n",
    "    excluded_model_types=['KNN']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e18a77-3142-4682-82e2-d9fd6c53a42b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-11-12T15:50:42.121814100Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor_C = TabularPredictor(\n",
    "    label='pv_measurement',\n",
    "    eval_metric=\"mean_absolute_error\"\n",
    ").fit(\n",
    "    train_data=combined_C,\n",
    "    tuning_data=tuning_data_C,\n",
    "    use_bag_holdout=True,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=2,\n",
    "    num_stack_levels=2,\n",
    "    time_limit=time_limit,\n",
    "    presets=\"best_quality\",\n",
    "    excluded_model_types=['KNN']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915985ab",
   "metadata": {},
   "source": [
    "### AutoGluon predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c814a68",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T15:50:42.127812900Z"
    }
   },
   "outputs": [],
   "source": [
    "A_prediction = predictor_A.predict(test_A)\n",
    "B_prediction = predictor_B.predict(test_B)\n",
    "C_prediction = predictor_C.predict(test_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312351cc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T15:50:42.131813100Z"
    }
   },
   "outputs": [],
   "source": [
    "autogloun_predictions = np.concatenate([A_prediction, B_prediction, C_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CatBoost\n",
    "## Preprocessing\n",
    "Resetting data objects"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c481a8ca6631fc9"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "def reset_data_objects(location: LocationData) -> None:\n",
    "    location.train_data = location.original_train_data.copy()\n",
    "    location.target_data = location.original_target_data.copy()\n",
    "    location.test_data = location.original_test_data.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:48.946607400Z",
     "start_time": "2023-11-12T16:12:48.794429200Z"
    }
   },
   "id": "647ff8ee65e8503d"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "reset_data_objects(A)\n",
    "reset_data_objects(B)\n",
    "reset_data_objects(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:59.682466300Z",
     "start_time": "2023-11-12T16:12:59.653387600Z"
    }
   },
   "id": "7a53b5de76fb9a40"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "Train: (136245, 47) [No. NaN: 168040], Target: (34085, 2) [No. NaN: 0], Test: (2880, 47) [No. NaN: 3971]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (134505, 47) [No. NaN: 158811], Target: (32848, 2) [No. NaN: 4], Test: (2880, 47) [No. NaN: 3912]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (134401, 47) [No. NaN: 157326], Target: (32155, 2) [No. NaN: 6060], Test: (2880, 47) [No. NaN: 4104]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:12:59.896501600Z",
     "start_time": "2023-11-12T16:12:59.683388600Z"
    }
   },
   "id": "102748ff31a74103"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running preprocessing pipeline for CatBoost model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d830517c6140c225"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Sample by selection got dataframe with NaN values\n",
      "[ERROR] Sample by selection got dataframe with NaN values\n"
     ]
    }
   ],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=A.train_data, target_data=A.target_data, is_cat_boost=True)\n",
    "test_data = pre_process_test_data(test_data=A.test_data, is_cat_boost=True)\n",
    "\n",
    "A.train_data = train_data\n",
    "A.target_data = target_data\n",
    "A.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:13:09.699048600Z",
     "start_time": "2023-11-12T16:13:08.229908500Z"
    }
   },
   "id": "2582b8b7f22b5750"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Sample by selection got dataframe with NaN values\n",
      "[ERROR] Sample by selection got dataframe with NaN values\n"
     ]
    }
   ],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=B.train_data, target_data=B.target_data, is_cat_boost=True)\n",
    "test_data = pre_process_test_data(test_data=B.test_data, is_cat_boost=True)\n",
    "\n",
    "B.train_data = train_data\n",
    "B.target_data = target_data\n",
    "B.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:13:21.488296100Z",
     "start_time": "2023-11-12T16:13:19.651943500Z"
    }
   },
   "id": "9db8c1ae2d5eb2b6"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Sample by selection got dataframe with NaN values\n",
      "[ERROR] Sample by selection got dataframe with NaN values\n"
     ]
    }
   ],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=C.train_data, target_data=C.target_data, is_cat_boost=True)\n",
    "test_data = pre_process_test_data(test_data=C.test_data, is_cat_boost=True)\n",
    "\n",
    "C.train_data = train_data\n",
    "C.target_data = target_data\n",
    "C.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:13:23.013015600Z",
     "start_time": "2023-11-12T16:13:21.359768500Z"
    }
   },
   "id": "18b136e2dbb714cd"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "Train: (34018, 49) [No. NaN: 0], Target: (34018, 1) [No. NaN: 0], Test: (720, 49) [No. NaN: 0]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (25875, 49) [No. NaN: 0], Target: (25875, 1) [No. NaN: 0], Test: (720, 49) [No. NaN: 0]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (21111, 49) [No. NaN: 0], Target: (21111, 1) [No. NaN: 0], Test: (720, 49) [No. NaN: 0]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:16:39.851385700Z",
     "start_time": "2023-11-12T16:16:39.436648700Z"
    }
   },
   "id": "2dd0b3962d03ffa4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbf5195d1813b58c"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "def create_sinusoidal_features(df: pd.DataFrame, hour_col: str = 'hour', day_col: str = 'day_of_year') -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    dates = df.index.values\n",
    "    df['date_forecast'] = dates\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Create hour and day_of_year columns\n",
    "    df['hour'] = df['date_forecast'].dt.hour\n",
    "    df['day_of_year'] = df['date_forecast'].dt.dayofyear\n",
    "\n",
    "    # Extract week number using isocalendar()\n",
    "    df['week_of_year'] = df['date_forecast'].dt.isocalendar().week\n",
    "    df['month'] = df['date_forecast'].dt.month\n",
    "\n",
    "    # Constants for sinusoidal functions\n",
    "    hours_in_day = 24\n",
    "    days_in_year = 365.25  # Accounting for leap years\n",
    "    weeks_in_year = 52\n",
    "    months_in_year = 12\n",
    "\n",
    "    # Create sinusoidal features based on the hour of the day\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * (df[hour_col] - 18) / hours_in_day)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * (df[hour_col] - 18) / hours_in_day)\n",
    "\n",
    "    # Create sinusoidal features based on the day of the year\n",
    "    df['sin_day'] = np.sin(2 * np.pi * (df[day_col] - 355) / days_in_year)\n",
    "    df['cos_day'] = np.cos(2 * np.pi * (df[day_col] - 355) / days_in_year)\n",
    "\n",
    "    # Create sinusoidal features based on the week of the year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['week_of_year'] / weeks_in_year)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['week_of_year'] / weeks_in_year)\n",
    "\n",
    "    # Create sinusoidal features based on the month\n",
    "    df['sin_month'] = np.sin(2 * np.pi * df['month'] / months_in_year)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * df['month'] / months_in_year)\n",
    "\n",
    "    dates = df['date_forecast'].to_numpy()\n",
    "    df.index = dates\n",
    "    df.drop(columns=['hour', 'day_of_year', 'week_of_year', 'month', 'date_forecast'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_sinusoidal_features_for_location(location: LocationData) -> None:\n",
    "    train_data = location.train_data.copy()\n",
    "    test_data = location.test_data.copy()\n",
    "\n",
    "    train_data = create_sinusoidal_features(train_data)\n",
    "    test_data = create_sinusoidal_features(test_data)\n",
    "\n",
    "    train_data.sort_index(inplace=True)\n",
    "    test_data.sort_index(inplace=True)\n",
    "\n",
    "    location.train_data = train_data\n",
    "    location.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:16:47.658097400Z",
     "start_time": "2023-11-12T16:16:47.602458500Z"
    }
   },
   "id": "6f48677ca909d56f"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "create_sinusoidal_features_for_location(A)\n",
    "create_sinusoidal_features_for_location(B)\n",
    "create_sinusoidal_features_for_location(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:16:57.952987Z",
     "start_time": "2023-11-12T16:16:57.730862700Z"
    }
   },
   "id": "6d2159c768e2285"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "Train: (34018, 57) [No. NaN: 0], Target: (34018, 1) [No. NaN: 0], Test: (720, 57) [No. NaN: 0]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (25875, 57) [No. NaN: 0], Target: (25875, 1) [No. NaN: 0], Test: (720, 57) [No. NaN: 0]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (21111, 57) [No. NaN: 0], Target: (21111, 1) [No. NaN: 0], Test: (720, 57) [No. NaN: 0]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:17:09.813052700Z",
     "start_time": "2023-11-12T16:17:09.404274900Z"
    }
   },
   "id": "f50b53df83c83aa6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining functions for CatBoost model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "871b8f994d7bd8e2"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val):\n",
    "    predictions = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, predictions)\n",
    "    return mae\n",
    "\n",
    "\n",
    "def train_catboost_ensemble(location, N_MODELS=10):\n",
    "    train_data = location.train_data\n",
    "    target_data = location.target_data\n",
    "\n",
    "    total_length = len(train_data)\n",
    "    val_size = int(0.1 * total_length)\n",
    "\n",
    "    mae_list = []\n",
    "    models = []\n",
    "    depth_values = [7, 7, 8, 8, 8, 8, 9, 9, 9, 10]\n",
    "    learning_rate_values = [0.01, 0.04, 0.01, 0.02, 0.03, 0.04, 0.02, 0.03, 0.04, 0.03]\n",
    "    summer_evaluated = []\n",
    "    train_dates = train_data.index.values\n",
    "\n",
    "    for i in range(N_MODELS):\n",
    "        val_start_idx = int((i / N_MODELS) * (total_length - val_size))\n",
    "\n",
    "        X_train = pd.concat([train_data[:val_start_idx], train_data[val_start_idx + val_size:]], axis=0)\n",
    "        y_train = pd.concat([target_data[:val_start_idx], target_data[val_start_idx + val_size:]], axis=0)\n",
    "        X_val = train_data[val_start_idx:val_start_idx + val_size]\n",
    "        y_val = target_data[val_start_idx:val_start_idx + val_size]\n",
    "\n",
    "        date_at_middle_of_val = pd.to_datetime(X_train.index[val_start_idx + int(val_size / 2)])\n",
    "        evaluation_month = date_at_middle_of_val.month + 7\n",
    "        if evaluation_month > 12:\n",
    "            evaluation_month -= 12\n",
    "\n",
    "        summer_months = [5, 6, 7]\n",
    "        is_summer = evaluation_month in summer_months\n",
    "        summer_evaluated.append(is_summer)\n",
    "\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=20000,\n",
    "            depth=depth_values[i],\n",
    "            learning_rate=learning_rate_values[i],\n",
    "            loss_function='MAE',\n",
    "            od_type='Iter',\n",
    "            od_wait=100,\n",
    "            l2_leaf_reg=3,\n",
    "            random_seed=42,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # Fit the model using the validation set for early stopping\n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True, early_stopping_rounds=100)\n",
    "\n",
    "        # After training, evaluate the final model on the validation set\n",
    "        final_mae = evaluate_model(model, X_val, y_val)\n",
    "        print(f\"Model {i + 1} M[{evaluation_month}]: MAE:\\t{round(final_mae, 4)}\")\n",
    "\n",
    "        mae_list.append(final_mae)\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    print(f\"Average MAE:\\t{round(np.mean(mae_list), 4)}\")\n",
    "\n",
    "    return models, summer_evaluated\n",
    "\n",
    "\n",
    "def get_ensemble_predictions(models, test_data, summer_evaluated):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        predictions = model.predict(test_data)\n",
    "        if summer_evaluated[i]:\n",
    "            all_predictions.extend([\n",
    "                predictions,\n",
    "                predictions,\n",
    "                predictions,\n",
    "                predictions,\n",
    "            ])\n",
    "        else:\n",
    "            all_predictions.append(predictions)\n",
    "\n",
    "    ensemble_predictions = np.mean(all_predictions, axis=0)\n",
    "    return ensemble_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:22:56.222898700Z",
     "start_time": "2023-11-12T16:22:56.192534800Z"
    }
   },
   "id": "e04fc78b95151b8e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training CatBoost models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62597d58cb55938d"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 M[8]: MAE:\t282.4645\n",
      "Model 2 M[12]: MAE:\t72.3829\n",
      "Model 3 M[4]: MAE:\t261.4866\n",
      "Model 4 M[8]: MAE:\t243.2635\n",
      "Model 5 M[12]: MAE:\t72.9029\n",
      "Model 6 M[5]: MAE:\t291.5898\n",
      "Model 7 M[9]: MAE:\t195.9941\n",
      "Model 8 M[1]: MAE:\t70.4181\n",
      "Model 9 M[5]: MAE:\t284.7359\n",
      "Model 10 M[10]: MAE:\t156.0276\n",
      "Average MAE:\t193.1266\n"
     ]
    }
   ],
   "source": [
    "A_models, A_summer_evaluated = train_catboost_ensemble(location=A)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:31:48.532553300Z",
     "start_time": "2023-11-12T16:23:02.723275900Z"
    }
   },
   "id": "9bd8e91049e19beb"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 M[2]: MAE:\t24.6271\n",
      "Model 2 M[5]: MAE:\t64.3168\n",
      "Model 3 M[9]: MAE:\t27.3004\n",
      "Model 4 M[12]: MAE:\t9.6495\n",
      "Model 5 M[5]: MAE:\t53.8982\n",
      "Model 6 M[9]: MAE:\t20.7133\n",
      "Model 7 M[2]: MAE:\t22.5745\n",
      "Model 8 M[7]: MAE:\t53.643\n",
      "Model 9 M[6]: MAE:\t8.0122\n",
      "Model 10 M[10]: MAE:\t22.9655\n",
      "Average MAE:\t30.77\n"
     ]
    }
   ],
   "source": [
    "B_models, B_summer_evaluated = train_catboost_ensemble(location=B)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:35:16.873506600Z",
     "start_time": "2023-11-12T16:31:48.450555Z"
    }
   },
   "id": "eb284a12468226b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 M[9]: MAE:\t15.4571\n"
     ]
    }
   ],
   "source": [
    "C_models, C_summer_evaluated = train_catboost_ensemble(location=C)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:35:16.793440700Z"
    }
   },
   "id": "ae149c86170b5b16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CatBoost predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a05099bb51fcec9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "A_prediction = get_ensemble_predictions(A_models, A.test_data, A_summer_evaluated)\n",
    "B_prediction = get_ensemble_predictions(B_models, B.test_data, B_summer_evaluated)\n",
    "C_prediction = get_ensemble_predictions(C_models, C.test_data, C_summer_evaluated)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:19:37.873348200Z"
    }
   },
   "id": "9f0373b4e9113427"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction_list = [A_prediction, B_prediction, C_prediction]\n",
    "predictions = np.concatenate(prediction_list)\n",
    "catboost_predictions = predictions.clip(min=0, max=None)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8b23491e45918e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction_list[autogloun_predictions, catboost_predictions]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:19:37.875347400Z"
    }
   },
   "id": "5ead482e9a4e7e44"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b91d04668e2728d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17eac1d7b99002",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:19:37.878425400Z"
    }
   },
   "outputs": [],
   "source": [
    "for prediction in prediction_list:\n",
    "    plt.figure(figsize=(30, 7))\n",
    "    plt.plot(catboost_predictions, 'b-')\n",
    "    plt.plot(autogloun_predictions, 'r-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c29779",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T16:19:37.881443100Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction_list = [np.mean(prediction_list, axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd360cbb63a5fc",
   "metadata": {},
   "source": [
    "# Saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4e0f1c8748630",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "\n",
    "def create_csv(*prediction_list: np.ndarray) -> None:\n",
    "    prediction_path: str = os.path.join(\n",
    "        DATA_DIR,\n",
    "        \"predictions\",\n",
    "        time.strftime(\"%Y%m%d-%H%M%S\") + \".csv\",\n",
    "    )\n",
    "\n",
    "    output: np.ndarray = np.concatenate(prediction_list)\n",
    "    output = output.clip(min=0, max=None)\n",
    "    indexes = np.arange(0, len(output), 1, dtype=int)\n",
    "\n",
    "    data = {\n",
    "        \"id\": indexes,\n",
    "        \"prediction\": output\n",
    "    }\n",
    "\n",
    "    dataframe: pd.DataFrame = pd.DataFrame(data)\n",
    "    dataframe.to_csv(prediction_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4562075a901b4",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_csv(*prediction_list)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
