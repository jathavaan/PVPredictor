{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "26d9f234-58de-4fc9-9f4e-443ea8265844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:22.936391600Z",
     "start_time": "2023-11-12T16:51:22.621723Z"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ea0ba8bc12eaef6f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:23.379927900Z",
     "start_time": "2023-11-12T16:51:22.640494800Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from catboost import CatBoostRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850e2dfc3bb16aa",
   "metadata": {},
   "source": [
    "Setting rules for libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8081b47cf627a139",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:23.384925500Z",
     "start_time": "2023-11-12T16:51:22.652280300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Warnings\n",
    "\n",
    "# Pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f768895a70a9bd57",
   "metadata": {},
   "source": [
    "# Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cf20b5c6b0ced7a4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:23.420943600Z",
     "start_time": "2023-11-12T16:51:22.674057900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Features to remove\n",
    "DATE_CALC: str = \"date_calc\"\n",
    "DATE_FORECAST: str = \"date_forecast\"\n",
    "ABSOLUTE_HUMIDITY: str = 'absolute_humidity_2m:gm3'\n",
    "AIR_DENSITY: str = 'air_density_2m:kgm3'\n",
    "CEILING_HEIGHT: str = 'ceiling_height_agl:m'\n",
    "CLEAR_SKY_ENERGY: str = 'clear_sky_energy_1h:J'\n",
    "CLEAR_SKY_RAD: str = 'clear_sky_rad:W'\n",
    "CLOUD_BASE: str = 'cloud_base_agl:m'\n",
    "DEW_OR_RIME: str = 'dew_or_rime:idx'\n",
    "DEW_POINT: str = 'dew_point_2m:K'\n",
    "DIFFUSE_RAD: str = 'diffuse_rad:W'\n",
    "DIFFUSE_RAD_1H: str = 'diffuse_rad_1h:J'\n",
    "DIRECT_RAD: str = 'direct_rad:W'\n",
    "DIRECT_RAD_1H: str = 'direct_rad_1h:J'\n",
    "EFFECTIVE_CLOUD_COVER: str = 'effective_cloud_cover:p'\n",
    "ELEVATION: str = 'elevation:m'\n",
    "FRESH_SNOW_12H: str = 'fresh_snow_12h:cm'\n",
    "FRESH_SNOW_1H: str = 'fresh_snow_1h:cm'\n",
    "FRESH_SNOW_24H: str = 'fresh_snow_24h:cm'\n",
    "FRESH_SNOW_3H: str = 'fresh_snow_3h:cm'\n",
    "FRESH_SNOW_6H: str = 'fresh_snow_6h:cm'\n",
    "IS_DAY: str = 'is_day:idx'\n",
    "IS_IN_SHADOW: str = 'is_in_shadow:idx'\n",
    "MSL_PRESSURE: str = 'msl_pressure:hPa'\n",
    "PRECIP_5MIN: str = 'precip_5min:mm'\n",
    "PRECIP_TYPE_5MIN: str = 'precip_type_5min:idx'\n",
    "PRESSURE_100M: str = 'pressure_100m:hPa'\n",
    "PRESSURE_50M: str = 'pressure_50m:hPa'\n",
    "PROB_RIME: str = 'prob_rime:p'\n",
    "RAIN_WATER: str = 'rain_water:kgm2'\n",
    "RELATIVE_HUMIDITY: str = 'relative_humidity_1000hPa:p'\n",
    "SFC_PRESSURE: str = 'sfc_pressure:hPa'\n",
    "SNOW_DENSITY: str = 'snow_density:kgm3'\n",
    "SNOW_DEPTH: str = 'snow_depth:cm'\n",
    "SNOW_DRIFT: str = 'snow_drift:idx'\n",
    "SNOW_MELT_10MIN: str = 'snow_melt_10min:mm'\n",
    "SNOW_WATER: str = 'snow_water:kgm2'\n",
    "SUN_AZIMUTH: str = 'sun_azimuth:d'\n",
    "SUN_ELEVATION: str = 'sun_elevation:d'\n",
    "SUPER_COOLED_LIQUID_WATER: str = 'super_cooled_liquid_water:kgm2'\n",
    "T_1000HPA: str = 't_1000hPa:K'\n",
    "TOTAL_CLOUD_COVER: str = 'total_cloud_cover:p'\n",
    "VISIBILITY: str = 'visibility:m'\n",
    "WIND_SPEED_10M: str = 'wind_speed_10m:ms'\n",
    "WIND_SPEED_U_10M: str = 'wind_speed_u_10m:ms'\n",
    "WIND_SPEED_V_10M: str = 'wind_speed_v_10m:ms'\n",
    "WIND_SPEED_W_1000HPA: str = 'wind_speed_w_1000hPa:ms'\n",
    "IS_ESTIMATED = 'is_estimated'\n",
    "\n",
    "TIME = 'time'\n",
    "PV_MEASUREMENT = 'pv_measurement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8e5747184b6faafe",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:23.455934Z",
     "start_time": "2023-11-12T16:51:22.682982500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def read_parquet(filepath: str) -> pd.DataFrame:\n",
    "    dataframe: pd.DataFrame = pd.read_parquet(filepath)\n",
    "    if DATE_CALC in dataframe.columns:\n",
    "        dataframe = dataframe.drop(columns=[DATE_CALC])\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def concat_observed_and_estimated_train_data(\n",
    "        X_observed: pd.DataFrame,\n",
    "        X_estimated: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    X_observed: pd.DataFrame = X_observed.copy()\n",
    "    X_estimated: pd.DataFrame = X_estimated.copy()\n",
    "\n",
    "    assert X_observed.shape[1] == X_estimated.shape[1]\n",
    "\n",
    "    concatenated_dataframe: pd.DataFrame = pd.concat(\n",
    "        objs=[X_observed, X_estimated],\n",
    "        axis=0\n",
    "    ).sort_values(\n",
    "        by=DATE_FORECAST,\n",
    "        ascending=True\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return concatenated_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a2f38c501e227ecc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:23.491535Z",
     "start_time": "2023-11-12T16:51:22.699982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data transfer object for location data\n",
    "class LocationData:\n",
    "    combined_data: pd.DataFrame\n",
    "\n",
    "    def __init__(self, train_data, target_data, test_data) -> None:\n",
    "        self.train_data = train_data\n",
    "        self.target_data = target_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "        self.original_train_data = train_data.copy()\n",
    "        self.original_target_data = target_data.copy()\n",
    "        self.original_test_data = test_data.copy()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Train: {self.train_data.shape} [No. NaN: {self.train_data.isna().sum().sum()}], Target: {self.target_data.shape} [No. NaN: {self.target_data.isna().sum().sum()}], Test: {self.test_data.shape} [No. NaN: {self.test_data.isna().sum().sum()}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "60d61a1645354f7c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:24.275695100Z",
     "start_time": "2023-11-12T16:51:22.715095400Z"
    }
   },
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/A/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/A/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/A/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/A/X_test_estimated.parquet\")\n",
    "\n",
    "observed_train['is_estimated'] = 0\n",
    "estimated_train['is_estimated'] = 1\n",
    "test_data['is_estimated'] = 1\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d6374820ca84a630",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:24.279705Z",
     "start_time": "2023-11-12T16:51:22.809031400Z"
    }
   },
   "outputs": [],
   "source": [
    "A = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "58333e566cc3633f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:24.330694600Z",
     "start_time": "2023-11-12T16:51:22.827047700Z"
    }
   },
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/B/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/B/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/B/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/B/X_test_estimated.parquet\")\n",
    "\n",
    "observed_train['is_estimated'] = 0\n",
    "estimated_train['is_estimated'] = 1\n",
    "test_data['is_estimated'] = 1\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")\n",
    "\n",
    "B = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "edbd3f707ed3d2f7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:24.333694Z",
     "start_time": "2023-11-12T16:51:22.934392100Z"
    }
   },
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/C/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/C/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/C/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/C/X_test_estimated.parquet\")\n",
    "\n",
    "observed_train['is_estimated'] = 0\n",
    "estimated_train['is_estimated'] = 1\n",
    "test_data['is_estimated'] = 1\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7a3ab86d84357bed",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:24.333694Z",
     "start_time": "2023-11-12T16:51:23.029906900Z"
    }
   },
   "outputs": [],
   "source": [
    "C = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b3d3d8fec6811b18",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:24.333694Z",
     "start_time": "2023-11-12T16:51:23.048912300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Train: (136245, 47) [No. NaN: 168040], Target: (34085, 2) [No. NaN: 0], Test: (2880, 47) [No. NaN: 3971]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (134505, 47) [No. NaN: 158811], Target: (32848, 2) [No. NaN: 4], Test: (2880, 47) [No. NaN: 3912]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (134401, 47) [No. NaN: 157326], Target: (32155, 2) [No. NaN: 6060], Test: (2880, 47) [No. NaN: 4104]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0124801bff563",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Defining functions for preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f4e6db271eea0",
   "metadata": {},
   "source": [
    "Function to remove features and replace `NaN` values with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "926a2ddc44c56339",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:24.333694Z",
     "start_time": "2023-11-12T16:51:23.093086300Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_feature(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    dataframe = dataframe.drop(columns=[*features])\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def replace_nan_with_zero(dataframe: pd.DataFrame, feature: str) -> pd.DataFrame:\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe[[feature]] = dataframe[[feature]].fillna(0)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f994238640695aa",
   "metadata": {},
   "source": [
    "Function to resample data by categorizing them\n",
    "- Features to average\n",
    "- Features to pick a value\n",
    "- Features to change redefine `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "76f0e24c1ba0fe4b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:24.333694Z",
     "start_time": "2023-11-12T16:51:23.113275300Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_by_selection(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "    if reduced_dataframe.isna().sum().sum() > 0:\n",
    "        print(\"[ERROR] Sample by selection got dataframe with NaN values\")\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.last()\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_by_mean(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.mean()\n",
    "    \n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_by_sum(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.sum()\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_cloud_base(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    cloud_base_frame = dataframe[[CLOUD_BASE]]\n",
    "\n",
    "    # Filling NaN values with -2_000\n",
    "    filled_frame = cloud_base_frame.copy()\n",
    "    filled_frame = filled_frame.fillna(-100_000)\n",
    "\n",
    "    # Resampling to 1H\n",
    "    resampler = filled_frame.resample('H')\n",
    "    cloud_base_frame = resampler.mean()\n",
    "\n",
    "    # Replacing negative values with -666\n",
    "    cloud_bases = cloud_base_frame[CLOUD_BASE].to_numpy()\n",
    "    negative_indexes = np.where(cloud_bases < 0)[0]\n",
    "    cloud_bases[negative_indexes] = -666\n",
    "    cloud_base_frame[CLOUD_BASE] = cloud_bases\n",
    "\n",
    "    # Dropping NaN values that were introduced by resampling\n",
    "    reduced_dataframe = cloud_base_frame.copy()\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def merge_frames(*dataframes: pd.DataFrame) -> pd.DataFrame:\n",
    "    indexes = [df.index for df in dataframes]\n",
    "\n",
    "    # Finding intersecting indexes in list of indexes, the indexes are datetime\n",
    "    intersecting_indexes = list(set(indexes[0]).intersection(*indexes))\n",
    "    filtered_dataframes = [df.loc[intersecting_indexes] for df in dataframes]\n",
    "\n",
    "    return pd.concat(filtered_dataframes, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4830f59eace7ae",
   "metadata": {},
   "source": [
    "Functions to synchronze dates in weather data and target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "98926ee1a8a03451",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:24.333694Z",
     "start_time": "2023-11-12T16:51:23.125274500Z"
    }
   },
   "outputs": [],
   "source": [
    "def synchronize_timestamps(train_data: pd.DataFrame, target_data: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # IndexError if index in both train data and target data is DateTimeIndex\n",
    "    if not isinstance(train_data.index, pd.DatetimeIndex):\n",
    "        raise IndexError(\"Train data does not have DateTimeIndex\")\n",
    "\n",
    "    if not isinstance(target_data.index, pd.DatetimeIndex):\n",
    "        raise IndexError(\"Target data does not have DateTimeIndex\")\n",
    "\n",
    "    train_data.sort_index(inplace=True)\n",
    "    target_data.sort_index(inplace=True)\n",
    "\n",
    "    train_dates = train_data.index.values\n",
    "    target_dates = target_data.index.values\n",
    "\n",
    "    # Finding intersecting dates and filtering dataframes\n",
    "    intersecting_dates = np.intersect1d(train_dates, target_dates)\n",
    "    train_data = train_data.loc[intersecting_dates]\n",
    "    target_data = target_data.loc[intersecting_dates]\n",
    "\n",
    "    return train_data, target_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dateset contains many constant values for some features, any row with constant values is removed. If there is are at least 24 rows with constant values, the rows are removed.\n",
    "- -666 in cloud base should not be removed as this is added in earlier "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f0c01434829e1ae"
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "472023231528654c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:24.334693800Z",
     "start_time": "2023-11-12T16:51:23.146280200Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_constant_measurements(\n",
    "        train_data: pd.DataFrame,\n",
    "        target_data: pd.DataFrame,\n",
    "        THRESHOLD: int = 24\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    indexes = []\n",
    "    count = 0\n",
    "    previous_value = None\n",
    "    start_index = None\n",
    "\n",
    "    # Setting date indexes as columns and resetting index\n",
    "    train_data[DATE_FORECAST] = train_data.index.values\n",
    "    target_data[TIME] = target_data.index.values\n",
    "\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    target_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Assuming y is a DataFrame and has a column for pv_measurement values\n",
    "    pv_measurements = target_data[PV_MEASUREMENT].to_numpy()\n",
    "\n",
    "    # Identifying indexes of consecutive constant values\n",
    "    for index, value in enumerate(pv_measurements):\n",
    "        if value == previous_value:\n",
    "            count += 1\n",
    "            if count == 1:\n",
    "                start_index = index - 1\n",
    "            if count >= THRESHOLD and value > -0.1:\n",
    "                indexes.extend(range(start_index, index + 1))\n",
    "        else:\n",
    "            count = 0\n",
    "        previous_value = value\n",
    "\n",
    "    # Assuming y has a column for dates and X has a similar column to filter on\n",
    "    dates = target_data.loc[indexes, TIME].unique()\n",
    "\n",
    "    filtered_train_data = train_data[~train_data[DATE_FORECAST].isin(dates)]\n",
    "    filtered_target_data = target_data[~target_data[TIME].isin(dates)]\n",
    "\n",
    "    # Switching back to date as index\n",
    "    filtered_train_data = filtered_train_data.set_index(DATE_FORECAST)\n",
    "    filtered_target_data = filtered_target_data.set_index(TIME)\n",
    "\n",
    "    return filtered_train_data, filtered_target_data\n",
    "\n",
    "\n",
    "def drop_nan_values_from_target_data(train_data: pd.DataFrame, target_data: pd.DataFrame) -> tuple[\n",
    "    pd.DataFrame, pd.DataFrame]:\n",
    "    target_data.dropna(axis=0, how='any', inplace=True)\n",
    "    train_data, target_data = synchronize_timestamps(train_data, target_data)\n",
    "    return train_data, target_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948b908f0f59cef",
   "metadata": {},
   "source": [
    "## Adding lag features, this has to come here because of the resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c1317434ce4b07e5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:24.334693800Z",
     "start_time": "2023-11-12T16:51:23.159290700Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_lag_features(dataframe: pd.DataFrame, features: list[str], lags: list[int]) -> tuple[pd.DataFrame, list[str]]:\n",
    "    dataframe = dataframe.copy()\n",
    "    lag_feature_names: list[str] = []\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            lag_feature_name = f\"{feature}_lag_{lag}\"\n",
    "            dataframe[lag_feature_name] = dataframe[feature].shift(lag)\n",
    "            lag_feature_names.append(lag_feature_name)\n",
    "\n",
    "    return dataframe, lag_feature_names\n",
    "\n",
    "\n",
    "def add_lead_features(dataframe: pd.DataFrame, features: list[str], lags: list[int]) -> tuple[pd.DataFrame, list[str]]:\n",
    "    dataframe = dataframe.copy()\n",
    "    lead_feature_names: list[str] = []\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            lead_feature_name = f\"{feature}_lead_{lag}\"\n",
    "            dataframe[lead_feature_name] = dataframe[feature].shift(-lag)\n",
    "            lead_feature_names.append(lead_feature_name)\n",
    "\n",
    "    return dataframe, lead_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4514f22c17b74b8e",
   "metadata": {},
   "source": [
    "Pipeline for preprocessing that is common for both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "92c2733c9a43a644",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:24.334693800Z",
     "start_time": "2023-11-12T16:51:23.177811600Z"
    }
   },
   "outputs": [],
   "source": [
    "def common_pre_processing(dataframe: pd.DataFrame, is_cat_boost: bool) -> pd.DataFrame:\n",
    "    features_to_drop = [SNOW_DRIFT, CEILING_HEIGHT, SNOW_DENSITY]\n",
    "    \n",
    "    if is_cat_boost: \n",
    "        dataframe = remove_feature(dataframe, *features_to_drop)\n",
    "\n",
    "    dataframe = dataframe.set_index(DATE_FORECAST)\n",
    "    feature_to_lag_and_lead: list[str] = [DIRECT_RAD]\n",
    "\n",
    "    # Adding lag and lead features\n",
    "    lag_features = []\n",
    "    lead_features = []\n",
    "\n",
    "    if is_cat_boost:\n",
    "        dataframe, lag_features = add_lag_features(dataframe, feature_to_lag_and_lead, [1, 2, 3])\n",
    "        dataframe, lead_features = add_lead_features(dataframe, feature_to_lag_and_lead, [1, 2, 3])\n",
    "\n",
    "    # Resample data to hours\n",
    "    selection_features = [\n",
    "        CLEAR_SKY_ENERGY,  # Maybe move\n",
    "        DIRECT_RAD_1H,  # MAYBE MOVE\n",
    "        DIFFUSE_RAD_1H,  # Maybe move\n",
    "        FRESH_SNOW_1H,\n",
    "        FRESH_SNOW_3H,\n",
    "        FRESH_SNOW_6H,\n",
    "        FRESH_SNOW_12H,\n",
    "        FRESH_SNOW_24H,\n",
    "        DEW_OR_RIME,\n",
    "        IS_DAY,\n",
    "        IS_IN_SHADOW,\n",
    "        PRECIP_TYPE_5MIN,\n",
    "        ELEVATION,\n",
    "        IS_ESTIMATED,\n",
    "        *lag_features,\n",
    "        *lead_features,\n",
    "    ]\n",
    "\n",
    "    mean_features = [\n",
    "        ABSOLUTE_HUMIDITY,\n",
    "        CLEAR_SKY_RAD,\n",
    "        DIFFUSE_RAD,\n",
    "        DIRECT_RAD,\n",
    "        AIR_DENSITY,\n",
    "        DEW_POINT,\n",
    "        MSL_PRESSURE,\n",
    "        PRESSURE_100M,\n",
    "        PRESSURE_50M,\n",
    "        SFC_PRESSURE,\n",
    "        RELATIVE_HUMIDITY,\n",
    "        T_1000HPA,\n",
    "        SUN_AZIMUTH,\n",
    "        SUN_ELEVATION,\n",
    "        EFFECTIVE_CLOUD_COVER,\n",
    "        TOTAL_CLOUD_COVER,\n",
    "        VISIBILITY,\n",
    "        WIND_SPEED_10M,\n",
    "        WIND_SPEED_U_10M,\n",
    "        WIND_SPEED_V_10M,\n",
    "        WIND_SPEED_W_1000HPA,\n",
    "        SNOW_MELT_10MIN,\n",
    "        PROB_RIME,\n",
    "        SUPER_COOLED_LIQUID_WATER,\n",
    "        SNOW_DEPTH, \n",
    "    ]\n",
    "    \n",
    "    sum_features = [\n",
    "        SNOW_WATER,\n",
    "        PRECIP_5MIN,\n",
    "        RAIN_WATER,\n",
    "    ]\n",
    "    \n",
    "    if not is_cat_boost:\n",
    "        for feature in features_to_drop:\n",
    "            mean_features.append(feature)\n",
    "\n",
    "    selection_df = sample_by_selection(dataframe, *selection_features)\n",
    "    mean_df = sample_by_mean(dataframe, *mean_features)\n",
    "    sum_df = sample_by_sum(dataframe, *sum_features)\n",
    "    ceiling_height_df = sample_cloud_base(dataframe)\n",
    "    dataframe = merge_frames(selection_df, mean_df, ceiling_height_df, sum_df)\n",
    "\n",
    "    if is_cat_boost:\n",
    "        dataframe = dataframe.dropna(axis=0, how='any')\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db91786254d2a576",
   "metadata": {},
   "source": [
    "### Preprocessing pipelines for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9556d94345fac3ba",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:24.334693800Z",
     "start_time": "2023-11-12T16:51:23.185809700Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process_train_data(\n",
    "        train_data: pd.DataFrame, \n",
    "        target_data: pd.DataFrame, \n",
    "        is_cat_boost: bool\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_data = common_pre_processing(dataframe=train_data, is_cat_boost=is_cat_boost)\n",
    "\n",
    "    # Setting time as index for target data\n",
    "    target_data[TIME] = pd.to_datetime(target_data[TIME])\n",
    "    target_data = target_data.set_index(TIME)\n",
    "\n",
    "    # Preprocessing that affects both train and target data\n",
    "    train_data, target_data = synchronize_timestamps(train_data=train_data, target_data=target_data)\n",
    "    train_data, target_data = remove_constant_measurements(train_data=train_data, target_data=target_data)\n",
    "    train_data, target_data = drop_nan_values_from_target_data(train_data=train_data, target_data=target_data)\n",
    "\n",
    "    return train_data, target_data\n",
    "\n",
    "\n",
    "def pre_process_test_data(test_data: pd.DataFrame, is_cat_boost: bool) -> pd.DataFrame:\n",
    "    if is_cat_boost:\n",
    "        test_data = common_pre_processing(dataframe=test_data, is_cat_boost=is_cat_boost)\n",
    "    else:\n",
    "        test_data = test_data.set_index(DATE_FORECAST)\n",
    "        test_data = test_data.groupby(test_data.index.floor('H')).mean()\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79beb4",
   "metadata": {},
   "source": [
    "# AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(\n",
    "    train_data=A.train_data,\n",
    "    target_data=A.target_data,\n",
    "    is_cat_boost=False\n",
    ")\n",
    "\n",
    "test_data = pre_process_test_data(test_data=A.test_data, is_cat_boost=False)\n",
    "A.train_data = train_data\n",
    "A.target_data = target_data\n",
    "A.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:24.506647600Z",
     "start_time": "2023-11-12T16:51:23.205809900Z"
    }
   },
   "id": "6ad5e575852aadfb"
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(\n",
    "    train_data=B.train_data,\n",
    "    target_data=B.target_data,\n",
    "    is_cat_boost=False\n",
    ")\n",
    "test_data = pre_process_test_data(test_data=B.test_data, is_cat_boost=False)\n",
    "B.train_data = train_data\n",
    "B.target_data = target_data\n",
    "B.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:25.007563400Z",
     "start_time": "2023-11-12T16:51:23.995652700Z"
    }
   },
   "id": "a4a11892d7383702"
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(\n",
    "    train_data=C.train_data,\n",
    "    target_data=C.target_data,\n",
    "    is_cat_boost=False\n",
    ")\n",
    "test_data = pre_process_test_data(test_data=C.test_data, is_cat_boost=False)\n",
    "C.train_data = train_data\n",
    "C.target_data = target_data\n",
    "C.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:26.794666100Z",
     "start_time": "2023-11-12T16:51:25.004561800Z"
    }
   },
   "id": "d7936de9ca1f1338"
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "data": {
      "text/plain": "Train: (34042, 46) [No. NaN: 40037], Target: (34042, 1) [No. NaN: 0], Test: (720, 46) [No. NaN: 978]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (25899, 46) [No. NaN: 30105], Target: (25899, 1) [No. NaN: 0], Test: (720, 46) [No. NaN: 965]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (21135, 46) [No. NaN: 26480], Target: (21135, 1) [No. NaN: 0], Test: (720, 46) [No. NaN: 1010]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:26.887326200Z",
     "start_time": "2023-11-12T16:51:26.797681200Z"
    }
   },
   "id": "53a3241e6b3792ef"
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a20a9cca-6d35-4e5a-aba2-ef43b90af3e8",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:26.983579800Z",
     "start_time": "2023-11-12T16:51:26.827831300Z"
    }
   },
   "outputs": [],
   "source": [
    "MINUTES: int = 1\n",
    "time_limit = int(60 * MINUTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "33e3fd13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:26.985580700Z",
     "start_time": "2023-11-12T16:51:26.847819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merged files required by the AutoGluon model\n",
    "def merge_train_and_target(location: LocationData) -> pd.DataFrame:\n",
    "    train_data = location.train_data.copy()\n",
    "    target_data = location.target_data.copy()\n",
    "\n",
    "    return pd.merge(left=train_data, right=target_data, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "\n",
    "def prepare_data_for_ag(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    for col in dataframe.columns:\n",
    "        if dataframe[col].dtype == 'object':\n",
    "            dataframe[col] = dataframe[col].astype(str)\n",
    "        else:\n",
    "            dataframe[col] = pd.to_numeric(dataframe[col], errors='coerce')\n",
    "    \"\"\"\n",
    "\n",
    "    if pd.api.types.is_datetime64_any_dtype(dataframe.index):\n",
    "        dataframe = dataframe.sort_index()  # Sort by date if the index is a datetime\n",
    "        dataframe = dataframe.reset_index()  # Reset index\n",
    "\n",
    "    if DATE_FORECAST in dataframe.columns:\n",
    "        dataframe = dataframe.drop(columns=[DATE_FORECAST])\n",
    "\n",
    "    if TIME in dataframe.columns:\n",
    "        dataframe = dataframe.drop(columns=[TIME])\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "cc33d2a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:27.032582Z",
     "start_time": "2023-11-12T16:51:26.859822900Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "combined_A = prepare_data_for_ag(merge_train_and_target(A))\n",
    "combined_B = prepare_data_for_ag(merge_train_and_target(B))\n",
    "combined_C = prepare_data_for_ag(merge_train_and_target(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "26d37c15-7870-48b9-8bf9-cb85db0caa68",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-12T16:51:27.034582800Z",
     "start_time": "2023-11-12T16:51:26.925056500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['clear_sky_energy_1h:J', 'direct_rad_1h:J', 'diffuse_rad_1h:J',\n       'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm',\n       'fresh_snow_12h:cm', 'fresh_snow_24h:cm', 'dew_or_rime:idx',\n       'is_day:idx', 'is_in_shadow:idx', 'precip_type_5min:idx',\n       'elevation:m', 'is_estimated', 'absolute_humidity_2m:gm3',\n       'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad:W',\n       'air_density_2m:kgm3', 'dew_point_2m:K', 'msl_pressure:hPa',\n       'pressure_100m:hPa', 'pressure_50m:hPa', 'sfc_pressure:hPa',\n       'relative_humidity_1000hPa:p', 't_1000hPa:K', 'sun_azimuth:d',\n       'sun_elevation:d', 'effective_cloud_cover:p',\n       'total_cloud_cover:p', 'visibility:m', 'wind_speed_10m:ms',\n       'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n       'wind_speed_w_1000hPa:ms', 'snow_melt_10min:mm', 'prob_rime:p',\n       'super_cooled_liquid_water:kgm2', 'snow_depth:cm',\n       'snow_drift:idx', 'ceiling_height_agl:m', 'snow_density:kgm3',\n       'cloud_base_agl:m', 'snow_water:kgm2', 'precip_5min:mm',\n       'rain_water:kgm2', 'pv_measurement'], dtype=object)"
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_data_A = combined_A[combined_A['is_estimated'] == 1].tail(10100)\n",
    "tuning_data_B = combined_B[combined_B['is_estimated'] == 1].tail(950)\n",
    "tuning_data_C = combined_C[combined_C['is_estimated'] == 1].tail(870)\n",
    "\n",
    "combined_A = combined_A[(~combined_A.index.isin(tuning_data_A.index))]\n",
    "combined_B = combined_B[(~combined_B.index.isin(tuning_data_B.index))]\n",
    "combined_C = combined_C[(~combined_C.index.isin(tuning_data_C.index))]\n",
    "\n",
    "test_A = prepare_data_for_ag(A.test_data)\n",
    "test_B = prepare_data_for_ag(B.test_data)\n",
    "test_C = prepare_data_for_ag(C.test_data)\n",
    "\n",
    "combined_A.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9c187ace-8822-4a31-b9cb-474fb0a01c75",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-12T16:52:01.400439100Z",
     "start_time": "2023-11-12T16:51:26.971580500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231112_165126\\\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=2, num_bag_folds=5, num_bag_sets=2\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231112_165126\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.18\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Disk Space Avail:   662.09 GB / 1022.87 GB (64.7%)\n",
      "Train Data Rows:    29648\n",
      "Train Data Columns: 46\n",
      "Tuning Data Rows:    4394\n",
      "Tuning Data Columns: 46\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 674.60957, 1195.77699)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    20580.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 6.4 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['snow_drift:idx']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['snow_drift:idx']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 45 | ['clear_sky_energy_1h:J', 'direct_rad_1h:J', 'diffuse_rad_1h:J', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 43 | ['clear_sky_energy_1h:J', 'direct_rad_1h:J', 'diffuse_rad_1h:J', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', ...]\n",
      "\t\t('int', ['bool']) :  2 | ['elevation:m', 'snow_density:kgm3']\n",
      "\t0.3s = Fit runtime\n",
      "\t45 features in original data used to generate 45 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.06 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.38s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 3 stack levels (L1 to L3) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 9 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 26.49s of the 59.61s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[168], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m predictor_A \u001B[38;5;241m=\u001B[39m \u001B[43mTabularPredictor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlabel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpv_measurement\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_metric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmean_absolute_error\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m      4\u001B[0m \u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcombined_A\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtuning_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtuning_data_A\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_bag_holdout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_bag_folds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_bag_sets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_stack_levels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtime_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtime_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpresets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbest_quality\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexcluded_model_types\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mKNN\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\core\\utils\\decorators.py:31\u001B[0m, in \u001B[0;36munpack.<locals>._unpack_inner.<locals>._call\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(f)\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     30\u001B[0m     gargs, gkwargs \u001B[38;5;241m=\u001B[39m g(\u001B[38;5;241m*\u001B[39mother_args, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 31\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39mgargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mgkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:986\u001B[0m, in \u001B[0;36mTabularPredictor.fit\u001B[1;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, calibrate_decision_threshold, num_cpus, num_gpus, **kwargs)\u001B[0m\n\u001B[0;32m    984\u001B[0m     aux_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit_weighted_ensemble\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    985\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave(silent\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)  \u001B[38;5;66;03m# Save predictor to disk to enable prediction and training after interrupt\u001B[39;00m\n\u001B[1;32m--> 986\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_learner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    987\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    988\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtuning_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    989\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_unlabeled\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munlabeled_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    990\u001B[0m \u001B[43m    \u001B[49m\u001B[43mholdout_frac\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mholdout_frac\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    991\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_bag_folds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_bag_folds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    992\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_bag_sets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_bag_sets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    993\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_stack_levels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_stack_levels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    994\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhyperparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhyperparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    995\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcore_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcore_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    996\u001B[0m \u001B[43m    \u001B[49m\u001B[43maux_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maux_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    997\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtime_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtime_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    998\u001B[0m \u001B[43m    \u001B[49m\u001B[43minfer_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    999\u001B[0m \u001B[43m    \u001B[49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1000\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbosity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbosity\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1001\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_bag_holdout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_bag_holdout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1002\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1003\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_post_fit_vars()\n\u001B[0;32m   1005\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post_fit(\n\u001B[0;32m   1006\u001B[0m     keep_only_best\u001B[38;5;241m=\u001B[39mkwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeep_only_best\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m   1007\u001B[0m     refit_full\u001B[38;5;241m=\u001B[39mkwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrefit_full\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1012\u001B[0m     infer_limit\u001B[38;5;241m=\u001B[39minfer_limit,\n\u001B[0;32m   1013\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:159\u001B[0m, in \u001B[0;36mAbstractTabularLearner.fit\u001B[1;34m(self, X, X_val, **kwargs)\u001B[0m\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLearner is already fit.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    158\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_fit_input(X\u001B[38;5;241m=\u001B[39mX, X_val\u001B[38;5;241m=\u001B[39mX_val, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 159\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(X\u001B[38;5;241m=\u001B[39mX, X_val\u001B[38;5;241m=\u001B[39mX_val, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\tabular\\learner\\default_learner.py:157\u001B[0m, in \u001B[0;36mDefaultLearner._fit\u001B[1;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001B[0m\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meval_metric \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39meval_metric\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m--> 157\u001B[0m trainer\u001B[38;5;241m.\u001B[39mfit(\n\u001B[0;32m    158\u001B[0m     X\u001B[38;5;241m=\u001B[39mX,\n\u001B[0;32m    159\u001B[0m     y\u001B[38;5;241m=\u001B[39my,\n\u001B[0;32m    160\u001B[0m     X_val\u001B[38;5;241m=\u001B[39mX_val,\n\u001B[0;32m    161\u001B[0m     y_val\u001B[38;5;241m=\u001B[39my_val,\n\u001B[0;32m    162\u001B[0m     X_unlabeled\u001B[38;5;241m=\u001B[39mX_unlabeled,\n\u001B[0;32m    163\u001B[0m     holdout_frac\u001B[38;5;241m=\u001B[39mholdout_frac,\n\u001B[0;32m    164\u001B[0m     time_limit\u001B[38;5;241m=\u001B[39mtime_limit_trainer,\n\u001B[0;32m    165\u001B[0m     infer_limit\u001B[38;5;241m=\u001B[39minfer_limit,\n\u001B[0;32m    166\u001B[0m     infer_limit_batch_size\u001B[38;5;241m=\u001B[39minfer_limit_batch_size,\n\u001B[0;32m    167\u001B[0m     groups\u001B[38;5;241m=\u001B[39mgroups,\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtrainer_fit_kwargs,\n\u001B[0;32m    169\u001B[0m )\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_trainer(trainer\u001B[38;5;241m=\u001B[39mtrainer)\n\u001B[0;32m    171\u001B[0m time_end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\auto_trainer.py:114\u001B[0m, in \u001B[0;36mAutoTrainer.fit\u001B[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001B[0m\n\u001B[0;32m    111\u001B[0m log_str \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m}\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    112\u001B[0m logger\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;241m20\u001B[39m, log_str)\n\u001B[1;32m--> 114\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_multi_and_ensemble\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    115\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    117\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    119\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_unlabeled\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_unlabeled\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhyperparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhyperparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_stack_levels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_stack_levels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtime_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtime_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    123\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcore_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcore_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    124\u001B[0m \u001B[43m    \u001B[49m\u001B[43maux_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maux_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[43m    \u001B[49m\u001B[43minfer_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    126\u001B[0m \u001B[43m    \u001B[49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    127\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2371\u001B[0m, in \u001B[0;36mAbstractTrainer._train_multi_and_ensemble\u001B[1;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001B[0m\n\u001B[0;32m   2369\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_rows_val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(X_val)\n\u001B[0;32m   2370\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_cols_train \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mlist\u001B[39m(X\u001B[38;5;241m.\u001B[39mcolumns))\n\u001B[1;32m-> 2371\u001B[0m model_names_fit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_multi_levels(\n\u001B[0;32m   2372\u001B[0m     X,\n\u001B[0;32m   2373\u001B[0m     y,\n\u001B[0;32m   2374\u001B[0m     hyperparameters\u001B[38;5;241m=\u001B[39mhyperparameters,\n\u001B[0;32m   2375\u001B[0m     X_val\u001B[38;5;241m=\u001B[39mX_val,\n\u001B[0;32m   2376\u001B[0m     y_val\u001B[38;5;241m=\u001B[39my_val,\n\u001B[0;32m   2377\u001B[0m     X_unlabeled\u001B[38;5;241m=\u001B[39mX_unlabeled,\n\u001B[0;32m   2378\u001B[0m     level_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   2379\u001B[0m     level_end\u001B[38;5;241m=\u001B[39mnum_stack_levels \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   2380\u001B[0m     time_limit\u001B[38;5;241m=\u001B[39mtime_limit,\n\u001B[0;32m   2381\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2382\u001B[0m )\n\u001B[0;32m   2383\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_model_names()) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   2384\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAutoGluon did not successfully train any models\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:395\u001B[0m, in \u001B[0;36mAbstractTrainer.train_multi_levels\u001B[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001B[0m\n\u001B[0;32m    393\u001B[0m         core_kwargs_level[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtime_limit\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m core_kwargs_level\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtime_limit\u001B[39m\u001B[38;5;124m\"\u001B[39m, time_limit_core)\n\u001B[0;32m    394\u001B[0m         aux_kwargs_level[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtime_limit\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m aux_kwargs_level\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtime_limit\u001B[39m\u001B[38;5;124m\"\u001B[39m, time_limit_aux)\n\u001B[1;32m--> 395\u001B[0m     base_model_names, aux_models \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack_new_level\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    396\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    397\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    398\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    399\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    400\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX_unlabeled\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_unlabeled\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    401\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhyperparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    402\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    403\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbase_model_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_model_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    404\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcore_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcore_kwargs_level\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    405\u001B[0m \u001B[43m        \u001B[49m\u001B[43maux_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maux_kwargs_level\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    406\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname_suffix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname_suffix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    407\u001B[0m \u001B[43m        \u001B[49m\u001B[43minfer_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    408\u001B[0m \u001B[43m        \u001B[49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    409\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    410\u001B[0m     model_names_fit \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m base_model_names \u001B[38;5;241m+\u001B[39m aux_models\n\u001B[0;32m    411\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_best \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(model_names_fit) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:539\u001B[0m, in \u001B[0;36mAbstractTrainer.stack_new_level\u001B[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size)\u001B[0m\n\u001B[0;32m    537\u001B[0m     core_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname_suffix\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m core_kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname_suffix\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m+\u001B[39m name_suffix\n\u001B[0;32m    538\u001B[0m     aux_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname_suffix\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m aux_kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname_suffix\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m+\u001B[39m name_suffix\n\u001B[1;32m--> 539\u001B[0m core_models \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstack_new_level_core(\n\u001B[0;32m    540\u001B[0m     X\u001B[38;5;241m=\u001B[39mX,\n\u001B[0;32m    541\u001B[0m     y\u001B[38;5;241m=\u001B[39my,\n\u001B[0;32m    542\u001B[0m     X_val\u001B[38;5;241m=\u001B[39mX_val,\n\u001B[0;32m    543\u001B[0m     y_val\u001B[38;5;241m=\u001B[39my_val,\n\u001B[0;32m    544\u001B[0m     X_unlabeled\u001B[38;5;241m=\u001B[39mX_unlabeled,\n\u001B[0;32m    545\u001B[0m     models\u001B[38;5;241m=\u001B[39mmodels,\n\u001B[0;32m    546\u001B[0m     level\u001B[38;5;241m=\u001B[39mlevel,\n\u001B[0;32m    547\u001B[0m     infer_limit\u001B[38;5;241m=\u001B[39minfer_limit,\n\u001B[0;32m    548\u001B[0m     infer_limit_batch_size\u001B[38;5;241m=\u001B[39minfer_limit_batch_size,\n\u001B[0;32m    549\u001B[0m     base_model_names\u001B[38;5;241m=\u001B[39mbase_model_names,\n\u001B[0;32m    550\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcore_kwargs,\n\u001B[0;32m    551\u001B[0m )\n\u001B[0;32m    553\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m X_val \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    554\u001B[0m     aux_models \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstack_new_level_aux(\n\u001B[0;32m    555\u001B[0m         X\u001B[38;5;241m=\u001B[39mX, y\u001B[38;5;241m=\u001B[39my, base_model_names\u001B[38;5;241m=\u001B[39mcore_models, level\u001B[38;5;241m=\u001B[39mlevel \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, infer_limit\u001B[38;5;241m=\u001B[39minfer_limit, infer_limit_batch_size\u001B[38;5;241m=\u001B[39minfer_limit_batch_size, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39maux_kwargs\n\u001B[0;32m    556\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:673\u001B[0m, in \u001B[0;36mAbstractTrainer.stack_new_level_core\u001B[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, included_model_types, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001B[0m\n\u001B[0;32m    670\u001B[0m fit_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(num_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_classes)\n\u001B[0;32m    672\u001B[0m \u001B[38;5;66;03m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001B[39;00m\n\u001B[1;32m--> 673\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_multi(\n\u001B[0;32m    674\u001B[0m     X\u001B[38;5;241m=\u001B[39mX_init,\n\u001B[0;32m    675\u001B[0m     y\u001B[38;5;241m=\u001B[39my,\n\u001B[0;32m    676\u001B[0m     X_val\u001B[38;5;241m=\u001B[39mX_val,\n\u001B[0;32m    677\u001B[0m     y_val\u001B[38;5;241m=\u001B[39my_val,\n\u001B[0;32m    678\u001B[0m     X_unlabeled\u001B[38;5;241m=\u001B[39mX_unlabeled,\n\u001B[0;32m    679\u001B[0m     models\u001B[38;5;241m=\u001B[39mmodels,\n\u001B[0;32m    680\u001B[0m     level\u001B[38;5;241m=\u001B[39mlevel,\n\u001B[0;32m    681\u001B[0m     stack_name\u001B[38;5;241m=\u001B[39mstack_name,\n\u001B[0;32m    682\u001B[0m     compute_score\u001B[38;5;241m=\u001B[39mcompute_score,\n\u001B[0;32m    683\u001B[0m     fit_kwargs\u001B[38;5;241m=\u001B[39mfit_kwargs,\n\u001B[0;32m    684\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    685\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2321\u001B[0m, in \u001B[0;36mAbstractTrainer._train_multi\u001B[1;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001B[0m\n\u001B[0;32m   2319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_repeat_start \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   2320\u001B[0m     time_start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m-> 2321\u001B[0m     model_names_trained \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_multi_initial(\n\u001B[0;32m   2322\u001B[0m         X\u001B[38;5;241m=\u001B[39mX,\n\u001B[0;32m   2323\u001B[0m         y\u001B[38;5;241m=\u001B[39my,\n\u001B[0;32m   2324\u001B[0m         models\u001B[38;5;241m=\u001B[39mmodels,\n\u001B[0;32m   2325\u001B[0m         k_fold\u001B[38;5;241m=\u001B[39mk_fold,\n\u001B[0;32m   2326\u001B[0m         n_repeats\u001B[38;5;241m=\u001B[39mn_repeats_initial,\n\u001B[0;32m   2327\u001B[0m         hyperparameter_tune_kwargs\u001B[38;5;241m=\u001B[39mhyperparameter_tune_kwargs,\n\u001B[0;32m   2328\u001B[0m         feature_prune_kwargs\u001B[38;5;241m=\u001B[39mfeature_prune_kwargs,\n\u001B[0;32m   2329\u001B[0m         time_limit\u001B[38;5;241m=\u001B[39mtime_limit,\n\u001B[0;32m   2330\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2331\u001B[0m     )\n\u001B[0;32m   2332\u001B[0m     n_repeat_start \u001B[38;5;241m=\u001B[39m n_repeats_initial\n\u001B[0;32m   2333\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m time_limit \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2170\u001B[0m, in \u001B[0;36mAbstractTrainer._train_multi_initial\u001B[1;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m   2168\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2169\u001B[0m     time_ratio \u001B[38;5;241m=\u001B[39m hpo_time_ratio \u001B[38;5;28;01mif\u001B[39;00m hpo_enabled \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 2170\u001B[0m     models \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_multi_fold(\n\u001B[0;32m   2171\u001B[0m         models\u001B[38;5;241m=\u001B[39mmodels,\n\u001B[0;32m   2172\u001B[0m         hyperparameter_tune_kwargs\u001B[38;5;241m=\u001B[39mhyperparameter_tune_kwargs,\n\u001B[0;32m   2173\u001B[0m         k_fold_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m   2174\u001B[0m         k_fold_end\u001B[38;5;241m=\u001B[39mk_fold,\n\u001B[0;32m   2175\u001B[0m         n_repeats\u001B[38;5;241m=\u001B[39mn_repeats,\n\u001B[0;32m   2176\u001B[0m         n_repeat_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m   2177\u001B[0m         time_limit\u001B[38;5;241m=\u001B[39mtime_limit,\n\u001B[0;32m   2178\u001B[0m         time_split\u001B[38;5;241m=\u001B[39mtime_split,\n\u001B[0;32m   2179\u001B[0m         time_ratio\u001B[38;5;241m=\u001B[39mtime_ratio,\n\u001B[0;32m   2180\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_args,\n\u001B[0;32m   2181\u001B[0m     )\n\u001B[0;32m   2183\u001B[0m multi_fold_time_elapsed \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m multi_fold_time_start\n\u001B[0;32m   2184\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m time_limit \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2278\u001B[0m, in \u001B[0;36mAbstractTrainer._train_multi_fold\u001B[1;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m   2276\u001B[0m         time_start_model \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m   2277\u001B[0m         time_left \u001B[38;5;241m=\u001B[39m time_limit \u001B[38;5;241m-\u001B[39m (time_start_model \u001B[38;5;241m-\u001B[39m time_start)\n\u001B[1;32m-> 2278\u001B[0m model_name_trained_lst \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_single_full(\n\u001B[0;32m   2279\u001B[0m     X, y, model, time_limit\u001B[38;5;241m=\u001B[39mtime_left, hyperparameter_tune_kwargs\u001B[38;5;241m=\u001B[39mhyperparameter_tune_kwargs_model, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m   2280\u001B[0m )\n\u001B[0;32m   2282\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[0;32m   2283\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m model\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2051\u001B[0m, in \u001B[0;36mAbstractTrainer._train_single_full\u001B[1;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, **kwargs)\u001B[0m\n\u001B[0;32m   2047\u001B[0m         bagged_model_fit_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_bagged_model_fit_kwargs(\n\u001B[0;32m   2048\u001B[0m             k_fold\u001B[38;5;241m=\u001B[39mk_fold, k_fold_start\u001B[38;5;241m=\u001B[39mk_fold_start, k_fold_end\u001B[38;5;241m=\u001B[39mk_fold_end, n_repeats\u001B[38;5;241m=\u001B[39mn_repeats, n_repeat_start\u001B[38;5;241m=\u001B[39mn_repeat_start\n\u001B[0;32m   2049\u001B[0m         )\n\u001B[0;32m   2050\u001B[0m         model_fit_kwargs\u001B[38;5;241m.\u001B[39mupdate(bagged_model_fit_kwargs)\n\u001B[1;32m-> 2051\u001B[0m     model_names_trained \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_and_save(\n\u001B[0;32m   2052\u001B[0m         X\u001B[38;5;241m=\u001B[39mX,\n\u001B[0;32m   2053\u001B[0m         y\u001B[38;5;241m=\u001B[39my,\n\u001B[0;32m   2054\u001B[0m         model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m   2055\u001B[0m         X_val\u001B[38;5;241m=\u001B[39mX_val,\n\u001B[0;32m   2056\u001B[0m         y_val\u001B[38;5;241m=\u001B[39my_val,\n\u001B[0;32m   2057\u001B[0m         X_unlabeled\u001B[38;5;241m=\u001B[39mX_unlabeled,\n\u001B[0;32m   2058\u001B[0m         stack_name\u001B[38;5;241m=\u001B[39mstack_name,\n\u001B[0;32m   2059\u001B[0m         level\u001B[38;5;241m=\u001B[39mlevel,\n\u001B[0;32m   2060\u001B[0m         compute_score\u001B[38;5;241m=\u001B[39mcompute_score,\n\u001B[0;32m   2061\u001B[0m         total_resources\u001B[38;5;241m=\u001B[39mtotal_resources,\n\u001B[0;32m   2062\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_fit_kwargs,\n\u001B[0;32m   2063\u001B[0m     )\n\u001B[0;32m   2064\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave()\n\u001B[0;32m   2065\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model_names_trained\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1733\u001B[0m, in \u001B[0;36mAbstractTrainer._train_and_save\u001B[1;34m(self, X, y, model, X_val, y_val, stack_name, level, compute_score, total_resources, **model_fit_kwargs)\u001B[0m\n\u001B[0;32m   1731\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_single(X_w_pseudo, y_w_pseudo, model, X_val, y_val, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_fit_kwargs)\n\u001B[0;32m   1732\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1733\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_single(X, y, model, X_val, y_val, total_resources\u001B[38;5;241m=\u001B[39mtotal_resources, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_fit_kwargs)\n\u001B[0;32m   1735\u001B[0m fit_end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m   1736\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight_evaluation:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1684\u001B[0m, in \u001B[0;36mAbstractTrainer._train_single\u001B[1;34m(self, X, y, model, X_val, y_val, total_resources, **model_fit_kwargs)\u001B[0m\n\u001B[0;32m   1679\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_train_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, model: AbstractModel, X_val\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, y_val\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, total_resources\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_fit_kwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m AbstractModel:\n\u001B[0;32m   1680\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1681\u001B[0m \u001B[38;5;124;03m    Trains model but does not add the trained model to this Trainer.\u001B[39;00m\n\u001B[0;32m   1682\u001B[0m \u001B[38;5;124;03m    Returns trained model object.\u001B[39;00m\n\u001B[0;32m   1683\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1684\u001B[0m     model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mfit(X\u001B[38;5;241m=\u001B[39mX, y\u001B[38;5;241m=\u001B[39my, X_val\u001B[38;5;241m=\u001B[39mX_val, y_val\u001B[38;5;241m=\u001B[39my_val, total_resources\u001B[38;5;241m=\u001B[39mtotal_resources, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_fit_kwargs)\n\u001B[0;32m   1685\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:829\u001B[0m, in \u001B[0;36mAbstractModel.fit\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m    827\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidate_fit_resources(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_fit_memory_usage(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 829\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    830\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m out \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    831\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py:169\u001B[0m, in \u001B[0;36mStackerEnsembleModel._fit\u001B[1;34m(self, X, y, compute_base_preds, time_limit, **kwargs)\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m time_limit \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    168\u001B[0m     time_limit \u001B[38;5;241m=\u001B[39m time_limit \u001B[38;5;241m-\u001B[39m (time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time)\n\u001B[1;32m--> 169\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m_fit(X\u001B[38;5;241m=\u001B[39mX, y\u001B[38;5;241m=\u001B[39my, time_limit\u001B[38;5;241m=\u001B[39mtime_limit, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py:266\u001B[0m, in \u001B[0;36mBaggedEnsembleModel._fit\u001B[1;34m(self, X, y, X_val, y_val, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, groups, _skip_oof, **kwargs)\u001B[0m\n\u001B[0;32m    264\u001B[0m         \u001B[38;5;66;03m# Reserve time for final refit model\u001B[39;00m\n\u001B[0;32m    265\u001B[0m         kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtime_limit\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtime_limit\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m*\u001B[39m folds_to_fit \u001B[38;5;241m/\u001B[39m (folds_to_fit \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1.2\u001B[39m)\n\u001B[1;32m--> 266\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_folds(\n\u001B[0;32m    267\u001B[0m     X\u001B[38;5;241m=\u001B[39mX,\n\u001B[0;32m    268\u001B[0m     y\u001B[38;5;241m=\u001B[39my,\n\u001B[0;32m    269\u001B[0m     model_base\u001B[38;5;241m=\u001B[39mmodel_base,\n\u001B[0;32m    270\u001B[0m     X_pseudo\u001B[38;5;241m=\u001B[39mX_pseudo,\n\u001B[0;32m    271\u001B[0m     y_pseudo\u001B[38;5;241m=\u001B[39my_pseudo,\n\u001B[0;32m    272\u001B[0m     k_fold\u001B[38;5;241m=\u001B[39mk_fold,\n\u001B[0;32m    273\u001B[0m     k_fold_start\u001B[38;5;241m=\u001B[39mk_fold_start,\n\u001B[0;32m    274\u001B[0m     k_fold_end\u001B[38;5;241m=\u001B[39mk_fold_end,\n\u001B[0;32m    275\u001B[0m     n_repeats\u001B[38;5;241m=\u001B[39mn_repeats,\n\u001B[0;32m    276\u001B[0m     n_repeat_start\u001B[38;5;241m=\u001B[39mn_repeat_start,\n\u001B[0;32m    277\u001B[0m     save_folds\u001B[38;5;241m=\u001B[39msave_bag_folds,\n\u001B[0;32m    278\u001B[0m     groups\u001B[38;5;241m=\u001B[39mgroups,\n\u001B[0;32m    279\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    280\u001B[0m )\n\u001B[0;32m    281\u001B[0m \u001B[38;5;66;03m# FIXME: Cleanup self\u001B[39;00m\n\u001B[0;32m    282\u001B[0m \u001B[38;5;66;03m# FIXME: Support `can_refit_full=False` models\u001B[39;00m\n\u001B[0;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m refit_folds:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py:592\u001B[0m, in \u001B[0;36mBaggedEnsembleModel._fit_folds\u001B[1;34m(self, X, y, model_base, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, time_limit, sample_weight, save_folds, groups, num_cpus, num_gpus, **kwargs)\u001B[0m\n\u001B[0;32m    590\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fold_fit_args \u001B[38;5;129;01min\u001B[39;00m fold_fit_args_list:\n\u001B[0;32m    591\u001B[0m     fold_fitting_strategy\u001B[38;5;241m.\u001B[39mschedule_fold_model_fit(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfold_fit_args)\n\u001B[1;32m--> 592\u001B[0m \u001B[43mfold_fitting_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mafter_all_folds_scheduled\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    594\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model \u001B[38;5;129;01min\u001B[39;00m models:\n\u001B[0;32m    595\u001B[0m     \u001B[38;5;66;03m# No need to add child times or save child here as this already occurred in the fold_fitting_strategy\u001B[39;00m\n\u001B[0;32m    596\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_child(model\u001B[38;5;241m=\u001B[39mmodel, add_child_times\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py:538\u001B[0m, in \u001B[0;36mParallelFoldFittingStrategy.after_all_folds_scheduled\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    536\u001B[0m unfinished \u001B[38;5;241m=\u001B[39m job_refs\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m unfinished:\n\u001B[1;32m--> 538\u001B[0m     finished, unfinished \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43munfinished\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_returns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    539\u001B[0m     finished \u001B[38;5;241m=\u001B[39m finished[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    540\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py:105\u001B[0m, in \u001B[0;36mclient_mode_hook.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    103\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minit\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m is_client_mode_enabled_by_default:\n\u001B[0;32m    104\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(ray, func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 105\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\ray\\_private\\worker.py:2578\u001B[0m, in \u001B[0;36mwait\u001B[1;34m(object_refs, num_returns, timeout, fetch_local)\u001B[0m\n\u001B[0;32m   2576\u001B[0m timeout \u001B[38;5;241m=\u001B[39m timeout \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m10\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m6\u001B[39m\n\u001B[0;32m   2577\u001B[0m timeout_milliseconds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m)\n\u001B[1;32m-> 2578\u001B[0m ready_ids, remaining_ids \u001B[38;5;241m=\u001B[39m \u001B[43mworker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcore_worker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2579\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobject_refs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2580\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_returns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2581\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout_milliseconds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2582\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_task_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2583\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfetch_local\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2584\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2585\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ready_ids, remaining_ids\n",
      "File \u001B[1;32mpython\\ray\\_raylet.pyx:1833\u001B[0m, in \u001B[0;36mray._raylet.CoreWorker.wait\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpython\\ray\\_raylet.pyx:199\u001B[0m, in \u001B[0;36mray._raylet.check_status\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "predictor_A = TabularPredictor(\n",
    "    label='pv_measurement',\n",
    "    eval_metric=\"mean_absolute_error\"\n",
    ").fit(\n",
    "    train_data=combined_A,\n",
    "    tuning_data=tuning_data_A,\n",
    "    use_bag_holdout=True,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=2,\n",
    "    num_stack_levels=2,\n",
    "    time_limit=time_limit,\n",
    "    presets=\"best_quality\",\n",
    "    excluded_model_types=['KNN']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961a928-3931-465c-924a-d6030ca0a0f3",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.316918900Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor_B = TabularPredictor(\n",
    "    label='pv_measurement',\n",
    "    eval_metric=\"mean_absolute_error\"\n",
    ").fit(\n",
    "    train_data=combined_B,\n",
    "    tuning_data=tuning_data_B,\n",
    "    use_bag_holdout=True,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=2,\n",
    "    num_stack_levels=2,\n",
    "    time_limit=time_limit,\n",
    "    presets=\"best_quality\",\n",
    "    excluded_model_types=['KNN']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e18a77-3142-4682-82e2-d9fd6c53a42b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.320920Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor_C = TabularPredictor(\n",
    "    label='pv_measurement',\n",
    "    eval_metric=\"mean_absolute_error\"\n",
    ").fit(\n",
    "    train_data=combined_C,\n",
    "    tuning_data=tuning_data_C,\n",
    "    use_bag_holdout=True,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=2,\n",
    "    num_stack_levels=2,\n",
    "    time_limit=time_limit,\n",
    "    presets=\"best_quality\",\n",
    "    excluded_model_types=['KNN']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915985ab",
   "metadata": {},
   "source": [
    "### AutoGluon predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c814a68",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.323918200Z"
    }
   },
   "outputs": [],
   "source": [
    "A_prediction = predictor_A.predict(test_A)\n",
    "B_prediction = predictor_B.predict(test_B)\n",
    "C_prediction = predictor_C.predict(test_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312351cc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.327918700Z"
    }
   },
   "outputs": [],
   "source": [
    "autogloun_predictions = np.concatenate([A_prediction, B_prediction, C_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CatBoost\n",
    "## Preprocessing\n",
    "Resetting data objects"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c481a8ca6631fc9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reset_data_objects(location: LocationData) -> None:\n",
    "    location.train_data = location.original_train_data.copy()\n",
    "    location.target_data = location.original_target_data.copy()\n",
    "    location.test_data = location.original_test_data.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.331919400Z"
    }
   },
   "id": "647ff8ee65e8503d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reset_data_objects(A)\n",
    "reset_data_objects(B)\n",
    "reset_data_objects(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.335919300Z"
    }
   },
   "id": "7a53b5de76fb9a40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.339919300Z"
    }
   },
   "id": "102748ff31a74103"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running preprocessing pipeline for CatBoost model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d830517c6140c225"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=A.train_data, target_data=A.target_data, is_cat_boost=True)\n",
    "test_data = pre_process_test_data(test_data=A.test_data, is_cat_boost=True)\n",
    "\n",
    "A.train_data = train_data\n",
    "A.target_data = target_data\n",
    "A.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.343922500Z"
    }
   },
   "id": "2582b8b7f22b5750"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=B.train_data, target_data=B.target_data, is_cat_boost=True)\n",
    "test_data = pre_process_test_data(test_data=B.test_data, is_cat_boost=True)\n",
    "\n",
    "B.train_data = train_data\n",
    "B.target_data = target_data\n",
    "B.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.347919Z"
    }
   },
   "id": "9db8c1ae2d5eb2b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=C.train_data, target_data=C.target_data, is_cat_boost=True)\n",
    "test_data = pre_process_test_data(test_data=C.test_data, is_cat_boost=True)\n",
    "\n",
    "C.train_data = train_data\n",
    "C.target_data = target_data\n",
    "C.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.351923600Z"
    }
   },
   "id": "18b136e2dbb714cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.355919100Z"
    }
   },
   "id": "2dd0b3962d03ffa4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbf5195d1813b58c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_sinusoidal_features(df: pd.DataFrame, hour_col: str = 'hour', day_col: str = 'day_of_year') -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    dates = df.index.values\n",
    "    df['date_forecast'] = dates\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Create hour and day_of_year columns\n",
    "    df['hour'] = df['date_forecast'].dt.hour\n",
    "    df['day_of_year'] = df['date_forecast'].dt.dayofyear\n",
    "\n",
    "    # Extract week number using isocalendar()\n",
    "    df['week_of_year'] = df['date_forecast'].dt.isocalendar().week\n",
    "    df['month'] = df['date_forecast'].dt.month\n",
    "\n",
    "    # Constants for sinusoidal functions\n",
    "    hours_in_day = 24\n",
    "    days_in_year = 365.25  # Accounting for leap years\n",
    "    weeks_in_year = 52\n",
    "    months_in_year = 12\n",
    "\n",
    "    # Create sinusoidal features based on the hour of the day\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * (df[hour_col] - 18) / hours_in_day)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * (df[hour_col] - 18) / hours_in_day)\n",
    "\n",
    "    # Create sinusoidal features based on the day of the year\n",
    "    df['sin_day'] = np.sin(2 * np.pi * (df[day_col] - 355) / days_in_year)\n",
    "    df['cos_day'] = np.cos(2 * np.pi * (df[day_col] - 355) / days_in_year)\n",
    "\n",
    "    # Create sinusoidal features based on the week of the year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['week_of_year'] / weeks_in_year)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['week_of_year'] / weeks_in_year)\n",
    "\n",
    "    # Create sinusoidal features based on the month\n",
    "    df['sin_month'] = np.sin(2 * np.pi * df['month'] / months_in_year)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * df['month'] / months_in_year)\n",
    "\n",
    "    dates = df['date_forecast'].to_numpy()\n",
    "    df.index = dates\n",
    "    df.drop(columns=['hour', 'day_of_year', 'week_of_year', 'month', 'date_forecast'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_sinusoidal_features_for_location(location: LocationData) -> None:\n",
    "    train_data = location.train_data.copy()\n",
    "    test_data = location.test_data.copy()\n",
    "\n",
    "    train_data = create_sinusoidal_features(train_data)\n",
    "    test_data = create_sinusoidal_features(test_data)\n",
    "\n",
    "    train_data.sort_index(inplace=True)\n",
    "    test_data.sort_index(inplace=True)\n",
    "\n",
    "    location.train_data = train_data\n",
    "    location.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.359918500Z"
    }
   },
   "id": "6f48677ca909d56f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "create_sinusoidal_features_for_location(A)\n",
    "create_sinusoidal_features_for_location(B)\n",
    "create_sinusoidal_features_for_location(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.363918900Z"
    }
   },
   "id": "6d2159c768e2285"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.366917100Z"
    }
   },
   "id": "f50b53df83c83aa6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining functions for CatBoost model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "871b8f994d7bd8e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val):\n",
    "    predictions = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, predictions)\n",
    "    return mae\n",
    "\n",
    "\n",
    "def train_catboost_ensemble(location, N_MODELS=10):\n",
    "    train_data = location.train_data\n",
    "    target_data = location.target_data\n",
    "\n",
    "    total_length = len(train_data)\n",
    "    val_size = int(0.1 * total_length)\n",
    "\n",
    "    mae_list = []\n",
    "    models = []\n",
    "    depth_values = [7, 7, 8, 8, 8, 8, 9, 9, 9, 10]\n",
    "    learning_rate_values = [0.01, 0.04, 0.01, 0.02, 0.03, 0.04, 0.02, 0.03, 0.04, 0.03]\n",
    "    summer_evaluated = []\n",
    "    train_dates = train_data.index.values\n",
    "\n",
    "    for i in range(N_MODELS):\n",
    "        val_start_idx = int((i / N_MODELS) * (total_length - val_size))\n",
    "\n",
    "        X_train = pd.concat([train_data[:val_start_idx], train_data[val_start_idx + val_size:]], axis=0)\n",
    "        y_train = pd.concat([target_data[:val_start_idx], target_data[val_start_idx + val_size:]], axis=0)\n",
    "        X_val = train_data[val_start_idx:val_start_idx + val_size]\n",
    "        y_val = target_data[val_start_idx:val_start_idx + val_size]\n",
    "\n",
    "        date_at_middle_of_val = pd.to_datetime(X_train.index[val_start_idx + int(val_size / 2)])\n",
    "        evaluation_month = date_at_middle_of_val.month + 7\n",
    "        if evaluation_month > 12:\n",
    "            evaluation_month -= 12\n",
    "\n",
    "        summer_months = [5, 6, 7]\n",
    "        is_summer = evaluation_month in summer_months\n",
    "        summer_evaluated.append(is_summer)\n",
    "\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=20000,\n",
    "            depth=depth_values[i],\n",
    "            learning_rate=learning_rate_values[i],\n",
    "            loss_function='MAE',\n",
    "            od_type='Iter',\n",
    "            od_wait=100,\n",
    "            l2_leaf_reg=3,\n",
    "            random_seed=42,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # Fit the model using the validation set for early stopping\n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True, early_stopping_rounds=100)\n",
    "\n",
    "        # After training, evaluate the final model on the validation set\n",
    "        final_mae = evaluate_model(model, X_val, y_val)\n",
    "        print(f\"Model {i + 1} M[{evaluation_month}]: MAE:\\t{round(final_mae, 4)}\")\n",
    "\n",
    "        mae_list.append(final_mae)\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    print(f\"Average MAE:\\t{round(np.mean(mae_list), 4)}\")\n",
    "\n",
    "    return models, summer_evaluated\n",
    "\n",
    "\n",
    "def get_ensemble_predictions(models, test_data, summer_evaluated):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        predictions = model.predict(test_data)\n",
    "        if summer_evaluated[i]:\n",
    "            all_predictions.extend([\n",
    "                predictions,\n",
    "                predictions,\n",
    "                predictions,\n",
    "                predictions,\n",
    "            ])\n",
    "        else:\n",
    "            all_predictions.append(predictions)\n",
    "\n",
    "    ensemble_predictions = np.mean(all_predictions, axis=0)\n",
    "    return ensemble_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.369919200Z"
    }
   },
   "id": "e04fc78b95151b8e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training CatBoost models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62597d58cb55938d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "A_models, A_summer_evaluated = train_catboost_ensemble(location=A)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.371916600Z"
    }
   },
   "id": "9bd8e91049e19beb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B_models, B_summer_evaluated = train_catboost_ensemble(location=B)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.374922100Z"
    }
   },
   "id": "eb284a12468226b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "C_models, C_summer_evaluated = train_catboost_ensemble(location=C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.376922400Z"
    }
   },
   "id": "ae149c86170b5b16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CatBoost predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a05099bb51fcec9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "A_prediction = get_ensemble_predictions(A_models, A.test_data, A_summer_evaluated)\n",
    "B_prediction = get_ensemble_predictions(B_models, B.test_data, B_summer_evaluated)\n",
    "C_prediction = get_ensemble_predictions(C_models, C.test_data, C_summer_evaluated)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.378922100Z"
    }
   },
   "id": "9f0373b4e9113427"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction_list = [A_prediction, B_prediction, C_prediction]\n",
    "predictions = np.concatenate(prediction_list)\n",
    "catboost_predictions = predictions.clip(min=0, max=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.379922200Z"
    }
   },
   "id": "f8b23491e45918e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction_list[autogloun_predictions, catboost_predictions]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.381922200Z"
    }
   },
   "id": "5ead482e9a4e7e44"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b91d04668e2728d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17eac1d7b99002",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.383922Z"
    }
   },
   "outputs": [],
   "source": [
    "for prediction in prediction_list:\n",
    "    plt.figure(figsize=(30, 7))\n",
    "    plt.plot(catboost_predictions, 'b-')\n",
    "    plt.plot(autogloun_predictions, 'r-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c29779",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.387922900Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction_list = [np.mean(prediction_list, axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd360cbb63a5fc",
   "metadata": {},
   "source": [
    "# Saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4e0f1c8748630",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.391434100Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "\n",
    "def create_csv(*prediction_list: np.ndarray) -> None:\n",
    "    prediction_path: str = os.path.join(\n",
    "        DATA_DIR,\n",
    "        \"predictions\",\n",
    "        time.strftime(\"%Y%m%d-%H%M%S\") + \".csv\",\n",
    "    )\n",
    "\n",
    "    output: np.ndarray = np.concatenate(prediction_list)\n",
    "    output = output.clip(min=0, max=None)\n",
    "    indexes = np.arange(0, len(output), 1, dtype=int)\n",
    "\n",
    "    data = {\n",
    "        \"id\": indexes,\n",
    "        \"prediction\": output\n",
    "    }\n",
    "\n",
    "    dataframe: pd.DataFrame = pd.DataFrame(data)\n",
    "    dataframe.to_csv(prediction_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4562075a901b4",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-11-12T16:52:01.393435700Z"
    }
   },
   "outputs": [],
   "source": [
    "create_csv(*prediction_list)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
