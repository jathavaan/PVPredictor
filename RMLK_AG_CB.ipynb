{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d9f234-58de-4fc9-9f4e-443ea8265844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ba8bc12eaef6f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850e2dfc3bb16aa",
   "metadata": {},
   "source": [
    "Setting rules for libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081b47cf627a139",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Warnings\n",
    "\n",
    "# Pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f768895a70a9bd57",
   "metadata": {},
   "source": [
    "# Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf20b5c6b0ced7a4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Features to remove\n",
    "DATE_CALC: str = \"date_calc\"\n",
    "DATE_FORECAST: str = \"date_forecast\"\n",
    "ABSOLUTE_HUMIDITY: str = 'absolute_humidity_2m:gm3'\n",
    "AIR_DENSITY: str = 'air_density_2m:kgm3'\n",
    "CEILING_HEIGHT: str = 'ceiling_height_agl:m'\n",
    "CLEAR_SKY_ENERGY: str = 'clear_sky_energy_1h:J'\n",
    "CLEAR_SKY_RAD: str = 'clear_sky_rad:W'\n",
    "CLOUD_BASE: str = 'cloud_base_agl:m'\n",
    "DEW_OR_RIME: str = 'dew_or_rime:idx'\n",
    "DEW_POINT: str = 'dew_point_2m:K'\n",
    "DIFFUSE_RAD: str = 'diffuse_rad:W'\n",
    "DIFFUSE_RAD_1H: str = 'diffuse_rad_1h:J'\n",
    "DIRECT_RAD: str = 'direct_rad:W'\n",
    "DIRECT_RAD_1H: str = 'direct_rad_1h:J'\n",
    "EFFECTIVE_CLOUD_COVER: str = 'effective_cloud_cover:p'\n",
    "ELEVATION: str = 'elevation:m'\n",
    "FRESH_SNOW_12H: str = 'fresh_snow_12h:cm'\n",
    "FRESH_SNOW_1H: str = 'fresh_snow_1h:cm'\n",
    "FRESH_SNOW_24H: str = 'fresh_snow_24h:cm'\n",
    "FRESH_SNOW_3H: str = 'fresh_snow_3h:cm'\n",
    "FRESH_SNOW_6H: str = 'fresh_snow_6h:cm'\n",
    "IS_DAY: str = 'is_day:idx'\n",
    "IS_IN_SHADOW: str = 'is_in_shadow:idx'\n",
    "MSL_PRESSURE: str = 'msl_pressure:hPa'\n",
    "PRECIP_5MIN: str = 'precip_5min:mm'\n",
    "PRECIP_TYPE_5MIN: str = 'precip_type_5min:idx'\n",
    "PRESSURE_100M: str = 'pressure_100m:hPa'\n",
    "PRESSURE_50M: str = 'pressure_50m:hPa'\n",
    "PROB_RIME: str = 'prob_rime:p'\n",
    "RAIN_WATER: str = 'rain_water:kgm2'\n",
    "RELATIVE_HUMIDITY: str = 'relative_humidity_1000hPa:p'\n",
    "SFC_PRESSURE: str = 'sfc_pressure:hPa'\n",
    "SNOW_DENSITY: str = 'snow_density:kgm3'\n",
    "SNOW_DEPTH: str = 'snow_depth:cm'\n",
    "SNOW_DRIFT: str = 'snow_drift:idx'\n",
    "SNOW_MELT_10MIN: str = 'snow_melt_10min:mm'\n",
    "SNOW_WATER: str = 'snow_water:kgm2'\n",
    "SUN_AZIMUTH: str = 'sun_azimuth:d'\n",
    "SUN_ELEVATION: str = 'sun_elevation:d'\n",
    "SUPER_COOLED_LIQUID_WATER: str = 'super_cooled_liquid_water:kgm2'\n",
    "T_1000HPA: str = 't_1000hPa:K'\n",
    "TOTAL_CLOUD_COVER: str = 'total_cloud_cover:p'\n",
    "VISIBILITY: str = 'visibility:m'\n",
    "WIND_SPEED_10M: str = 'wind_speed_10m:ms'\n",
    "WIND_SPEED_U_10M: str = 'wind_speed_u_10m:ms'\n",
    "WIND_SPEED_V_10M: str = 'wind_speed_v_10m:ms'\n",
    "WIND_SPEED_W_1000HPA: str = 'wind_speed_w_1000hPa:ms'\n",
    "IS_ESTIMATED = 'is_estimated'\n",
    "\n",
    "TIME = 'time'\n",
    "PV_MEASUREMENT = 'pv_measurement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5747184b6faafe",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def read_parquet(filepath: str) -> pd.DataFrame:\n",
    "    dataframe: pd.DataFrame = pd.read_parquet(filepath)\n",
    "    if DATE_CALC in dataframe.columns:\n",
    "        dataframe = dataframe.drop(columns=[DATE_CALC])\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def concat_observed_and_estimated_train_data(\n",
    "        X_observed: pd.DataFrame,\n",
    "        X_estimated: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    X_observed: pd.DataFrame = X_observed.copy()\n",
    "    X_estimated: pd.DataFrame = X_estimated.copy()\n",
    "\n",
    "    assert X_observed.shape[1] == X_estimated.shape[1]\n",
    "\n",
    "    concatenated_dataframe: pd.DataFrame = pd.concat(\n",
    "        objs=[X_observed, X_estimated],\n",
    "        axis=0\n",
    "    ).sort_values(\n",
    "        by=DATE_FORECAST,\n",
    "        ascending=True\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return concatenated_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f38c501e227ecc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data transfer object for location data\n",
    "class LocationData:\n",
    "    combined_data: pd.DataFrame\n",
    "\n",
    "    def __init__(self, train_data, target_data, test_data) -> None:\n",
    "        self.train_data = train_data\n",
    "        self.target_data = target_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "        self.original_train_data = train_data.copy()\n",
    "        self.original_target_data = target_data.copy()\n",
    "        self.original_test_data = test_data.copy()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Train: {self.train_data.shape} [No. NaN: {self.train_data.isna().sum().sum()}], Target: {self.target_data.shape} [No. NaN: {self.target_data.isna().sum().sum()}], Test: {self.test_data.shape} [No. NaN: {self.test_data.isna().sum().sum()}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d61a1645354f7c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/A/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/A/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/A/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/A/X_test_estimated.parquet\")\n",
    "\n",
    "observed_train['is_estimated'] = 0\n",
    "estimated_train['is_estimated'] = 1\n",
    "test_data['is_estimated'] = 1\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6374820ca84a630",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58333e566cc3633f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/B/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/B/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/B/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/B/X_test_estimated.parquet\")\n",
    "\n",
    "observed_train['is_estimated'] = 0\n",
    "estimated_train['is_estimated'] = 1\n",
    "test_data['is_estimated'] = 1\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")\n",
    "\n",
    "B = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd3f707ed3d2f7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/C/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/C/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/C/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/C/X_test_estimated.parquet\")\n",
    "\n",
    "observed_train['is_estimated'] = 0\n",
    "estimated_train['is_estimated'] = 1\n",
    "test_data['is_estimated'] = 1\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ab86d84357bed",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d3d8fec6811b18",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0124801bff563",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Defining functions for preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f4e6db271eea0",
   "metadata": {},
   "source": [
    "Function to remove features and replace `NaN` values with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a2ddc44c56339",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_feature(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    dataframe = dataframe.drop(columns=[*features])\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def replace_nan_with_zero(dataframe: pd.DataFrame, feature: str) -> pd.DataFrame:\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe[[feature]] = dataframe[[feature]].fillna(0)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f994238640695aa",
   "metadata": {},
   "source": [
    "Function to resample data by categorizing them\n",
    "- Features to average\n",
    "- Features to pick a value\n",
    "- Features to change redefine `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0e24c1ba0fe4b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_by_selection(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "    if reduced_dataframe.isna().sum().sum() > 0:\n",
    "        print(\"[ERROR] Sample by selection got dataframe with NaN values\")\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.last()\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_by_mean(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.mean()\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_by_sum(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.sum()\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_cloud_base(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    cloud_base_frame = dataframe[[CLOUD_BASE]]\n",
    "\n",
    "    # Filling NaN values with -2_000\n",
    "    filled_frame = cloud_base_frame.copy()\n",
    "    filled_frame = filled_frame.fillna(-100_000)\n",
    "\n",
    "    # Resampling to 1H\n",
    "    resampler = filled_frame.resample('H')\n",
    "    cloud_base_frame = resampler.mean()\n",
    "\n",
    "    # Replacing negative values with -666\n",
    "    cloud_bases = cloud_base_frame[CLOUD_BASE].to_numpy()\n",
    "    negative_indexes = np.where(cloud_bases < 0)[0]\n",
    "    cloud_bases[negative_indexes] = -666\n",
    "    cloud_base_frame[CLOUD_BASE] = cloud_bases\n",
    "\n",
    "    # Dropping NaN values that were introduced by resampling\n",
    "    reduced_dataframe = cloud_base_frame.copy()\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def merge_frames(*dataframes: pd.DataFrame) -> pd.DataFrame:\n",
    "    indexes = [df.index for df in dataframes]\n",
    "\n",
    "    # Finding intersecting indexes in list of indexes, the indexes are datetime\n",
    "    intersecting_indexes = list(set(indexes[0]).intersection(*indexes))\n",
    "    filtered_dataframes = [df.loc[intersecting_indexes] for df in dataframes]\n",
    "\n",
    "    return pd.concat(filtered_dataframes, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4830f59eace7ae",
   "metadata": {},
   "source": [
    "Functions to synchronze dates in weather data and target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98926ee1a8a03451",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def synchronize_timestamps(train_data: pd.DataFrame, target_data: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # IndexError if index in both train data and target data is DateTimeIndex\n",
    "    if not isinstance(train_data.index, pd.DatetimeIndex):\n",
    "        raise IndexError(\"Train data does not have DateTimeIndex\")\n",
    "\n",
    "    if not isinstance(target_data.index, pd.DatetimeIndex):\n",
    "        raise IndexError(\"Target data does not have DateTimeIndex\")\n",
    "\n",
    "    train_data.sort_index(inplace=True)\n",
    "    target_data.sort_index(inplace=True)\n",
    "\n",
    "    train_dates = train_data.index.values\n",
    "    target_dates = target_data.index.values\n",
    "\n",
    "    # Finding intersecting dates and filtering dataframes\n",
    "    intersecting_dates = np.intersect1d(train_dates, target_dates)\n",
    "    train_data = train_data.loc[intersecting_dates]\n",
    "    target_data = target_data.loc[intersecting_dates]\n",
    "\n",
    "    return train_data, target_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c01434829e1ae",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The dateset contains many constant values for some features, any row with constant values is removed. If there is are at least 24 rows with constant values, the rows are removed.\n",
    "- -666 in cloud base should not be removed as this is added in earlier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472023231528654c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_constant_measurements(\n",
    "        train_data: pd.DataFrame,\n",
    "        target_data: pd.DataFrame,\n",
    "        THRESHOLD: int = 24\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    indexes = []\n",
    "    count = 0\n",
    "    previous_value = None\n",
    "    start_index = None\n",
    "\n",
    "    # Setting date indexes as columns and resetting index\n",
    "    train_data[DATE_FORECAST] = train_data.index.values\n",
    "    target_data[TIME] = target_data.index.values\n",
    "\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    target_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Assuming y is a DataFrame and has a column for pv_measurement values\n",
    "    pv_measurements = target_data[PV_MEASUREMENT].to_numpy()\n",
    "\n",
    "    # Identifying indexes of consecutive constant values\n",
    "    for index, value in enumerate(pv_measurements):\n",
    "        if value == previous_value:\n",
    "            count += 1\n",
    "            if count == 1:\n",
    "                start_index = index - 1\n",
    "            if count >= THRESHOLD and value > -0.1:\n",
    "                indexes.extend(range(start_index, index + 1))\n",
    "        else:\n",
    "            count = 0\n",
    "        previous_value = value\n",
    "\n",
    "    # Assuming y has a column for dates and X has a similar column to filter on\n",
    "    dates = target_data.loc[indexes, TIME].unique()\n",
    "\n",
    "    filtered_train_data = train_data[~train_data[DATE_FORECAST].isin(dates)]\n",
    "    filtered_target_data = target_data[~target_data[TIME].isin(dates)]\n",
    "\n",
    "    # Switching back to date as index\n",
    "    filtered_train_data = filtered_train_data.set_index(DATE_FORECAST)\n",
    "    filtered_target_data = filtered_target_data.set_index(TIME)\n",
    "\n",
    "    return filtered_train_data, filtered_target_data\n",
    "\n",
    "\n",
    "def drop_nan_values_from_target_data(train_data: pd.DataFrame, target_data: pd.DataFrame) -> tuple[\n",
    "    pd.DataFrame, pd.DataFrame]:\n",
    "    target_data.dropna(axis=0, how='any', inplace=True)\n",
    "    train_data, target_data = synchronize_timestamps(train_data, target_data)\n",
    "    return train_data, target_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948b908f0f59cef",
   "metadata": {},
   "source": [
    "## Adding lag features, this has to come here because of the resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1317434ce4b07e5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_lag_features(dataframe: pd.DataFrame, features: list[str], lags: list[int]) -> tuple[pd.DataFrame, list[str]]:\n",
    "    dataframe = dataframe.copy()\n",
    "    lag_feature_names: list[str] = []\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            lag_feature_name = f\"{feature}_lag_{lag}\"\n",
    "            dataframe[lag_feature_name] = dataframe[feature].shift(lag)\n",
    "            lag_feature_names.append(lag_feature_name)\n",
    "\n",
    "    return dataframe, lag_feature_names\n",
    "\n",
    "\n",
    "def add_lead_features(dataframe: pd.DataFrame, features: list[str], lags: list[int]) -> tuple[pd.DataFrame, list[str]]:\n",
    "    dataframe = dataframe.copy()\n",
    "    lead_feature_names: list[str] = []\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            lead_feature_name = f\"{feature}_lead_{lag}\"\n",
    "            dataframe[lead_feature_name] = dataframe[feature].shift(-lag)\n",
    "            lead_feature_names.append(lead_feature_name)\n",
    "\n",
    "    return dataframe, lead_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4514f22c17b74b8e",
   "metadata": {},
   "source": [
    "Pipeline for preprocessing that is common for both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c2733c9a43a644",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def common_pre_processing(dataframe: pd.DataFrame, is_cat_boost: bool) -> pd.DataFrame:\n",
    "    features_to_drop = [SNOW_DRIFT, CEILING_HEIGHT, SNOW_DENSITY]\n",
    "\n",
    "    if is_cat_boost:\n",
    "        dataframe = remove_feature(dataframe, *features_to_drop)\n",
    "\n",
    "    dataframe = dataframe.set_index(DATE_FORECAST)\n",
    "    feature_to_lag_and_lead: list[str] = [DIRECT_RAD]\n",
    "\n",
    "    # Adding lag and lead features\n",
    "    lag_features = []\n",
    "    lead_features = []\n",
    "\n",
    "    if is_cat_boost:\n",
    "        dataframe, lag_features = add_lag_features(dataframe, feature_to_lag_and_lead, [1, 2, 3])\n",
    "        dataframe, lead_features = add_lead_features(dataframe, feature_to_lag_and_lead, [1, 2, 3])\n",
    "\n",
    "    # Resample data to hours\n",
    "    selection_features = [\n",
    "        CLEAR_SKY_ENERGY,  # Maybe move\n",
    "        DIRECT_RAD_1H,  # MAYBE MOVE\n",
    "        DIFFUSE_RAD_1H,  # Maybe move\n",
    "        FRESH_SNOW_1H,\n",
    "        FRESH_SNOW_3H,\n",
    "        FRESH_SNOW_6H,\n",
    "        FRESH_SNOW_12H,\n",
    "        FRESH_SNOW_24H,\n",
    "        DEW_OR_RIME,\n",
    "        IS_DAY,\n",
    "        IS_IN_SHADOW,\n",
    "        PRECIP_TYPE_5MIN,\n",
    "        ELEVATION,\n",
    "        IS_ESTIMATED,\n",
    "        *lag_features,\n",
    "        *lead_features,\n",
    "    ]\n",
    "\n",
    "    mean_features = [\n",
    "        ABSOLUTE_HUMIDITY,\n",
    "        CLEAR_SKY_RAD,\n",
    "        DIFFUSE_RAD,\n",
    "        DIRECT_RAD,\n",
    "        AIR_DENSITY,\n",
    "        DEW_POINT,\n",
    "        MSL_PRESSURE,\n",
    "        PRESSURE_100M,\n",
    "        PRESSURE_50M,\n",
    "        SFC_PRESSURE,\n",
    "        RELATIVE_HUMIDITY,\n",
    "        T_1000HPA,\n",
    "        SUN_AZIMUTH,\n",
    "        SUN_ELEVATION,\n",
    "        EFFECTIVE_CLOUD_COVER,\n",
    "        TOTAL_CLOUD_COVER,\n",
    "        VISIBILITY,\n",
    "        WIND_SPEED_10M,\n",
    "        WIND_SPEED_U_10M,\n",
    "        WIND_SPEED_V_10M,\n",
    "        WIND_SPEED_W_1000HPA,\n",
    "        SNOW_MELT_10MIN,\n",
    "        PROB_RIME,\n",
    "        SUPER_COOLED_LIQUID_WATER,\n",
    "        SNOW_DEPTH,\n",
    "    ]\n",
    "\n",
    "    sum_features = [\n",
    "        SNOW_WATER,\n",
    "        PRECIP_5MIN,\n",
    "        RAIN_WATER,\n",
    "    ]\n",
    "\n",
    "    if not is_cat_boost:\n",
    "        for feature in features_to_drop:\n",
    "            mean_features.append(feature)\n",
    "\n",
    "    selection_df = sample_by_selection(dataframe, *selection_features)\n",
    "    mean_df = sample_by_mean(dataframe, *mean_features)\n",
    "    sum_df = sample_by_sum(dataframe, *sum_features)\n",
    "    ceiling_height_df = sample_cloud_base(dataframe)\n",
    "    dataframe = merge_frames(selection_df, mean_df, ceiling_height_df, sum_df)\n",
    "\n",
    "    if is_cat_boost:\n",
    "        categorical_features = [\n",
    "            DEW_OR_RIME,\n",
    "            IS_DAY,\n",
    "            IS_IN_SHADOW,\n",
    "            PRECIP_TYPE_5MIN,\n",
    "            ELEVATION,\n",
    "            IS_ESTIMATED,\n",
    "        ]\n",
    "\n",
    "        dataframe = dataframe.dropna(axis=0, how='any')\n",
    "        dataframe[categorical_features] = dataframe[categorical_features].astype(int)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db91786254d2a576",
   "metadata": {},
   "source": [
    "### Preprocessing pipelines for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9556d94345fac3ba",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pre_process_train_data(\n",
    "        train_data: pd.DataFrame,\n",
    "        target_data: pd.DataFrame,\n",
    "        is_cat_boost: bool\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_data = common_pre_processing(dataframe=train_data, is_cat_boost=is_cat_boost)\n",
    "\n",
    "    # Setting time as index for target data\n",
    "    target_data[TIME] = pd.to_datetime(target_data[TIME])\n",
    "    target_data = target_data.set_index(TIME)\n",
    "\n",
    "    # Preprocessing that affects both train and target data\n",
    "    train_data, target_data = synchronize_timestamps(train_data=train_data, target_data=target_data)\n",
    "    train_data, target_data = remove_constant_measurements(train_data=train_data, target_data=target_data)\n",
    "    train_data, target_data = drop_nan_values_from_target_data(train_data=train_data, target_data=target_data)\n",
    "\n",
    "    return train_data, target_data\n",
    "\n",
    "\n",
    "def pre_process_test_data(test_data: pd.DataFrame, is_cat_boost: bool) -> pd.DataFrame:\n",
    "    if is_cat_boost:\n",
    "        test_data = common_pre_processing(dataframe=test_data, is_cat_boost=is_cat_boost)\n",
    "    else:\n",
    "        test_data = test_data.set_index(DATE_FORECAST)\n",
    "        test_data = test_data.groupby(test_data.index.floor('H')).mean()\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79beb4",
   "metadata": {},
   "source": [
    "# AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad5e575852aadfb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(\n",
    "    train_data=A.train_data,\n",
    "    target_data=A.target_data,\n",
    "    is_cat_boost=False\n",
    ")\n",
    "\n",
    "test_data = pre_process_test_data(test_data=A.test_data, is_cat_boost=False)\n",
    "A.train_data = train_data\n",
    "A.target_data = target_data\n",
    "A.test_data = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a11892d7383702",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(\n",
    "    train_data=B.train_data,\n",
    "    target_data=B.target_data,\n",
    "    is_cat_boost=False\n",
    ")\n",
    "test_data = pre_process_test_data(test_data=B.test_data, is_cat_boost=False)\n",
    "B.train_data = train_data\n",
    "B.target_data = target_data\n",
    "B.test_data = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7936de9ca1f1338",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(\n",
    "    train_data=C.train_data,\n",
    "    target_data=C.target_data,\n",
    "    is_cat_boost=False\n",
    ")\n",
    "test_data = pre_process_test_data(test_data=C.test_data, is_cat_boost=False)\n",
    "C.train_data = train_data\n",
    "C.target_data = target_data\n",
    "C.test_data = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3241e6b3792ef",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a9cca-6d35-4e5a-aba2-ef43b90af3e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MINUTES: int = 1\n",
    "time_limit = int(60 * MINUTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3fd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged files required by the AutoGluon model\n",
    "def merge_train_and_target(location: LocationData) -> pd.DataFrame:\n",
    "    train_data = location.train_data.copy()\n",
    "    target_data = location.target_data.copy()\n",
    "\n",
    "    return pd.merge(left=train_data, right=target_data, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "\n",
    "def prepare_data_for_ag(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    for col in dataframe.columns:\n",
    "        if dataframe[col].dtype == 'object':\n",
    "            dataframe[col] = dataframe[col].astype(str)\n",
    "        else:\n",
    "            dataframe[col] = pd.to_numeric(dataframe[col], errors='coerce')\n",
    "    \"\"\"\n",
    "\n",
    "    if pd.api.types.is_datetime64_any_dtype(dataframe.index):\n",
    "        dataframe = dataframe.sort_index()  # Sort by date if the index is a datetime\n",
    "        dataframe = dataframe.reset_index()  # Reset index\n",
    "\n",
    "    if DATE_FORECAST in dataframe.columns:\n",
    "        dataframe = dataframe.drop(columns=[DATE_FORECAST])\n",
    "\n",
    "    if TIME in dataframe.columns:\n",
    "        dataframe = dataframe.drop(columns=[TIME])\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_A = prepare_data_for_ag(merge_train_and_target(A))\n",
    "combined_B = prepare_data_for_ag(merge_train_and_target(B))\n",
    "combined_C = prepare_data_for_ag(merge_train_and_target(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d37c15-7870-48b9-8bf9-cb85db0caa68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuning_data_A = combined_A[combined_A['is_estimated'] == 1].tail(1100)\n",
    "tuning_data_B = combined_B[combined_B['is_estimated'] == 1].tail(950)\n",
    "tuning_data_C = combined_C[combined_C['is_estimated'] == 1].tail(870)\n",
    "\n",
    "combined_A = combined_A[(~combined_A.index.isin(tuning_data_A.index))]\n",
    "combined_B = combined_B[(~combined_B.index.isin(tuning_data_B.index))]\n",
    "combined_C = combined_C[(~combined_C.index.isin(tuning_data_C.index))]\n",
    "\n",
    "test_A = prepare_data_for_ag(A.test_data)\n",
    "test_B = prepare_data_for_ag(B.test_data)\n",
    "test_C = prepare_data_for_ag(C.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c187ace-8822-4a31-b9cb-474fb0a01c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor_A = TabularPredictor(\n",
    "    label='pv_measurement',\n",
    "    eval_metric=\"mean_absolute_error\"\n",
    ").fit(\n",
    "    train_data=combined_A,\n",
    "    tuning_data=tuning_data_A,\n",
    "    use_bag_holdout=True,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=2,\n",
    "    num_stack_levels=2,\n",
    "    time_limit=time_limit,\n",
    "    presets=\"best_quality\",\n",
    "    excluded_model_types=['KNN']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961a928-3931-465c-924a-d6030ca0a0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor_B = TabularPredictor(\n",
    "    label='pv_measurement',\n",
    "    eval_metric=\"mean_absolute_error\"\n",
    ").fit(\n",
    "    train_data=combined_B,\n",
    "    tuning_data=tuning_data_B,\n",
    "    use_bag_holdout=True,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=2,\n",
    "    num_stack_levels=2,\n",
    "    time_limit=time_limit,\n",
    "    presets=\"best_quality\",\n",
    "    excluded_model_types=['KNN']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e18a77-3142-4682-82e2-d9fd6c53a42b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor_C = TabularPredictor(\n",
    "    label='pv_measurement',\n",
    "    eval_metric=\"mean_absolute_error\"\n",
    ").fit(\n",
    "    train_data=combined_C,\n",
    "    tuning_data=tuning_data_C,\n",
    "    use_bag_holdout=True,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=2,\n",
    "    num_stack_levels=2,\n",
    "    time_limit=time_limit,\n",
    "    presets=\"best_quality\",\n",
    "    excluded_model_types=['KNN']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915985ab",
   "metadata": {},
   "source": [
    "### AutoGluon predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c814a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prediction = predictor_A.predict(test_A)\n",
    "B_prediction = predictor_B.predict(test_B)\n",
    "C_prediction = predictor_C.predict(test_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312351cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "autogloun_predictions = np.concatenate([A_prediction, B_prediction, C_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c481a8ca6631fc9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CatBoost\n",
    "## Preprocessing\n",
    "Resetting data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ff8ee65e8503d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset_data_objects(location: LocationData) -> None:\n",
    "    location.train_data = location.original_train_data.copy()\n",
    "    location.target_data = location.original_target_data.copy()\n",
    "    location.test_data = location.original_test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a53b5de76fb9a40",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_data_objects(A)\n",
    "reset_data_objects(B)\n",
    "reset_data_objects(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102748ff31a74103",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d830517c6140c225",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Running preprocessing pipeline for CatBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582b8b7f22b5750",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=A.train_data, target_data=A.target_data, is_cat_boost=True)\n",
    "test_data = pre_process_test_data(test_data=A.test_data, is_cat_boost=True)\n",
    "\n",
    "A.train_data = train_data\n",
    "A.target_data = target_data\n",
    "A.test_data = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8c1ae2d5eb2b6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=B.train_data, target_data=B.target_data, is_cat_boost=True)\n",
    "test_data = pre_process_test_data(test_data=B.test_data, is_cat_boost=True)\n",
    "\n",
    "B.train_data = train_data\n",
    "B.target_data = target_data\n",
    "B.test_data = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b136e2dbb714cd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=C.train_data, target_data=C.target_data, is_cat_boost=True)\n",
    "test_data = pre_process_test_data(test_data=C.test_data, is_cat_boost=True)\n",
    "\n",
    "C.train_data = train_data\n",
    "C.target_data = target_data\n",
    "C.test_data = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0b3962d03ffa4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf5195d1813b58c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48677ca909d56f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_sinusoidal_features(df: pd.DataFrame, hour_col: str = 'hour', day_col: str = 'day_of_year') -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    dates = df.index.values\n",
    "    df['date_forecast'] = dates\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Create hour and day_of_year columns\n",
    "    df['hour'] = df['date_forecast'].dt.hour\n",
    "    df['day_of_year'] = df['date_forecast'].dt.dayofyear\n",
    "\n",
    "    # Extract week number using isocalendar()\n",
    "    df['week_of_year'] = df['date_forecast'].dt.isocalendar().week\n",
    "    df['month'] = df['date_forecast'].dt.month\n",
    "\n",
    "    # Constants for sinusoidal functions\n",
    "    hours_in_day = 24\n",
    "    days_in_year = 365.25  # Accounting for leap years\n",
    "    weeks_in_year = 52\n",
    "    months_in_year = 12\n",
    "\n",
    "    # Create sinusoidal features based on the hour of the day\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * (df[hour_col] - 18) / hours_in_day)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * (df[hour_col] - 18) / hours_in_day)\n",
    "\n",
    "    # Create sinusoidal features based on the day of the year\n",
    "    df['sin_day'] = np.sin(2 * np.pi * (df[day_col] - 355) / days_in_year)\n",
    "    df['cos_day'] = np.cos(2 * np.pi * (df[day_col] - 355) / days_in_year)\n",
    "\n",
    "    # Create sinusoidal features based on the week of the year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['week_of_year'] / weeks_in_year)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['week_of_year'] / weeks_in_year)\n",
    "\n",
    "    # Create sinusoidal features based on the month\n",
    "    df['sin_month'] = np.sin(2 * np.pi * df['month'] / months_in_year)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * df['month'] / months_in_year)\n",
    "\n",
    "    dates = df['date_forecast'].to_numpy()\n",
    "    df.index = dates\n",
    "    df.drop(columns=['hour', 'day_of_year', 'week_of_year', 'month', 'date_forecast'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_sinusoidal_features_for_location(location: LocationData) -> None:\n",
    "    train_data = location.train_data.copy()\n",
    "    test_data = location.test_data.copy()\n",
    "\n",
    "    train_data = create_sinusoidal_features(train_data)\n",
    "    test_data = create_sinusoidal_features(test_data)\n",
    "\n",
    "    train_data.sort_index(inplace=True)\n",
    "    test_data.sort_index(inplace=True)\n",
    "\n",
    "    location.train_data = train_data\n",
    "    location.test_data = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2159c768e2285",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_sinusoidal_features_for_location(A)\n",
    "create_sinusoidal_features_for_location(B)\n",
    "create_sinusoidal_features_for_location(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b53df83c83aa6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b8f994d7bd8e2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Defining functions for CatBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04fc78b95151b8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T19:06:10.891114900Z",
     "start_time": "2023-11-12T19:06:10.763112100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_catboost_kfolds(location: LocationData, N_FOLDS=10) -> list:\n",
    "    train_data = location.train_data\n",
    "    target_data = location.target_data\n",
    "\n",
    "    categorical_features = [\n",
    "        DEW_OR_RIME,\n",
    "        IS_DAY,\n",
    "        IS_IN_SHADOW,\n",
    "        IS_ESTIMATED,\n",
    "        PRECIP_TYPE_5MIN,\n",
    "    ]\n",
    "\n",
    "    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "    catboost_regressor = CatBoostRegressor(\n",
    "        iterations=20000,\n",
    "        depth=8,\n",
    "        learning_rate=0.01,\n",
    "        l2_leaf_reg=3,\n",
    "        loss_function='MAE',\n",
    "        random_seed=42,\n",
    "        verbose=True,\n",
    "        cat_features=categorical_features,\n",
    "    )\n",
    "\n",
    "    X_val = train_data[train_data[IS_ESTIMATED] == 1]\n",
    "    y_val = target_data.loc[X_val.index.values]\n",
    "\n",
    "    catboost_models = []\n",
    "\n",
    "    for train_idx, _ in kf.split(train_data):\n",
    "        X_train, X_val = train_data.iloc[train_idx], X_val\n",
    "        y_train, y_val = target_data.iloc[train_idx], y_val\n",
    "\n",
    "        train = Pool(\n",
    "            data=X_train,\n",
    "            label=y_train,\n",
    "            cat_features=categorical_features\n",
    "        )\n",
    "\n",
    "        test = Pool(\n",
    "            data=X_val,\n",
    "            label=y_val,\n",
    "            cat_features=categorical_features\n",
    "        )\n",
    "\n",
    "        catboost_regressor.fit(\n",
    "            train,\n",
    "            eval_set=test,\n",
    "            use_best_model=True,\n",
    "            early_stopping_rounds=200\n",
    "        )\n",
    "\n",
    "        catboost_models.append(catboost_regressor)\n",
    "\n",
    "    return catboost_models\n",
    "\n",
    "\n",
    "def mean_catboost_prediction(location: LocationData, catboost_models: list) -> np.ndarray:\n",
    "    test_data = location.test_data\n",
    "    predictions = []\n",
    "\n",
    "    for model in catboost_models:\n",
    "        prediction = model.predict(test_data)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62597d58cb55938d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Training CatBoost models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3bd0190fc846e3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T19:06:16.692113300Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "cb_models_A = train_catboost_kfolds(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17c139647880a6",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "cb_models_B = train_catboost_kfolds(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22eb6bdb3b90b7e",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "cb_models_C = train_catboost_kfolds(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73759fd02438788",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cb_preds_A = mean_catboost_prediction(A, cb_models_A)\n",
    "cb_preds_B = mean_catboost_prediction(B, cb_models_B)\n",
    "cb_preds_C = mean_catboost_prediction(C, cb_models_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a05099bb51fcec9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### CatBoost predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b23491e45918e9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_list = [cb_preds_A, cb_preds_B, cb_preds_C]\n",
    "predictions = np.concatenate(prediction_list)\n",
    "catboost_predictions = predictions.clip(min=0, max=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead482e9a4e7e44",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_list = [autogloun_predictions, catboost_predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b91d04668e2728d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Plotting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17eac1d7b99002",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for prediction in prediction_list:\n",
    "    plt.figure(figsize=(30, 7))\n",
    "    plt.plot(catboost_predictions, 'b-')\n",
    "    plt.plot(autogloun_predictions, 'r-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c29779",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = [np.mean(prediction_list, axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e6b1b4455d05e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Postprocessing: Removing negative values and setting night time values to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5449a8e8b4473476",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_zero_in_time_range(data: pd.DataFrame, start_hour: int, end_hour: int):\n",
    "    time_column = TIME\n",
    "    if not pd.api.types.is_datetime64_any_dtype(data[time_column]):\n",
    "        data[time_column] = pd.to_datetime(data[time_column])\n",
    "\n",
    "    if start_hour <= end_hour:\n",
    "        mask = data[time_column].dt.hour.between(start_hour, end_hour, inclusive='left')\n",
    "    else:\n",
    "        mask = (data[time_column].dt.hour >= start_hour) | (data[time_column].dt.hour <= end_hour)\n",
    "\n",
    "    data.loc[mask, 'pv_measurement'] = 0\n",
    "    return data\n",
    "\n",
    "\n",
    "def set_zero_in_time(predictions: np.ndarray, start_hour: int, end_hour: int) -> np.ndarray:\n",
    "    locations: list[LocationData] = [A, B, C]\n",
    "    location_predictions: list[np.ndarray] = []\n",
    "\n",
    "    segment_size = len(predictions) // len(locations)\n",
    "\n",
    "    for i in range(len(locations)):\n",
    "        # Calculate the start and end indices for each segment\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = (i + 1) * segment_size\n",
    "\n",
    "        # Extract the segment from predictions and append to location_predictions\n",
    "        segment = predictions[start_idx:end_idx]\n",
    "        location_predictions.append(segment)\n",
    "\n",
    "    data_list = []\n",
    "    for location, prediction in zip(locations, location_predictions):\n",
    "        test_dates = location.test_data.index.values\n",
    "        data = pd.DataFrame(data=prediction, index=test_dates, columns=[PV_MEASUREMENT])\n",
    "        data = set_zero_in_time_range(data, start_hour, end_hour)\n",
    "        data.append(data[PV_MEASUREMENT].to_numpy())\n",
    "\n",
    "    return np.concatenate(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd360cbb63a5fc",
   "metadata": {},
   "source": [
    "# Saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4e0f1c8748630",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "\n",
    "def create_csv(*prediction_list: np.ndarray) -> None:\n",
    "    prediction_path: str = os.path.join(\n",
    "        DATA_DIR,\n",
    "        \"predictions\",\n",
    "        time.strftime(\"%Y%m%d-%H%M%S\") + \".csv\",\n",
    "    )\n",
    "\n",
    "    output: np.ndarray = np.concatenate(prediction_list)\n",
    "    output = output.clip(min=0, max=None)\n",
    "    indexes = np.arange(0, len(output), 1, dtype=int)\n",
    "\n",
    "    data = {\n",
    "        \"id\": indexes,\n",
    "        \"prediction\": output\n",
    "    }\n",
    "\n",
    "    dataframe: pd.DataFrame = pd.DataFrame(data)\n",
    "    dataframe.to_csv(prediction_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4562075a901b4",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_csv(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a90fb813388f7d9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for prediction in prediction_list:\n",
    "    plt.figure(figsize=(30, 7))\n",
    "    plt.plot(catboost_predictions, 'b-')\n",
    "    plt.plot(autogloun_predictions, 'r-')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
