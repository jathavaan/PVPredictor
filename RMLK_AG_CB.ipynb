{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "26d9f234-58de-4fc9-9f4e-443ea8265844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:03.905687300Z",
     "start_time": "2023-11-12T15:37:03.470462500Z"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "ea0ba8bc12eaef6f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:04.317372100Z",
     "start_time": "2023-11-12T15:37:03.478466200Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from catboost import CatBoostRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850e2dfc3bb16aa",
   "metadata": {},
   "source": [
    "Setting rules for libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "8081b47cf627a139",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:05.447887800Z",
     "start_time": "2023-11-12T15:37:03.498464400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Warnings\n",
    "\n",
    "# Pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f768895a70a9bd57",
   "metadata": {},
   "source": [
    "# Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "cf20b5c6b0ced7a4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:05.821955700Z",
     "start_time": "2023-11-12T15:37:03.509461100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Features to remove\n",
    "DATE_CALC: str = \"date_calc\"\n",
    "DATE_FORECAST: str = \"date_forecast\"\n",
    "ABSOLUTE_HUMIDITY: str = 'absolute_humidity_2m:gm3'\n",
    "AIR_DENSITY: str = 'air_density_2m:kgm3'\n",
    "CEILING_HEIGHT: str = 'ceiling_height_agl:m'\n",
    "CLEAR_SKY_ENERGY: str = 'clear_sky_energy_1h:J'\n",
    "CLEAR_SKY_RAD: str = 'clear_sky_rad:W'\n",
    "CLOUD_BASE: str = 'cloud_base_agl:m'\n",
    "DEW_OR_RIME: str = 'dew_or_rime:idx'\n",
    "DEW_POINT: str = 'dew_point_2m:K'\n",
    "DIFFUSE_RAD: str = 'diffuse_rad:W'\n",
    "DIFFUSE_RAD_1H: str = 'diffuse_rad_1h:J'\n",
    "DIRECT_RAD: str = 'direct_rad:W'\n",
    "DIRECT_RAD_1H: str = 'direct_rad_1h:J'\n",
    "EFFECTIVE_CLOUD_COVER: str = 'effective_cloud_cover:p'\n",
    "ELEVATION: str = 'elevation:m'\n",
    "FRESH_SNOW_12H: str = 'fresh_snow_12h:cm'\n",
    "FRESH_SNOW_1H: str = 'fresh_snow_1h:cm'\n",
    "FRESH_SNOW_24H: str = 'fresh_snow_24h:cm'\n",
    "FRESH_SNOW_3H: str = 'fresh_snow_3h:cm'\n",
    "FRESH_SNOW_6H: str = 'fresh_snow_6h:cm'\n",
    "IS_DAY: str = 'is_day:idx'\n",
    "IS_IN_SHADOW: str = 'is_in_shadow:idx'\n",
    "MSL_PRESSURE: str = 'msl_pressure:hPa'\n",
    "PRECIP_5MIN: str = 'precip_5min:mm'\n",
    "PRECIP_TYPE_5MIN: str = 'precip_type_5min:idx'\n",
    "PRESSURE_100M: str = 'pressure_100m:hPa'\n",
    "PRESSURE_50M: str = 'pressure_50m:hPa'\n",
    "PROB_RIME: str = 'prob_rime:p'\n",
    "RAIN_WATER: str = 'rain_water:kgm2'\n",
    "RELATIVE_HUMIDITY: str = 'relative_humidity_1000hPa:p'\n",
    "SFC_PRESSURE: str = 'sfc_pressure:hPa'\n",
    "SNOW_DENSITY: str = 'snow_density:kgm3'\n",
    "SNOW_DEPTH: str = 'snow_depth:cm'\n",
    "SNOW_DRIFT: str = 'snow_drift:idx'\n",
    "SNOW_MELT_10MIN: str = 'snow_melt_10min:mm'\n",
    "SNOW_WATER: str = 'snow_water:kgm2'\n",
    "SUN_AZIMUTH: str = 'sun_azimuth:d'\n",
    "SUN_ELEVATION: str = 'sun_elevation:d'\n",
    "SUPER_COOLED_LIQUID_WATER: str = 'super_cooled_liquid_water:kgm2'\n",
    "T_1000HPA: str = 't_1000hPa:K'\n",
    "TOTAL_CLOUD_COVER: str = 'total_cloud_cover:p'\n",
    "VISIBILITY: str = 'visibility:m'\n",
    "WIND_SPEED_10M: str = 'wind_speed_10m:ms'\n",
    "WIND_SPEED_U_10M: str = 'wind_speed_u_10m:ms'\n",
    "WIND_SPEED_V_10M: str = 'wind_speed_v_10m:ms'\n",
    "WIND_SPEED_W_1000HPA: str = 'wind_speed_w_1000hPa:ms'\n",
    "IS_ESTIMATED = 'is_estimated'\n",
    "\n",
    "TIME = 'time'\n",
    "PV_MEASUREMENT = 'pv_measurement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "8e5747184b6faafe",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:06.102854400Z",
     "start_time": "2023-11-12T15:37:03.527462600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def read_parquet(filepath: str) -> pd.DataFrame:\n",
    "    dataframe: pd.DataFrame = pd.read_parquet(filepath)\n",
    "    if DATE_CALC in dataframe.columns:\n",
    "        dataframe = dataframe.drop(columns=[DATE_CALC])\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def concat_observed_and_estimated_train_data(\n",
    "        X_observed: pd.DataFrame,\n",
    "        X_estimated: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    X_observed: pd.DataFrame = X_observed.copy()\n",
    "    X_estimated: pd.DataFrame = X_estimated.copy()\n",
    "\n",
    "    assert X_observed.shape[1] == X_estimated.shape[1]\n",
    "\n",
    "    concatenated_dataframe: pd.DataFrame = pd.concat(\n",
    "        objs=[X_observed, X_estimated],\n",
    "        axis=0\n",
    "    ).sort_values(\n",
    "        by=DATE_FORECAST,\n",
    "        ascending=True\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return concatenated_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "a2f38c501e227ecc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:06.219922100Z",
     "start_time": "2023-11-12T15:37:03.542466800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data transfer object for location data\n",
    "class LocationData:\n",
    "    combined_data: pd.DataFrame\n",
    "\n",
    "    def __init__(self, train_data, target_data, test_data) -> None:\n",
    "        self.train_data = train_data\n",
    "        self.target_data = target_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "        self.original_train_data = train_data.copy()\n",
    "        self.original_target_data = target_data.copy()\n",
    "        self.original_test_data = test_data.copy()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Train: {self.train_data.shape} [No. NaN: {self.train_data.isna().sum().sum()}], Target: {self.target_data.shape} [No. NaN: {self.target_data.isna().sum().sum()}], Test: {self.test_data.shape} [No. NaN: {self.test_data.isna().sum().sum()}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "60d61a1645354f7c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:07.854284100Z",
     "start_time": "2023-11-12T15:37:03.555468400Z"
    }
   },
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/A/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/A/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/A/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/A/X_test_estimated.parquet\")\n",
    "\n",
    "observed_train['is_estimated'] = 0\n",
    "estimated_train['is_estimated'] = 1\n",
    "test_data['is_estimated'] = 1\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "d6374820ca84a630",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:07.903809500Z",
     "start_time": "2023-11-12T15:37:03.647740500Z"
    }
   },
   "outputs": [],
   "source": [
    "A = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "58333e566cc3633f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:07.904821500Z",
     "start_time": "2023-11-12T15:37:03.664756600Z"
    }
   },
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/B/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/B/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/B/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/B/X_test_estimated.parquet\")\n",
    "\n",
    "observed_train['is_estimated'] = 0\n",
    "estimated_train['is_estimated'] = 1\n",
    "test_data['is_estimated'] = 1\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")\n",
    "\n",
    "B = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "edbd3f707ed3d2f7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:07.905810500Z",
     "start_time": "2023-11-12T15:37:03.772080600Z"
    }
   },
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/C/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/C/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/C/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/C/X_test_estimated.parquet\")\n",
    "\n",
    "observed_train['is_estimated'] = 0\n",
    "estimated_train['is_estimated'] = 1\n",
    "test_data['is_estimated'] = 1\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "7a3ab86d84357bed",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:07.905810500Z",
     "start_time": "2023-11-12T15:37:03.865112300Z"
    }
   },
   "outputs": [],
   "source": [
    "C = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "b3d3d8fec6811b18",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:07.963818600Z",
     "start_time": "2023-11-12T15:37:03.879687600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Train: (136245, 47) [No. NaN: 168040], Target: (34085, 2) [No. NaN: 0], Test: (2880, 47) [No. NaN: 3971]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (134505, 47) [No. NaN: 158811], Target: (32848, 2) [No. NaN: 4], Test: (2880, 47) [No. NaN: 3912]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (134401, 47) [No. NaN: 157326], Target: (32155, 2) [No. NaN: 6060], Test: (2880, 47) [No. NaN: 4104]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0124801bff563",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Defining functions for preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f4e6db271eea0",
   "metadata": {},
   "source": [
    "Function to remove features and replace `NaN` values with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "926a2ddc44c56339",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:07.963818600Z",
     "start_time": "2023-11-12T15:37:03.928691200Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_feature(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    dataframe = dataframe.drop(columns=[*features])\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def replace_nan_with_zero(dataframe: pd.DataFrame, feature: str) -> pd.DataFrame:\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe[[feature]] = dataframe[[feature]].fillna(0)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f994238640695aa",
   "metadata": {},
   "source": [
    "Function to resample data by categorizing them\n",
    "- Features to average\n",
    "- Features to pick a value\n",
    "- Features to change redefine `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "76f0e24c1ba0fe4b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:07.963818600Z",
     "start_time": "2023-11-12T15:37:03.948688400Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_by_selection(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "    if reduced_dataframe.isna().sum().sum() > 0:\n",
    "        print(\"[ERROR] Sample by selection got dataframe with NaN values\")\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.last()\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_by_mean(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.mean()\n",
    "    \n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_by_sum(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.sum()\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_cloud_base(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    cloud_base_frame = dataframe[[CLOUD_BASE]]\n",
    "\n",
    "    # Filling NaN values with -2_000\n",
    "    filled_frame = cloud_base_frame.copy()\n",
    "    filled_frame = filled_frame.fillna(-100_000)\n",
    "\n",
    "    # Resampling to 1H\n",
    "    resampler = filled_frame.resample('H')\n",
    "    cloud_base_frame = resampler.mean()\n",
    "\n",
    "    # Replacing negative values with -666\n",
    "    cloud_bases = cloud_base_frame[CLOUD_BASE].to_numpy()\n",
    "    negative_indexes = np.where(cloud_bases < 0)[0]\n",
    "    cloud_bases[negative_indexes] = -666\n",
    "    cloud_base_frame[CLOUD_BASE] = cloud_bases\n",
    "\n",
    "    # Dropping NaN values that were introduced by resampling\n",
    "    reduced_dataframe = cloud_base_frame.copy()\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def merge_frames(*dataframes: pd.DataFrame) -> pd.DataFrame:\n",
    "    indexes = [df.index for df in dataframes]\n",
    "\n",
    "    # Finding intersecting indexes in list of indexes, the indexes are datetime\n",
    "    intersecting_indexes = list(set(indexes[0]).intersection(*indexes))\n",
    "    filtered_dataframes = [df.loc[intersecting_indexes] for df in dataframes]\n",
    "\n",
    "    return pd.concat(filtered_dataframes, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4830f59eace7ae",
   "metadata": {},
   "source": [
    "Functions to synchronze dates in weather data and target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "98926ee1a8a03451",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:08.015500500Z",
     "start_time": "2023-11-12T15:37:03.962693100Z"
    }
   },
   "outputs": [],
   "source": [
    "def synchronize_timestamps(train_data: pd.DataFrame, target_data: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # IndexError if index in both train data and target data is DateTimeIndex\n",
    "    if not isinstance(train_data.index, pd.DatetimeIndex):\n",
    "        raise IndexError(\"Train data does not have DateTimeIndex\")\n",
    "\n",
    "    if not isinstance(target_data.index, pd.DatetimeIndex):\n",
    "        raise IndexError(\"Target data does not have DateTimeIndex\")\n",
    "\n",
    "    train_data.sort_index(inplace=True)\n",
    "    target_data.sort_index(inplace=True)\n",
    "\n",
    "    train_dates = train_data.index.values\n",
    "    target_dates = target_data.index.values\n",
    "\n",
    "    # Finding intersecting dates and filtering dataframes\n",
    "    intersecting_dates = np.intersect1d(train_dates, target_dates)\n",
    "    train_data = train_data.loc[intersecting_dates]\n",
    "    target_data = target_data.loc[intersecting_dates]\n",
    "\n",
    "    return train_data, target_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dateset contains many constant values for some features, any row with constant values is removed. If there is are at least 24 rows with constant values, the rows are removed.\n",
    "- -666 in cloud base should not be removed as this is added in earlier "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f0c01434829e1ae"
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "472023231528654c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:08.015500500Z",
     "start_time": "2023-11-12T15:37:03.978229Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_constant_measurements(\n",
    "        train_data: pd.DataFrame,\n",
    "        target_data: pd.DataFrame,\n",
    "        THRESHOLD: int = 24\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    indexes = []\n",
    "    count = 0\n",
    "    previous_value = None\n",
    "    start_index = None\n",
    "\n",
    "    # Setting date indexes as columns and resetting index\n",
    "    train_data[DATE_FORECAST] = train_data.index.values\n",
    "    target_data[TIME] = target_data.index.values\n",
    "\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    target_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Assuming y is a DataFrame and has a column for pv_measurement values\n",
    "    pv_measurements = target_data[PV_MEASUREMENT].to_numpy()\n",
    "\n",
    "    # Identifying indexes of consecutive constant values\n",
    "    for index, value in enumerate(pv_measurements):\n",
    "        if value == previous_value:\n",
    "            count += 1\n",
    "            if count == 1:\n",
    "                start_index = index - 1\n",
    "            if count >= THRESHOLD and value > -0.1:\n",
    "                indexes.extend(range(start_index, index + 1))\n",
    "        else:\n",
    "            count = 0\n",
    "        previous_value = value\n",
    "\n",
    "    # Assuming y has a column for dates and X has a similar column to filter on\n",
    "    dates = target_data.loc[indexes, TIME].unique()\n",
    "\n",
    "    filtered_train_data = train_data[~train_data[DATE_FORECAST].isin(dates)]\n",
    "    filtered_target_data = target_data[~target_data[TIME].isin(dates)]\n",
    "\n",
    "    # Switching back to date as index\n",
    "    filtered_train_data = filtered_train_data.set_index(DATE_FORECAST)\n",
    "    filtered_target_data = filtered_target_data.set_index(TIME)\n",
    "\n",
    "    return filtered_train_data, filtered_target_data\n",
    "\n",
    "\n",
    "def drop_nan_values_from_target_data(train_data: pd.DataFrame, target_data: pd.DataFrame) -> tuple[\n",
    "    pd.DataFrame, pd.DataFrame]:\n",
    "    target_data.dropna(axis=0, how='any', inplace=True)\n",
    "    train_data, target_data = synchronize_timestamps(train_data, target_data)\n",
    "    return train_data, target_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948b908f0f59cef",
   "metadata": {},
   "source": [
    "## Adding lag features, this has to come here because of the resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "c1317434ce4b07e5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:08.015500500Z",
     "start_time": "2023-11-12T15:37:03.996230600Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_lag_features(dataframe: pd.DataFrame, features: list[str], lags: list[int]) -> tuple[pd.DataFrame, list[str]]:\n",
    "    dataframe = dataframe.copy()\n",
    "    lag_feature_names: list[str] = []\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            lag_feature_name = f\"{feature}_lag_{lag}\"\n",
    "            dataframe[lag_feature_name] = dataframe[feature].shift(lag)\n",
    "            lag_feature_names.append(lag_feature_name)\n",
    "\n",
    "    return dataframe, lag_feature_names\n",
    "\n",
    "\n",
    "def add_lead_features(dataframe: pd.DataFrame, features: list[str], lags: list[int]) -> tuple[pd.DataFrame, list[str]]:\n",
    "    dataframe = dataframe.copy()\n",
    "    lead_feature_names: list[str] = []\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            lead_feature_name = f\"{feature}_lead_{lag}\"\n",
    "            dataframe[lead_feature_name] = dataframe[feature].shift(-lag)\n",
    "            lead_feature_names.append(lead_feature_name)\n",
    "\n",
    "    return dataframe, lead_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4514f22c17b74b8e",
   "metadata": {},
   "source": [
    "Pipeline for preprocessing that is common for both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "92c2733c9a43a644",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:08.015500500Z",
     "start_time": "2023-11-12T15:37:04.011430900Z"
    }
   },
   "outputs": [],
   "source": [
    "def common_pre_processing(dataframe: pd.DataFrame, is_cat_boost: bool) -> pd.DataFrame:\n",
    "    features_to_drop = [SNOW_DRIFT, CEILING_HEIGHT, SNOW_DRIFT]\n",
    "    \n",
    "    if is_cat_boost: \n",
    "        dataframe = remove_feature(dataframe, *features_to_drop)\n",
    "\n",
    "    dataframe = dataframe.set_index(DATE_FORECAST)\n",
    "    feature_to_lag_and_lead: list[str] = [DIRECT_RAD]\n",
    "\n",
    "    # Adding lag and lead features\n",
    "    lag_features = []\n",
    "    lead_features = []\n",
    "\n",
    "    if is_cat_boost:\n",
    "        dataframe, lag_features = add_lag_features(dataframe, feature_to_lag_and_lead, [1, 2, 3])\n",
    "        dataframe, lead_features = add_lead_features(dataframe, feature_to_lag_and_lead, [1, 2, 3])\n",
    "\n",
    "    # Resample data to hours\n",
    "    selection_features = [\n",
    "        CLEAR_SKY_ENERGY,  # Maybe move\n",
    "        DIRECT_RAD_1H,  # MAYBE MOVE\n",
    "        DIFFUSE_RAD_1H,  # Maybe move\n",
    "        FRESH_SNOW_1H,\n",
    "        FRESH_SNOW_3H,\n",
    "        FRESH_SNOW_6H,\n",
    "        FRESH_SNOW_12H,\n",
    "        FRESH_SNOW_24H,\n",
    "        DEW_OR_RIME,\n",
    "        IS_DAY,\n",
    "        IS_IN_SHADOW,\n",
    "        PRECIP_TYPE_5MIN,\n",
    "        ELEVATION,\n",
    "        IS_ESTIMATED,\n",
    "        *lag_features,\n",
    "        *lead_features,\n",
    "    ]\n",
    "\n",
    "    mean_features = [\n",
    "        ABSOLUTE_HUMIDITY,\n",
    "        CLEAR_SKY_RAD,\n",
    "        DIFFUSE_RAD,\n",
    "        DIRECT_RAD,\n",
    "        AIR_DENSITY,\n",
    "        DEW_POINT,\n",
    "        MSL_PRESSURE,\n",
    "        PRESSURE_100M,\n",
    "        PRESSURE_50M,\n",
    "        SFC_PRESSURE,\n",
    "        RELATIVE_HUMIDITY,\n",
    "        T_1000HPA,\n",
    "        SUN_AZIMUTH,\n",
    "        SUN_ELEVATION,\n",
    "        EFFECTIVE_CLOUD_COVER,\n",
    "        TOTAL_CLOUD_COVER,\n",
    "        VISIBILITY,\n",
    "        WIND_SPEED_10M,\n",
    "        WIND_SPEED_U_10M,\n",
    "        WIND_SPEED_V_10M,\n",
    "        WIND_SPEED_W_1000HPA,\n",
    "        SNOW_MELT_10MIN,\n",
    "        PROB_RIME,\n",
    "        SUPER_COOLED_LIQUID_WATER,\n",
    "        SNOW_DEPTH, \n",
    "    ]\n",
    "    \n",
    "    sum_features = [\n",
    "        SNOW_WATER,\n",
    "        PRECIP_5MIN,\n",
    "        RAIN_WATER,\n",
    "    ]\n",
    "    \n",
    "    if not is_cat_boost:\n",
    "        for feature in mean_features:\n",
    "            mean_features.append(feature)\n",
    "\n",
    "    selection_df = sample_by_selection(dataframe, *selection_features)\n",
    "    mean_df = sample_by_mean(dataframe, *mean_features)\n",
    "    sum_df = sample_by_sum(dataframe, *sum_features)\n",
    "    ceiling_height_df = sample_cloud_base(dataframe)\n",
    "    dataframe = merge_frames(selection_df, mean_df, ceiling_height_df, sum_df)\n",
    "\n",
    "    if is_cat_boost:\n",
    "        dataframe = dataframe.dropna(axis=0, how='any')\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db91786254d2a576",
   "metadata": {},
   "source": [
    "### Preprocessing pipelines for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "9556d94345fac3ba",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:37:08.016536200Z",
     "start_time": "2023-11-12T15:37:04.024571400Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process_train_data(\n",
    "        train_data: pd.DataFrame, \n",
    "        target_data: pd.DataFrame, \n",
    "        is_cat_boost: bool\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_data = common_pre_processing(dataframe=train_data, is_cat_boost=is_cat_boost)\n",
    "\n",
    "    # Setting time as index for target data\n",
    "    target_data[TIME] = pd.to_datetime(target_data[TIME])\n",
    "    target_data = target_data.set_index(TIME)\n",
    "\n",
    "    # Preprocessing that affects both train and target data\n",
    "    train_data, target_data = synchronize_timestamps(train_data=train_data, target_data=target_data)\n",
    "    train_data, target_data = remove_constant_measurements(train_data=train_data, target_data=target_data)\n",
    "    train_data, target_data = drop_nan_values_from_target_data(train_data=train_data, target_data=target_data)\n",
    "\n",
    "    return train_data, target_data\n",
    "\n",
    "\n",
    "def pre_process_test_data(test_data: pd.DataFrame, is_cat_boost: bool) -> pd.DataFrame:\n",
    "    test_data = test_data.set_index(DATE_FORECAST)\n",
    "\n",
    "    if is_cat_boost:\n",
    "        test_data = common_pre_processing(dataframe=test_data, is_cat_boost=is_cat_boost)\n",
    "    else:\n",
    "        test_data = test_data.groupby(test_data.index.floor('H')).mean()\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79beb4",
   "metadata": {},
   "source": [
    "# AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(\n",
    "    train_data=A.train_data,\n",
    "    target_data=A.target_data,\n",
    "    is_cat_boost=False\n",
    ")\n",
    "\n",
    "test_data = pre_process_test_data(test_data=A.test_data, is_cat_boost=False)\n",
    "A.train_data = train_data\n",
    "A.target_data = target_data\n",
    "A.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-12T15:37:04.039571200Z"
    }
   },
   "id": "6ad5e575852aadfb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(\n",
    "    train_data=B.train_data,\n",
    "    target_data=B.target_data,\n",
    "    is_cat_boost=False\n",
    ")\n",
    "test_data = pre_process_test_data(test_data=B.test_data, is_cat_boost=False)\n",
    "B.train_data = train_data\n",
    "B.target_data = target_data\n",
    "B.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a4a11892d7383702"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(\n",
    "    train_data=C.train_data,\n",
    "    target_data=C.target_data,\n",
    "    is_cat_boost=False\n",
    ")\n",
    "test_data = pre_process_test_data(test_data=C.test_data, is_cat_boost=False)\n",
    "C.train_data = train_data\n",
    "C.target_data = target_data\n",
    "C.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d7936de9ca1f1338"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "53a3241e6b3792ef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a9cca-6d35-4e5a-aba2-ef43b90af3e8",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "MINUTES: int = 1\n",
    "time_limit = int(60 * MINUTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3fd13",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Merged files required by the AutoGluon model\n",
    "def merge_train_and_target(location: LocationData) -> pd.DataFrame:\n",
    "    train_data = location.train_data.copy()\n",
    "    target_data = location.target_data.copy()\n",
    "\n",
    "    return pd.merge(left=train_data, right=target_data, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "\n",
    "def prepare_data_for_ag(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in dataframe.columns:\n",
    "        if dataframe[col].dtype == 'object':\n",
    "            dataframe[col] = dataframe[col].astype(str)\n",
    "        else:\n",
    "            dataframe[col] = pd.to_numeric(dataframe[col], errors='coerce')\n",
    "\n",
    "    if pd.api.types.is_datetime64_any_dtype(dataframe.index):\n",
    "        dataframe = dataframe.sort_index()  # Sort by date if the index is a datetime\n",
    "        dataframe = dataframe.reset_index()  # Reset index\n",
    "\n",
    "    if DATE_FORECAST in dataframe.columns:\n",
    "        dataframe = dataframe.drop(columns=[DATE_FORECAST])\n",
    "\n",
    "    if TIME in dataframe.columns:\n",
    "        dataframe = dataframe.drop(columns=[TIME])\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33d2a5",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "combined_A = prepare_data_for_ag(merge_train_and_target(A))\n",
    "combined_B = prepare_data_for_ag(merge_train_and_target(B))\n",
    "combined_C = prepare_data_for_ag(merge_train_and_target(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d37c15-7870-48b9-8bf9-cb85db0caa68",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "tuning_data_A = combined_A[combined_A['is_estimated'] == 1].tail(10100)\n",
    "tuning_data_B = combined_B[combined_B['is_estimated'] == 1].tail(950)\n",
    "tuning_data_C = combined_C[combined_C['is_estimated'] == 1].tail(870)\n",
    "\n",
    "combined_A = combined_A[(~combined_A.index.isin(tuning_data_A.index))]\n",
    "combined_B = combined_B[(~combined_B.index.isin(tuning_data_B.index))]\n",
    "combined_C = combined_C[(~combined_C.index.isin(tuning_data_C.index))]\n",
    "\n",
    "test_A = prepare_data_for_ag(A.test_data)\n",
    "test_B = prepare_data_for_ag(B.test_data)\n",
    "test_C = prepare_data_for_ag(C.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c187ace-8822-4a31-b9cb-474fb0a01c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor_A = TabularPredictor(\n",
    "    label='pv_measurement',\n",
    "    eval_metric=\"mean_absolute_error\"\n",
    ").fit(\n",
    "    train_data=combined_A,\n",
    "    tuning_data=tuning_data_A,\n",
    "    use_bag_holdout=True,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=2,\n",
    "    num_stack_levels=2,\n",
    "    time_limit=time_limit,\n",
    "    presets=\"best_quality\",\n",
    "    excluded_model_types=['KNN']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961a928-3931-465c-924a-d6030ca0a0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor_B = TabularPredictor(\n",
    "    label='pv_measurement',\n",
    "    eval_metric=\"mean_absolute_error\"\n",
    ").fit(\n",
    "    train_data=combined_B,\n",
    "    tuning_data=tuning_data_B,\n",
    "    use_bag_holdout=True,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=2,\n",
    "    num_stack_levels=2,\n",
    "    time_limit=time_limit,\n",
    "    presets=\"best_quality\",\n",
    "    excluded_model_types=['KNN']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e18a77-3142-4682-82e2-d9fd6c53a42b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor_C = TabularPredictor(\n",
    "    label='pv_measurement',\n",
    "    eval_metric=\"mean_absolute_error\"\n",
    ").fit(\n",
    "    train_data=combined_C,\n",
    "    tuning_data=tuning_data_C,\n",
    "    use_bag_holdout=True,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=2,\n",
    "    num_stack_levels=2,\n",
    "    time_limit=time_limit,\n",
    "    presets=\"best_quality\",\n",
    "    excluded_model_types=['KNN']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915985ab",
   "metadata": {},
   "source": [
    "### AutoGluon predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c814a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prediction = predictor_A.predict(test_A)\n",
    "B_prediction = predictor_B.predict(test_B)\n",
    "C_prediction = predictor_C.predict(test_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312351cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "autogloun_predictions = np.concatenate([A_prediction, B_prediction, C_prediction])\n",
    "prediction_list = [autogloun_predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CatBoost\n",
    "## Preprocessing\n",
    "Resetting data objects"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c481a8ca6631fc9"
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [],
   "source": [
    "def reset_data_objects(location: LocationData) -> None:\n",
    "    location.train_data = location.original_train_data.copy()\n",
    "    location.target_data = location.original_target_data.copy()\n",
    "    location.test_data = location.original_test_data.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:30:54.067304Z",
     "start_time": "2023-11-12T15:30:53.865664700Z"
    }
   },
   "id": "647ff8ee65e8503d"
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [],
   "source": [
    "reset_data_objects(A)\n",
    "reset_data_objects(B)\n",
    "reset_data_objects(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:31:01.261232600Z",
     "start_time": "2023-11-12T15:31:01.203231300Z"
    }
   },
   "id": "7a53b5de76fb9a40"
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [
    {
     "data": {
      "text/plain": "Train: (136245, 47) [No. NaN: 168040], Target: (34085, 2) [No. NaN: 0], Test: (2880, 47) [No. NaN: 3971]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (134505, 47) [No. NaN: 158811], Target: (32848, 2) [No. NaN: 4], Test: (2880, 47) [No. NaN: 3912]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (134401, 47) [No. NaN: 157326], Target: (32155, 2) [No. NaN: 6060], Test: (2880, 47) [No. NaN: 4104]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:31:37.201617400Z",
     "start_time": "2023-11-12T15:31:37.003541300Z"
    }
   },
   "id": "102748ff31a74103"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running preprocessing pipeline for CatBoost model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d830517c6140c225"
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Sample by selection got dataframe with NaN values\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['snow_drift:idx', 'ceiling_height_agl:m', 'snow_density:kgm3'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[287], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_data, target_data \u001B[38;5;241m=\u001B[39m \u001B[43mpre_process_train_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mA\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mA\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_cat_boost\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m test_data \u001B[38;5;241m=\u001B[39m pre_process_test_data(test_data\u001B[38;5;241m=\u001B[39mA\u001B[38;5;241m.\u001B[39mtest_data, is_cat_boost\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      4\u001B[0m A\u001B[38;5;241m.\u001B[39mtrain_data \u001B[38;5;241m=\u001B[39m train_data\n",
      "Cell \u001B[1;32mIn[266], line 6\u001B[0m, in \u001B[0;36mpre_process_train_data\u001B[1;34m(train_data, target_data, is_cat_boost)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpre_process_train_data\u001B[39m(\n\u001B[0;32m      2\u001B[0m         train_data: pd\u001B[38;5;241m.\u001B[39mDataFrame, \n\u001B[0;32m      3\u001B[0m         target_data: pd\u001B[38;5;241m.\u001B[39mDataFrame, \n\u001B[0;32m      4\u001B[0m         is_cat_boost: \u001B[38;5;28mbool\u001B[39m\n\u001B[0;32m      5\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[pd\u001B[38;5;241m.\u001B[39mDataFrame, pd\u001B[38;5;241m.\u001B[39mDataFrame]:\n\u001B[1;32m----> 6\u001B[0m     train_data \u001B[38;5;241m=\u001B[39m \u001B[43mcommon_pre_processing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataframe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_cat_boost\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_cat_boost\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;66;03m# Setting time as index for target data\u001B[39;00m\n\u001B[0;32m      9\u001B[0m     target_data[TIME] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(target_data[TIME])\n",
      "Cell \u001B[1;32mIn[249], line 73\u001B[0m, in \u001B[0;36mcommon_pre_processing\u001B[1;34m(dataframe, is_cat_boost)\u001B[0m\n\u001B[0;32m     66\u001B[0m sum_features \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     67\u001B[0m     SNOW_WATER,\n\u001B[0;32m     68\u001B[0m     PRECIP_5MIN,\n\u001B[0;32m     69\u001B[0m     RAIN_WATER,\n\u001B[0;32m     70\u001B[0m ]\n\u001B[0;32m     72\u001B[0m selection_df \u001B[38;5;241m=\u001B[39m sample_by_selection(dataframe, \u001B[38;5;241m*\u001B[39mselection_features)\n\u001B[1;32m---> 73\u001B[0m mean_df \u001B[38;5;241m=\u001B[39m \u001B[43msample_by_mean\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmean_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     74\u001B[0m sum_df \u001B[38;5;241m=\u001B[39m sample_by_sum(dataframe, \u001B[38;5;241m*\u001B[39msum_features)\n\u001B[0;32m     75\u001B[0m ceiling_height_df \u001B[38;5;241m=\u001B[39m sample_cloud_base(dataframe)\n",
      "Cell \u001B[1;32mIn[245], line 13\u001B[0m, in \u001B[0;36msample_by_mean\u001B[1;34m(dataframe, *features)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msample_by_mean\u001B[39m(dataframe: pd\u001B[38;5;241m.\u001B[39mDataFrame, \u001B[38;5;241m*\u001B[39mfeatures: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame:\n\u001B[1;32m---> 13\u001B[0m     reduced_dataframe \u001B[38;5;241m=\u001B[39m \u001B[43mdataframe\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     15\u001B[0m     resampler \u001B[38;5;241m=\u001B[39m reduced_dataframe\u001B[38;5;241m.\u001B[39mresample(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mH\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     16\u001B[0m     reduced_dataframe \u001B[38;5;241m=\u001B[39m resampler\u001B[38;5;241m.\u001B[39mmean()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\pandas\\core\\frame.py:3813\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3811\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n\u001B[0;32m   3812\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[1;32m-> 3813\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcolumns\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m   3815\u001B[0m \u001B[38;5;66;03m# take() does not accept boolean indexers\u001B[39;00m\n\u001B[0;32m   3816\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(indexer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6070\u001B[0m, in \u001B[0;36mIndex._get_indexer_strict\u001B[1;34m(self, key, axis_name)\u001B[0m\n\u001B[0;32m   6067\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   6068\u001B[0m     keyarr, indexer, new_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_non_unique(keyarr)\n\u001B[1;32m-> 6070\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raise_if_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   6072\u001B[0m keyarr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtake(indexer)\n\u001B[0;32m   6073\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[0;32m   6074\u001B[0m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6133\u001B[0m, in \u001B[0;36mIndex._raise_if_missing\u001B[1;34m(self, key, indexer, axis_name)\u001B[0m\n\u001B[0;32m   6130\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   6132\u001B[0m not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m.\u001B[39munique())\n\u001B[1;32m-> 6133\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: \"['snow_drift:idx', 'ceiling_height_agl:m', 'snow_density:kgm3'] not in index\""
     ]
    }
   ],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=A.train_data, target_data=A.target_data, is_cat_boost=True)\n",
    "test_data = pre_process_test_data(test_data=A.test_data, is_cat_boost=True)\n",
    "\n",
    "A.train_data = train_data\n",
    "A.target_data = target_data\n",
    "A.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:31:52.332691900Z",
     "start_time": "2023-11-12T15:31:52.081210200Z"
    }
   },
   "id": "2582b8b7f22b5750"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=B.train_data, target_data=B.target_data, is_cat_boost=True)\n",
    "test_data = pre_process_test_data(test_data=B.test_data, is_cat_boost=True)\n",
    "\n",
    "B.train_data = train_data\n",
    "B.target_data = target_data\n",
    "B.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9db8c1ae2d5eb2b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=C.train_data, target_data=C.target_data, is_cat_boost=True)\n",
    "test_data = pre_process_test_data(test_data=C.test_data, is_cat_boost=True)\n",
    "\n",
    "C.train_data = train_data\n",
    "C.target_data = target_data\n",
    "C.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18b136e2dbb714cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbf5195d1813b58c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_sinusoidal_features(df: pd.DataFrame, hour_col: str = 'hour', day_col: str = 'day_of_year') -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    dates = df.index.values\n",
    "    df['date_forecast'] = dates\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Create hour and day_of_year columns\n",
    "    df['hour'] = df['date_forecast'].dt.hour\n",
    "    df['day_of_year'] = df['date_forecast'].dt.dayofyear\n",
    "\n",
    "    # Extract week number using isocalendar()\n",
    "    df['week_of_year'] = df['date_forecast'].dt.isocalendar().week\n",
    "    df['month'] = df['date_forecast'].dt.month\n",
    "\n",
    "    # Constants for sinusoidal functions\n",
    "    hours_in_day = 24\n",
    "    days_in_year = 365.25  # Accounting for leap years\n",
    "    weeks_in_year = 52\n",
    "    months_in_year = 12\n",
    "\n",
    "    # Create sinusoidal features based on the hour of the day\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * (df[hour_col] - 18) / hours_in_day)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * (df[hour_col] - 18) / hours_in_day)\n",
    "\n",
    "    # Create sinusoidal features based on the day of the year\n",
    "    df['sin_day'] = np.sin(2 * np.pi * (df[day_col] - 355) / days_in_year)\n",
    "    df['cos_day'] = np.cos(2 * np.pi * (df[day_col] - 355) / days_in_year)\n",
    "\n",
    "    # Create sinusoidal features based on the week of the year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['week_of_year'] / weeks_in_year)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['week_of_year'] / weeks_in_year)\n",
    "\n",
    "    # Create sinusoidal features based on the month\n",
    "    df['sin_month'] = np.sin(2 * np.pi * df['month'] / months_in_year)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * df['month'] / months_in_year)\n",
    "\n",
    "    dates = df['date_forecast'].to_numpy()\n",
    "    df.index = dates\n",
    "    df.drop(columns=['hour', 'day_of_year', 'week_of_year', 'month', 'date_forecast'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_sinusoidal_features_for_location(location: LocationData) -> None:\n",
    "    train_data = location.train_data.copy()\n",
    "    test_data = location.test_data.copy()\n",
    "\n",
    "    train_data = create_sinusoidal_features(train_data)\n",
    "    test_data = create_sinusoidal_features(test_data)\n",
    "\n",
    "    train_data.sort_index(inplace=True)\n",
    "    test_data.sort_index(inplace=True)\n",
    "\n",
    "    location.train_data = train_data\n",
    "    location.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f48677ca909d56f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "create_sinusoidal_features_for_location(A)\n",
    "create_sinusoidal_features_for_location(B)\n",
    "create_sinusoidal_features_for_location(C)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d2159c768e2285"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining functions for CatBoost model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "871b8f994d7bd8e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val):\n",
    "    predictions = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, predictions)\n",
    "    return mae\n",
    "\n",
    "\n",
    "def train_catboost_ensemble(location, N_MODELS=5):\n",
    "    train_data = location.train_data\n",
    "    target_data = location.target_data\n",
    "\n",
    "    validation_data = train_data\n",
    "\n",
    "    validation_length = len(validation_data)\n",
    "    val_size = int(0.1 * validation_length)\n",
    "\n",
    "    mae_list = []\n",
    "    models = []\n",
    "    depth_values = [7, 8, 9, 9, 10]\n",
    "    learning_rate_values = [0.01, 0.04, 0.01, 0.02, 0.03]\n",
    "    summer_evaluated = []\n",
    "\n",
    "    for i in range(N_MODELS):\n",
    "        val_start_idx = int((i / N_MODELS) * (validation_length - val_size))\n",
    "\n",
    "        X_train = pd.concat([train_data[:val_start_idx], train_data[val_start_idx + val_size:]], axis=0)\n",
    "        y_train = pd.concat([target_data[:val_start_idx], target_data[val_start_idx + val_size:]], axis=0)\n",
    "        X_val = train_data[val_start_idx:val_start_idx + val_size]\n",
    "        y_val = target_data[val_start_idx:val_start_idx + val_size]\n",
    "\n",
    "        date_at_middle_of_val = pd.to_datetime(X_train.index[val_start_idx + int(val_size / 2)])\n",
    "        evaluation_month = date_at_middle_of_val.month + 7\n",
    "        if evaluation_month > 12:\n",
    "            evaluation_month -= 12\n",
    "\n",
    "        summer_months = [5, 6, 7]\n",
    "        is_summer = evaluation_month in summer_months\n",
    "        summer_evaluated.append(is_summer)\n",
    "\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=20000,\n",
    "            depth=depth_values[i],\n",
    "            learning_rate=learning_rate_values[i],\n",
    "            loss_function='MAE',\n",
    "            od_type='Iter',\n",
    "            od_wait=100,\n",
    "            l2_leaf_reg=3,\n",
    "            random_seed=42,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # Fit the model using the validation set for early stopping\n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True, early_stopping_rounds=100)\n",
    "\n",
    "        # After training, evaluate the final model on the validation set\n",
    "        final_mae = evaluate_model(model, X_val, y_val)\n",
    "        print(f\"Model {i + 1} M[{evaluation_month}]: MAE:\\t{round(final_mae, 4)}\")\n",
    "\n",
    "        mae_list.append(final_mae)\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    print(f\"Average MAE:\\t{round(np.mean(mae_list), 4)}\")\n",
    "\n",
    "    return models, summer_evaluated\n",
    "\n",
    "\n",
    "def get_ensemble_predictions(models, test_data, summer_evaluated):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        predictions = model.predict(test_data)\n",
    "        if summer_evaluated[i]:\n",
    "            all_predictions.extend([\n",
    "                predictions,\n",
    "                predictions,\n",
    "                predictions,\n",
    "                predictions,\n",
    "            ])\n",
    "        else:\n",
    "            all_predictions.append(predictions)\n",
    "\n",
    "    ensemble_predictions = np.mean(all_predictions, axis=0)\n",
    "    return ensemble_predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e04fc78b95151b8e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training CatBoost models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62597d58cb55938d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "A_models, A_summer_evaluated = train_catboost_ensemble(location=A)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bd8e91049e19beb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B_models, B_summer_evaluated = train_catboost_ensemble(location=B)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb284a12468226b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "C_models, C_summer_evaluated = train_catboost_ensemble(location=C)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae149c86170b5b16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CatBoost predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a05099bb51fcec9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "A_prediction = get_ensemble_predictions(A_models, A.test_data, A_summer_evaluated)\n",
    "B_prediction = get_ensemble_predictions(B_models, B.test_data, B_summer_evaluated)\n",
    "C_prediction = get_ensemble_predictions(C_models, C.test_data, C_summer_evaluated)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f0373b4e9113427"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction_list = [A_prediction, B_prediction, C_prediction]\n",
    "predictions = np.concatenate(prediction_list)\n",
    "catboost_predictions = predictions.clip(min=0, max=None)\n",
    "prediction_list.append(catboost_predictions)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ead482e9a4e7e44"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b91d04668e2728d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17eac1d7b99002",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for prediction in prediction_list:\n",
    "    plt.figure(figsize=(30, 7))\n",
    "    plt.plot(catboost_predictions, 'b-')\n",
    "    plt.plot(autogloun_predictions, 'r-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c29779",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = [np.mean([autogloun_predictions], axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd360cbb63a5fc",
   "metadata": {},
   "source": [
    "# Saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4e0f1c8748630",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "\n",
    "def create_csv(*prediction_list: np.ndarray) -> None:\n",
    "    prediction_path: str = os.path.join(\n",
    "        DATA_DIR,\n",
    "        \"predictions\",\n",
    "        time.strftime(\"%Y%m%d-%H%M%S\") + \".csv\",\n",
    "    )\n",
    "\n",
    "    output: np.ndarray = np.concatenate(prediction_list)\n",
    "    output = output.clip(min=0, max=None)\n",
    "    indexes = np.arange(0, len(output), 1, dtype=int)\n",
    "\n",
    "    data = {\n",
    "        \"id\": indexes,\n",
    "        \"prediction\": output\n",
    "    }\n",
    "\n",
    "    dataframe: pd.DataFrame = pd.DataFrame(data)\n",
    "    dataframe.to_csv(prediction_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4562075a901b4",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_csv(*prediction_list)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
