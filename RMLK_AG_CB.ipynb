{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "26d9f234-58de-4fc9-9f4e-443ea8265844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:12.450499200Z",
     "start_time": "2023-11-12T17:20:12.122073300Z"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ea0ba8bc12eaef6f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:12.714265300Z",
     "start_time": "2023-11-12T17:20:12.127073200Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from catboost import CatBoostRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850e2dfc3bb16aa",
   "metadata": {},
   "source": [
    "Setting rules for libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8081b47cf627a139",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:12.963246200Z",
     "start_time": "2023-11-12T17:20:12.169075800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Warnings\n",
    "\n",
    "# Pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f768895a70a9bd57",
   "metadata": {},
   "source": [
    "# Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cf20b5c6b0ced7a4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:12.973237400Z",
     "start_time": "2023-11-12T17:20:12.195589700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Features to remove\n",
    "DATE_CALC: str = \"date_calc\"\n",
    "DATE_FORECAST: str = \"date_forecast\"\n",
    "ABSOLUTE_HUMIDITY: str = 'absolute_humidity_2m:gm3'\n",
    "AIR_DENSITY: str = 'air_density_2m:kgm3'\n",
    "CEILING_HEIGHT: str = 'ceiling_height_agl:m'\n",
    "CLEAR_SKY_ENERGY: str = 'clear_sky_energy_1h:J'\n",
    "CLEAR_SKY_RAD: str = 'clear_sky_rad:W'\n",
    "CLOUD_BASE: str = 'cloud_base_agl:m'\n",
    "DEW_OR_RIME: str = 'dew_or_rime:idx'\n",
    "DEW_POINT: str = 'dew_point_2m:K'\n",
    "DIFFUSE_RAD: str = 'diffuse_rad:W'\n",
    "DIFFUSE_RAD_1H: str = 'diffuse_rad_1h:J'\n",
    "DIRECT_RAD: str = 'direct_rad:W'\n",
    "DIRECT_RAD_1H: str = 'direct_rad_1h:J'\n",
    "EFFECTIVE_CLOUD_COVER: str = 'effective_cloud_cover:p'\n",
    "ELEVATION: str = 'elevation:m'\n",
    "FRESH_SNOW_12H: str = 'fresh_snow_12h:cm'\n",
    "FRESH_SNOW_1H: str = 'fresh_snow_1h:cm'\n",
    "FRESH_SNOW_24H: str = 'fresh_snow_24h:cm'\n",
    "FRESH_SNOW_3H: str = 'fresh_snow_3h:cm'\n",
    "FRESH_SNOW_6H: str = 'fresh_snow_6h:cm'\n",
    "IS_DAY: str = 'is_day:idx'\n",
    "IS_IN_SHADOW: str = 'is_in_shadow:idx'\n",
    "MSL_PRESSURE: str = 'msl_pressure:hPa'\n",
    "PRECIP_5MIN: str = 'precip_5min:mm'\n",
    "PRECIP_TYPE_5MIN: str = 'precip_type_5min:idx'\n",
    "PRESSURE_100M: str = 'pressure_100m:hPa'\n",
    "PRESSURE_50M: str = 'pressure_50m:hPa'\n",
    "PROB_RIME: str = 'prob_rime:p'\n",
    "RAIN_WATER: str = 'rain_water:kgm2'\n",
    "RELATIVE_HUMIDITY: str = 'relative_humidity_1000hPa:p'\n",
    "SFC_PRESSURE: str = 'sfc_pressure:hPa'\n",
    "SNOW_DENSITY: str = 'snow_density:kgm3'\n",
    "SNOW_DEPTH: str = 'snow_depth:cm'\n",
    "SNOW_DRIFT: str = 'snow_drift:idx'\n",
    "SNOW_MELT_10MIN: str = 'snow_melt_10min:mm'\n",
    "SNOW_WATER: str = 'snow_water:kgm2'\n",
    "SUN_AZIMUTH: str = 'sun_azimuth:d'\n",
    "SUN_ELEVATION: str = 'sun_elevation:d'\n",
    "SUPER_COOLED_LIQUID_WATER: str = 'super_cooled_liquid_water:kgm2'\n",
    "T_1000HPA: str = 't_1000hPa:K'\n",
    "TOTAL_CLOUD_COVER: str = 'total_cloud_cover:p'\n",
    "VISIBILITY: str = 'visibility:m'\n",
    "WIND_SPEED_10M: str = 'wind_speed_10m:ms'\n",
    "WIND_SPEED_U_10M: str = 'wind_speed_u_10m:ms'\n",
    "WIND_SPEED_V_10M: str = 'wind_speed_v_10m:ms'\n",
    "WIND_SPEED_W_1000HPA: str = 'wind_speed_w_1000hPa:ms'\n",
    "IS_ESTIMATED = 'is_estimated'\n",
    "\n",
    "TIME = 'time'\n",
    "PV_MEASUREMENT = 'pv_measurement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8e5747184b6faafe",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:13.043853Z",
     "start_time": "2023-11-12T17:20:12.204590900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def read_parquet(filepath: str) -> pd.DataFrame:\n",
    "    dataframe: pd.DataFrame = pd.read_parquet(filepath)\n",
    "    if DATE_CALC in dataframe.columns:\n",
    "        dataframe = dataframe.drop(columns=[DATE_CALC])\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def concat_observed_and_estimated_train_data(\n",
    "        X_observed: pd.DataFrame,\n",
    "        X_estimated: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    X_observed: pd.DataFrame = X_observed.copy()\n",
    "    X_estimated: pd.DataFrame = X_estimated.copy()\n",
    "\n",
    "    assert X_observed.shape[1] == X_estimated.shape[1]\n",
    "\n",
    "    concatenated_dataframe: pd.DataFrame = pd.concat(\n",
    "        objs=[X_observed, X_estimated],\n",
    "        axis=0\n",
    "    ).sort_values(\n",
    "        by=DATE_FORECAST,\n",
    "        ascending=True\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return concatenated_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a2f38c501e227ecc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:13.048760100Z",
     "start_time": "2023-11-12T17:20:12.222592500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data transfer object for location data\n",
    "class LocationData:\n",
    "    combined_data: pd.DataFrame\n",
    "\n",
    "    def __init__(self, train_data, target_data, test_data) -> None:\n",
    "        self.train_data = train_data\n",
    "        self.target_data = target_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "        self.original_train_data = train_data.copy()\n",
    "        self.original_target_data = target_data.copy()\n",
    "        self.original_test_data = test_data.copy()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Train: {self.train_data.shape} [No. NaN: {self.train_data.isna().sum().sum()}], Target: {self.target_data.shape} [No. NaN: {self.target_data.isna().sum().sum()}], Test: {self.test_data.shape} [No. NaN: {self.test_data.isna().sum().sum()}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "60d61a1645354f7c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:13.768980400Z",
     "start_time": "2023-11-12T17:20:12.239593600Z"
    }
   },
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/A/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/A/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/A/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/A/X_test_estimated.parquet\")\n",
    "\n",
    "observed_train['is_estimated'] = 0\n",
    "estimated_train['is_estimated'] = 1\n",
    "test_data['is_estimated'] = 1\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d6374820ca84a630",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:13.801549600Z",
     "start_time": "2023-11-12T17:20:12.331306400Z"
    }
   },
   "outputs": [],
   "source": [
    "A = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "58333e566cc3633f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:13.852546900Z",
     "start_time": "2023-11-12T17:20:12.346305100Z"
    }
   },
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/B/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/B/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/B/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/B/X_test_estimated.parquet\")\n",
    "\n",
    "observed_train['is_estimated'] = 0\n",
    "estimated_train['is_estimated'] = 1\n",
    "test_data['is_estimated'] = 1\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")\n",
    "\n",
    "B = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "edbd3f707ed3d2f7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:13.855545Z",
     "start_time": "2023-11-12T17:20:12.440493100Z"
    }
   },
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/C/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/C/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/C/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/C/X_test_estimated.parquet\")\n",
    "\n",
    "observed_train['is_estimated'] = 0\n",
    "estimated_train['is_estimated'] = 1\n",
    "test_data['is_estimated'] = 1\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7a3ab86d84357bed",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:13.855545Z",
     "start_time": "2023-11-12T17:20:12.534013100Z"
    }
   },
   "outputs": [],
   "source": [
    "C = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b3d3d8fec6811b18",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:13.856544900Z",
     "start_time": "2023-11-12T17:20:12.553015300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Train: (136245, 47) [No. NaN: 168040], Target: (34085, 2) [No. NaN: 0], Test: (2880, 47) [No. NaN: 3971]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (134505, 47) [No. NaN: 158811], Target: (32848, 2) [No. NaN: 4], Test: (2880, 47) [No. NaN: 3912]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (134401, 47) [No. NaN: 157326], Target: (32155, 2) [No. NaN: 6060], Test: (2880, 47) [No. NaN: 4104]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0124801bff563",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Defining functions for preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f4e6db271eea0",
   "metadata": {},
   "source": [
    "Function to remove features and replace `NaN` values with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "926a2ddc44c56339",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:13.856544900Z",
     "start_time": "2023-11-12T17:20:12.597634400Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_feature(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    dataframe = dataframe.drop(columns=[*features])\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def replace_nan_with_zero(dataframe: pd.DataFrame, feature: str) -> pd.DataFrame:\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe[[feature]] = dataframe[[feature]].fillna(0)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f994238640695aa",
   "metadata": {},
   "source": [
    "Function to resample data by categorizing them\n",
    "- Features to average\n",
    "- Features to pick a value\n",
    "- Features to change redefine `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "76f0e24c1ba0fe4b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:13.856544900Z",
     "start_time": "2023-11-12T17:20:12.616634400Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_by_selection(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "    if reduced_dataframe.isna().sum().sum() > 0:\n",
    "        print(\"[ERROR] Sample by selection got dataframe with NaN values\")\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.last()\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_by_mean(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.mean()\n",
    "    \n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_by_sum(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.sum()\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_cloud_base(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    cloud_base_frame = dataframe[[CLOUD_BASE]]\n",
    "\n",
    "    # Filling NaN values with -2_000\n",
    "    filled_frame = cloud_base_frame.copy()\n",
    "    filled_frame = filled_frame.fillna(-100_000)\n",
    "\n",
    "    # Resampling to 1H\n",
    "    resampler = filled_frame.resample('H')\n",
    "    cloud_base_frame = resampler.mean()\n",
    "\n",
    "    # Replacing negative values with -666\n",
    "    cloud_bases = cloud_base_frame[CLOUD_BASE].to_numpy()\n",
    "    negative_indexes = np.where(cloud_bases < 0)[0]\n",
    "    cloud_bases[negative_indexes] = -666\n",
    "    cloud_base_frame[CLOUD_BASE] = cloud_bases\n",
    "\n",
    "    # Dropping NaN values that were introduced by resampling\n",
    "    reduced_dataframe = cloud_base_frame.copy()\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def merge_frames(*dataframes: pd.DataFrame) -> pd.DataFrame:\n",
    "    indexes = [df.index for df in dataframes]\n",
    "\n",
    "    # Finding intersecting indexes in list of indexes, the indexes are datetime\n",
    "    intersecting_indexes = list(set(indexes[0]).intersection(*indexes))\n",
    "    filtered_dataframes = [df.loc[intersecting_indexes] for df in dataframes]\n",
    "\n",
    "    return pd.concat(filtered_dataframes, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4830f59eace7ae",
   "metadata": {},
   "source": [
    "Functions to synchronze dates in weather data and target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "98926ee1a8a03451",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:13.856544900Z",
     "start_time": "2023-11-12T17:20:12.630631500Z"
    }
   },
   "outputs": [],
   "source": [
    "def synchronize_timestamps(train_data: pd.DataFrame, target_data: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # IndexError if index in both train data and target data is DateTimeIndex\n",
    "    if not isinstance(train_data.index, pd.DatetimeIndex):\n",
    "        raise IndexError(\"Train data does not have DateTimeIndex\")\n",
    "\n",
    "    if not isinstance(target_data.index, pd.DatetimeIndex):\n",
    "        raise IndexError(\"Target data does not have DateTimeIndex\")\n",
    "\n",
    "    train_data.sort_index(inplace=True)\n",
    "    target_data.sort_index(inplace=True)\n",
    "\n",
    "    train_dates = train_data.index.values\n",
    "    target_dates = target_data.index.values\n",
    "\n",
    "    # Finding intersecting dates and filtering dataframes\n",
    "    intersecting_dates = np.intersect1d(train_dates, target_dates)\n",
    "    train_data = train_data.loc[intersecting_dates]\n",
    "    target_data = target_data.loc[intersecting_dates]\n",
    "\n",
    "    return train_data, target_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dateset contains many constant values for some features, any row with constant values is removed. If there is are at least 24 rows with constant values, the rows are removed.\n",
    "- -666 in cloud base should not be removed as this is added in earlier "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f0c01434829e1ae"
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "472023231528654c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:13.856544900Z",
     "start_time": "2023-11-12T17:20:12.643633Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_constant_measurements(\n",
    "        train_data: pd.DataFrame,\n",
    "        target_data: pd.DataFrame,\n",
    "        THRESHOLD: int = 24\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    indexes = []\n",
    "    count = 0\n",
    "    previous_value = None\n",
    "    start_index = None\n",
    "\n",
    "    # Setting date indexes as columns and resetting index\n",
    "    train_data[DATE_FORECAST] = train_data.index.values\n",
    "    target_data[TIME] = target_data.index.values\n",
    "\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    target_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Assuming y is a DataFrame and has a column for pv_measurement values\n",
    "    pv_measurements = target_data[PV_MEASUREMENT].to_numpy()\n",
    "\n",
    "    # Identifying indexes of consecutive constant values\n",
    "    for index, value in enumerate(pv_measurements):\n",
    "        if value == previous_value:\n",
    "            count += 1\n",
    "            if count == 1:\n",
    "                start_index = index - 1\n",
    "            if count >= THRESHOLD and value > -0.1:\n",
    "                indexes.extend(range(start_index, index + 1))\n",
    "        else:\n",
    "            count = 0\n",
    "        previous_value = value\n",
    "\n",
    "    # Assuming y has a column for dates and X has a similar column to filter on\n",
    "    dates = target_data.loc[indexes, TIME].unique()\n",
    "\n",
    "    filtered_train_data = train_data[~train_data[DATE_FORECAST].isin(dates)]\n",
    "    filtered_target_data = target_data[~target_data[TIME].isin(dates)]\n",
    "\n",
    "    # Switching back to date as index\n",
    "    filtered_train_data = filtered_train_data.set_index(DATE_FORECAST)\n",
    "    filtered_target_data = filtered_target_data.set_index(TIME)\n",
    "\n",
    "    return filtered_train_data, filtered_target_data\n",
    "\n",
    "\n",
    "def drop_nan_values_from_target_data(train_data: pd.DataFrame, target_data: pd.DataFrame) -> tuple[\n",
    "    pd.DataFrame, pd.DataFrame]:\n",
    "    target_data.dropna(axis=0, how='any', inplace=True)\n",
    "    train_data, target_data = synchronize_timestamps(train_data, target_data)\n",
    "    return train_data, target_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948b908f0f59cef",
   "metadata": {},
   "source": [
    "## Adding lag features, this has to come here because of the resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c1317434ce4b07e5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:13.856544900Z",
     "start_time": "2023-11-12T17:20:12.660150100Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_lag_features(dataframe: pd.DataFrame, features: list[str], lags: list[int]) -> tuple[pd.DataFrame, list[str]]:\n",
    "    dataframe = dataframe.copy()\n",
    "    lag_feature_names: list[str] = []\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            lag_feature_name = f\"{feature}_lag_{lag}\"\n",
    "            dataframe[lag_feature_name] = dataframe[feature].shift(lag)\n",
    "            lag_feature_names.append(lag_feature_name)\n",
    "\n",
    "    return dataframe, lag_feature_names\n",
    "\n",
    "\n",
    "def add_lead_features(dataframe: pd.DataFrame, features: list[str], lags: list[int]) -> tuple[pd.DataFrame, list[str]]:\n",
    "    dataframe = dataframe.copy()\n",
    "    lead_feature_names: list[str] = []\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            lead_feature_name = f\"{feature}_lead_{lag}\"\n",
    "            dataframe[lead_feature_name] = dataframe[feature].shift(-lag)\n",
    "            lead_feature_names.append(lead_feature_name)\n",
    "\n",
    "    return dataframe, lead_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4514f22c17b74b8e",
   "metadata": {},
   "source": [
    "Pipeline for preprocessing that is common for both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "92c2733c9a43a644",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:13.856544900Z",
     "start_time": "2023-11-12T17:20:12.680156900Z"
    }
   },
   "outputs": [],
   "source": [
    "def common_pre_processing(dataframe: pd.DataFrame, is_cat_boost: bool) -> pd.DataFrame:\n",
    "    features_to_drop = [SNOW_DRIFT, CEILING_HEIGHT, SNOW_DENSITY]\n",
    "    \n",
    "    if is_cat_boost: \n",
    "        dataframe = remove_feature(dataframe, *features_to_drop)\n",
    "\n",
    "    dataframe = dataframe.set_index(DATE_FORECAST)\n",
    "    feature_to_lag_and_lead: list[str] = [DIRECT_RAD]\n",
    "\n",
    "    # Adding lag and lead features\n",
    "    lag_features = []\n",
    "    lead_features = []\n",
    "\n",
    "    if is_cat_boost:\n",
    "        dataframe, lag_features = add_lag_features(dataframe, feature_to_lag_and_lead, [1, 2, 3])\n",
    "        dataframe, lead_features = add_lead_features(dataframe, feature_to_lag_and_lead, [1, 2, 3])\n",
    "\n",
    "    # Resample data to hours\n",
    "    selection_features = [\n",
    "        CLEAR_SKY_ENERGY,  # Maybe move\n",
    "        DIRECT_RAD_1H,  # MAYBE MOVE\n",
    "        DIFFUSE_RAD_1H,  # Maybe move\n",
    "        FRESH_SNOW_1H,\n",
    "        FRESH_SNOW_3H,\n",
    "        FRESH_SNOW_6H,\n",
    "        FRESH_SNOW_12H,\n",
    "        FRESH_SNOW_24H,\n",
    "        DEW_OR_RIME,\n",
    "        IS_DAY,\n",
    "        IS_IN_SHADOW,\n",
    "        PRECIP_TYPE_5MIN,\n",
    "        ELEVATION,\n",
    "        IS_ESTIMATED,\n",
    "        *lag_features,\n",
    "        *lead_features,\n",
    "    ]\n",
    "\n",
    "    mean_features = [\n",
    "        ABSOLUTE_HUMIDITY,\n",
    "        CLEAR_SKY_RAD,\n",
    "        DIFFUSE_RAD,\n",
    "        DIRECT_RAD,\n",
    "        AIR_DENSITY,\n",
    "        DEW_POINT,\n",
    "        MSL_PRESSURE,\n",
    "        PRESSURE_100M,\n",
    "        PRESSURE_50M,\n",
    "        SFC_PRESSURE,\n",
    "        RELATIVE_HUMIDITY,\n",
    "        T_1000HPA,\n",
    "        SUN_AZIMUTH,\n",
    "        SUN_ELEVATION,\n",
    "        EFFECTIVE_CLOUD_COVER,\n",
    "        TOTAL_CLOUD_COVER,\n",
    "        VISIBILITY,\n",
    "        WIND_SPEED_10M,\n",
    "        WIND_SPEED_U_10M,\n",
    "        WIND_SPEED_V_10M,\n",
    "        WIND_SPEED_W_1000HPA,\n",
    "        SNOW_MELT_10MIN,\n",
    "        PROB_RIME,\n",
    "        SUPER_COOLED_LIQUID_WATER,\n",
    "        SNOW_DEPTH, \n",
    "    ]\n",
    "    \n",
    "    sum_features = [\n",
    "        SNOW_WATER,\n",
    "        PRECIP_5MIN,\n",
    "        RAIN_WATER,\n",
    "    ]\n",
    "    \n",
    "    if not is_cat_boost:\n",
    "        for feature in features_to_drop:\n",
    "            mean_features.append(feature)\n",
    "\n",
    "    selection_df = sample_by_selection(dataframe, *selection_features)\n",
    "    mean_df = sample_by_mean(dataframe, *mean_features)\n",
    "    sum_df = sample_by_sum(dataframe, *sum_features)\n",
    "    ceiling_height_df = sample_cloud_base(dataframe)\n",
    "    dataframe = merge_frames(selection_df, mean_df, ceiling_height_df, sum_df)\n",
    "\n",
    "    if is_cat_boost:\n",
    "        dataframe = dataframe.dropna(axis=0, how='any')\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db91786254d2a576",
   "metadata": {},
   "source": [
    "### Preprocessing pipelines for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9556d94345fac3ba",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:13.856544900Z",
     "start_time": "2023-11-12T17:20:12.694569400Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process_train_data(\n",
    "        train_data: pd.DataFrame, \n",
    "        target_data: pd.DataFrame, \n",
    "        is_cat_boost: bool\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_data = common_pre_processing(dataframe=train_data, is_cat_boost=is_cat_boost)\n",
    "\n",
    "    # Setting time as index for target data\n",
    "    target_data[TIME] = pd.to_datetime(target_data[TIME])\n",
    "    target_data = target_data.set_index(TIME)\n",
    "\n",
    "    # Preprocessing that affects both train and target data\n",
    "    train_data, target_data = synchronize_timestamps(train_data=train_data, target_data=target_data)\n",
    "    train_data, target_data = remove_constant_measurements(train_data=train_data, target_data=target_data)\n",
    "    train_data, target_data = drop_nan_values_from_target_data(train_data=train_data, target_data=target_data)\n",
    "\n",
    "    return train_data, target_data\n",
    "\n",
    "\n",
    "def pre_process_test_data(test_data: pd.DataFrame, is_cat_boost: bool) -> pd.DataFrame:\n",
    "    if is_cat_boost:\n",
    "        test_data = common_pre_processing(dataframe=test_data, is_cat_boost=is_cat_boost)\n",
    "    else:\n",
    "        test_data = test_data.set_index(DATE_FORECAST)\n",
    "        test_data = test_data.groupby(test_data.index.floor('H')).mean()\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79beb4",
   "metadata": {},
   "source": [
    "# AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(\n",
    "    train_data=A.train_data,\n",
    "    target_data=A.target_data,\n",
    "    is_cat_boost=False\n",
    ")\n",
    "\n",
    "test_data = pre_process_test_data(test_data=A.test_data, is_cat_boost=False)\n",
    "A.train_data = train_data\n",
    "A.target_data = target_data\n",
    "A.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:14.070632900Z",
     "start_time": "2023-11-12T17:20:12.708267800Z"
    }
   },
   "id": "6ad5e575852aadfb"
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(\n",
    "    train_data=B.train_data,\n",
    "    target_data=B.target_data,\n",
    "    is_cat_boost=False\n",
    ")\n",
    "test_data = pre_process_test_data(test_data=B.test_data, is_cat_boost=False)\n",
    "B.train_data = train_data\n",
    "B.target_data = target_data\n",
    "B.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:14.361838600Z",
     "start_time": "2023-11-12T17:20:13.425169200Z"
    }
   },
   "id": "a4a11892d7383702"
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(\n",
    "    train_data=C.train_data,\n",
    "    target_data=C.target_data,\n",
    "    is_cat_boost=False\n",
    ")\n",
    "test_data = pre_process_test_data(test_data=C.test_data, is_cat_boost=False)\n",
    "C.train_data = train_data\n",
    "C.target_data = target_data\n",
    "C.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:15.277005800Z",
     "start_time": "2023-11-12T17:20:14.364838900Z"
    }
   },
   "id": "d7936de9ca1f1338"
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [
    {
     "data": {
      "text/plain": "Train: (34042, 46) [No. NaN: 40037], Target: (34042, 1) [No. NaN: 0], Test: (720, 46) [No. NaN: 978]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (25899, 46) [No. NaN: 30105], Target: (25899, 1) [No. NaN: 0], Test: (720, 46) [No. NaN: 965]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (21135, 46) [No. NaN: 26480], Target: (21135, 1) [No. NaN: 0], Test: (720, 46) [No. NaN: 1010]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:15.352535700Z",
     "start_time": "2023-11-12T17:20:15.278014400Z"
    }
   },
   "id": "53a3241e6b3792ef"
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a20a9cca-6d35-4e5a-aba2-ef43b90af3e8",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:15.394117400Z",
     "start_time": "2023-11-12T17:20:15.308526200Z"
    }
   },
   "outputs": [],
   "source": [
    "MINUTES: int = 1\n",
    "time_limit = int(60 * MINUTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "33e3fd13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:15.424103400Z",
     "start_time": "2023-11-12T17:20:15.328527500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merged files required by the AutoGluon model\n",
    "def merge_train_and_target(location: LocationData) -> pd.DataFrame:\n",
    "    train_data = location.train_data.copy()\n",
    "    target_data = location.target_data.copy()\n",
    "\n",
    "    return pd.merge(left=train_data, right=target_data, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "\n",
    "def prepare_data_for_ag(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    for col in dataframe.columns:\n",
    "        if dataframe[col].dtype == 'object':\n",
    "            dataframe[col] = dataframe[col].astype(str)\n",
    "        else:\n",
    "            dataframe[col] = pd.to_numeric(dataframe[col], errors='coerce')\n",
    "    \"\"\"\n",
    "\n",
    "    if pd.api.types.is_datetime64_any_dtype(dataframe.index):\n",
    "        dataframe = dataframe.sort_index()  # Sort by date if the index is a datetime\n",
    "        dataframe = dataframe.reset_index()  # Reset index\n",
    "\n",
    "    if DATE_FORECAST in dataframe.columns:\n",
    "        dataframe = dataframe.drop(columns=[DATE_FORECAST])\n",
    "\n",
    "    if TIME in dataframe.columns:\n",
    "        dataframe = dataframe.drop(columns=[TIME])\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "cc33d2a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:15.483106300Z",
     "start_time": "2023-11-12T17:20:15.341529400Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "combined_A = prepare_data_for_ag(merge_train_and_target(A))\n",
    "combined_B = prepare_data_for_ag(merge_train_and_target(B))\n",
    "combined_C = prepare_data_for_ag(merge_train_and_target(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "26d37c15-7870-48b9-8bf9-cb85db0caa68",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-12T17:20:15.529621500Z",
     "start_time": "2023-11-12T17:20:15.406098600Z"
    }
   },
   "outputs": [],
   "source": [
    "tuning_data_A = combined_A[combined_A['is_estimated'] == 1].tail(1100)\n",
    "tuning_data_B = combined_B[combined_B['is_estimated'] == 1].tail(950)\n",
    "tuning_data_C = combined_C[combined_C['is_estimated'] == 1].tail(870)\n",
    "\n",
    "combined_A = combined_A[(~combined_A.index.isin(tuning_data_A.index))]\n",
    "combined_B = combined_B[(~combined_B.index.isin(tuning_data_B.index))]\n",
    "combined_C = combined_C[(~combined_C.index.isin(tuning_data_C.index))]\n",
    "\n",
    "test_A = prepare_data_for_ag(A.test_data)\n",
    "test_B = prepare_data_for_ag(B.test_data)\n",
    "test_C = prepare_data_for_ag(C.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9c187ace-8822-4a31-b9cb-474fb0a01c75",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-12T17:21:17.057986700Z",
     "start_time": "2023-11-12T17:20:15.439096Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231112_172015\\\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=2, num_bag_folds=5, num_bag_sets=2\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231112_172015\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.18\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Disk Space Avail:   662.01 GB / 1022.87 GB (64.7%)\n",
      "Train Data Rows:    32942\n",
      "Train Data Columns: 46\n",
      "Tuning Data Rows:    1100\n",
      "Tuning Data Columns: 46\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 618.12095, 1151.76805)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    19389.72 MB\n",
      "\tTrain Data (Original)  Memory Usage: 6.4 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['snow_drift:idx']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['snow_drift:idx']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 45 | ['clear_sky_energy_1h:J', 'direct_rad_1h:J', 'diffuse_rad_1h:J', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 43 | ['clear_sky_energy_1h:J', 'direct_rad_1h:J', 'diffuse_rad_1h:J', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', ...]\n",
      "\t\t('int', ['bool']) :  2 | ['elevation:m', 'snow_density:kgm3']\n",
      "\t0.5s = Fit runtime\n",
      "\t45 features in original data used to generate 45 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.06 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 3 stack levels (L1 to L3) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 9 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 26.43s of the 59.49s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-278.8472\t = Validation score   (-mean_absolute_error)\n",
      "\t25.55s\t = Training   runtime\n",
      "\t23.87s\t = Validation runtime\n",
      "Completed 1/2 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.49s of the 24.86s of remaining time.\n",
      "\t-278.8472\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 16.55s of the 24.81s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-273.0829\t = Validation score   (-mean_absolute_error)\n",
      "\t5.9s\t = Training   runtime\n",
      "\t0.77s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 7.06s of the 15.32s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-277.5546\t = Validation score   (-mean_absolute_error)\n",
      "\t4.92s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Completed 1/2 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 59.49s of the 7.1s of remaining time.\n",
      "\t-273.0829\t = Validation score   (-mean_absolute_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 9 L3 models ...\n",
      "Fitting model: LightGBMXT_BAG_L3 ... Training model for up to 7.0s of the 6.97s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-266.8009\t = Validation score   (-mean_absolute_error)\n",
      "\t5.42s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Completed 1/2 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L4 ... Training model for up to 59.49s of the -1.42s of remaining time.\n",
      "\t-266.8009\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 61.46s ... Best model: \"WeightedEnsemble_L4\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231112_172015\\\")\n"
     ]
    }
   ],
   "source": [
    "predictor_A = TabularPredictor(\n",
    "    label='pv_measurement',\n",
    "    eval_metric=\"mean_absolute_error\"\n",
    ").fit(\n",
    "    train_data=combined_A,\n",
    "    tuning_data=tuning_data_A,\n",
    "    use_bag_holdout=True,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=2,\n",
    "    num_stack_levels=2,\n",
    "    time_limit=time_limit,\n",
    "    presets=\"best_quality\",\n",
    "    excluded_model_types=['KNN']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1961a928-3931-465c-924a-d6030ca0a0f3",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-12T17:22:23.387504300Z",
     "start_time": "2023-11-12T17:21:16.950455400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231112_172116\\\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=2, num_bag_folds=5, num_bag_sets=2\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231112_172116\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.18\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Disk Space Avail:   661.90 GB / 1022.87 GB (64.7%)\n",
      "Train Data Rows:    24949\n",
      "Train Data Columns: 46\n",
      "Tuning Data Rows:    950\n",
      "Tuning Data Columns: 46\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (1152.3, -0.0, 105.15761, 211.56122)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    18946.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 4.87 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 46 | ['clear_sky_energy_1h:J', 'direct_rad_1h:J', 'diffuse_rad_1h:J', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 44 | ['clear_sky_energy_1h:J', 'direct_rad_1h:J', 'diffuse_rad_1h:J', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', ...]\n",
      "\t\t('int', ['bool']) :  2 | ['elevation:m', 'snow_density:kgm3']\n",
      "\t0.2s = Fit runtime\n",
      "\t46 features in original data used to generate 46 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 4.71 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.2s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 3 stack levels (L1 to L3) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 9 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 26.57s of the 59.79s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-44.5585\t = Validation score   (-mean_absolute_error)\n",
      "\t26.22s\t = Training   runtime\n",
      "\t20.64s\t = Validation runtime\n",
      "Completed 1/2 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.8s of the 22.93s of remaining time.\n",
      "\t-44.5585\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 15.26s of the 22.88s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-42.4672\t = Validation score   (-mean_absolute_error)\n",
      "\t7.92s\t = Training   runtime\n",
      "\t0.94s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 3.41s of the 11.03s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-44.308\t = Validation score   (-mean_absolute_error)\n",
      "\t4.05s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Completed 1/2 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 59.8s of the 3.62s of remaining time.\n",
      "\t-42.4672\t = Validation score   (-mean_absolute_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 9 L3 models ...\n",
      "Fitting model: LightGBMXT_BAG_L3 ... Training model for up to 3.52s of the 3.49s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-41.9416\t = Validation score   (-mean_absolute_error)\n",
      "\t5.91s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Completed 1/2 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L4 ... Training model for up to 59.8s of the -6.26s of remaining time.\n",
      "\t-41.9416\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 66.31s ... Best model: \"WeightedEnsemble_L4\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231112_172116\\\")\n"
     ]
    }
   ],
   "source": [
    "predictor_B = TabularPredictor(\n",
    "    label='pv_measurement',\n",
    "    eval_metric=\"mean_absolute_error\"\n",
    ").fit(\n",
    "    train_data=combined_B,\n",
    "    tuning_data=tuning_data_B,\n",
    "    use_bag_holdout=True,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=2,\n",
    "    num_stack_levels=2,\n",
    "    time_limit=time_limit,\n",
    "    presets=\"best_quality\",\n",
    "    excluded_model_types=['KNN']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "89e18a77-3142-4682-82e2-d9fd6c53a42b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-12T17:23:26.973779300Z",
     "start_time": "2023-11-12T17:22:23.307964800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231112_172223\\\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=2, num_bag_folds=5, num_bag_sets=2\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231112_172223\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.18\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Disk Space Avail:   661.80 GB / 1022.87 GB (64.7%)\n",
      "Train Data Rows:    20265\n",
      "Train Data Columns: 46\n",
      "Tuning Data Rows:    870\n",
      "Tuning Data Columns: 46\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
      "\tLabel info (max, min, mean, stddev): (999.6, 0.0, 94.9687, 179.79421)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    18981.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 3.97 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['snow_drift:idx']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['snow_drift:idx']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 45 | ['clear_sky_energy_1h:J', 'direct_rad_1h:J', 'diffuse_rad_1h:J', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 43 | ['clear_sky_energy_1h:J', 'direct_rad_1h:J', 'diffuse_rad_1h:J', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', ...]\n",
      "\t\t('int', ['bool']) :  2 | ['elevation:m', 'snow_density:kgm3']\n",
      "\t0.4s = Fit runtime\n",
      "\t45 features in original data used to generate 45 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.76 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.42s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 3 stack levels (L1 to L3) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 9 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 26.47s of the 59.57s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-46.5067\t = Validation score   (-mean_absolute_error)\n",
      "\t25.67s\t = Training   runtime\n",
      "\t17.69s\t = Validation runtime\n",
      "Completed 1/2 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.58s of the 23.76s of remaining time.\n",
      "\t-46.5067\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 15.81s of the 23.7s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-50.2232\t = Validation score   (-mean_absolute_error)\n",
      "\t6.6s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 5.43s of the 13.32s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-46.8253\t = Validation score   (-mean_absolute_error)\n",
      "\t5.01s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Completed 1/2 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 59.58s of the 5.15s of remaining time.\n",
      "\t-46.8253\t = Validation score   (-mean_absolute_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 9 L3 models ...\n",
      "Fitting model: LightGBMXT_BAG_L3 ... Training model for up to 5.02s of the 5.0s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-50.0967\t = Validation score   (-mean_absolute_error)\n",
      "\t4.7s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Completed 1/2 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L4 ... Training model for up to 59.58s of the -3.38s of remaining time.\n",
      "\t-50.0967\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 63.42s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231112_172223\\\")\n"
     ]
    }
   ],
   "source": [
    "predictor_C = TabularPredictor(\n",
    "    label='pv_measurement',\n",
    "    eval_metric=\"mean_absolute_error\"\n",
    ").fit(\n",
    "    train_data=combined_C,\n",
    "    tuning_data=tuning_data_C,\n",
    "    use_bag_holdout=True,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=2,\n",
    "    num_stack_levels=2,\n",
    "    time_limit=time_limit,\n",
    "    presets=\"best_quality\",\n",
    "    excluded_model_types=['KNN']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915985ab",
   "metadata": {},
   "source": [
    "### AutoGluon predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3c814a68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:23:30.761066300Z",
     "start_time": "2023-11-12T17:23:26.780725400Z"
    }
   },
   "outputs": [],
   "source": [
    "A_prediction = predictor_A.predict(test_A)\n",
    "B_prediction = predictor_B.predict(test_B)\n",
    "C_prediction = predictor_C.predict(test_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "312351cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:23:30.812773300Z",
     "start_time": "2023-11-12T17:23:30.764174500Z"
    }
   },
   "outputs": [],
   "source": [
    "autogloun_predictions = np.concatenate([A_prediction, B_prediction, C_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CatBoost\n",
    "## Preprocessing\n",
    "Resetting data objects"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c481a8ca6631fc9"
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "def reset_data_objects(location: LocationData) -> None:\n",
    "    location.train_data = location.original_train_data.copy()\n",
    "    location.target_data = location.original_target_data.copy()\n",
    "    location.test_data = location.original_test_data.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:23:30.822777800Z",
     "start_time": "2023-11-12T17:23:30.776250800Z"
    }
   },
   "id": "647ff8ee65e8503d"
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "reset_data_objects(A)\n",
    "reset_data_objects(B)\n",
    "reset_data_objects(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:23:30.853776400Z",
     "start_time": "2023-11-12T17:23:30.793250600Z"
    }
   },
   "id": "7a53b5de76fb9a40"
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "data": {
      "text/plain": "Train: (136245, 47) [No. NaN: 168040], Target: (34085, 2) [No. NaN: 0], Test: (2880, 47) [No. NaN: 3971]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (134505, 47) [No. NaN: 158811], Target: (32848, 2) [No. NaN: 4], Test: (2880, 47) [No. NaN: 3912]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (134401, 47) [No. NaN: 157326], Target: (32155, 2) [No. NaN: 6060], Test: (2880, 47) [No. NaN: 4104]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:23:30.892849200Z",
     "start_time": "2023-11-12T17:23:30.839774200Z"
    }
   },
   "id": "102748ff31a74103"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running preprocessing pipeline for CatBoost model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d830517c6140c225"
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Sample by selection got dataframe with NaN values\n",
      "[ERROR] Sample by selection got dataframe with NaN values\n"
     ]
    }
   ],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=A.train_data, target_data=A.target_data, is_cat_boost=True)\n",
    "test_data = pre_process_test_data(test_data=A.test_data, is_cat_boost=True)\n",
    "\n",
    "A.train_data = train_data\n",
    "A.target_data = target_data\n",
    "A.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:23:32.033291100Z",
     "start_time": "2023-11-12T17:23:30.889772500Z"
    }
   },
   "id": "2582b8b7f22b5750"
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Sample by selection got dataframe with NaN values\n",
      "[ERROR] Sample by selection got dataframe with NaN values\n"
     ]
    }
   ],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=B.train_data, target_data=B.target_data, is_cat_boost=True)\n",
    "test_data = pre_process_test_data(test_data=B.test_data, is_cat_boost=True)\n",
    "\n",
    "B.train_data = train_data\n",
    "B.target_data = target_data\n",
    "B.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:23:33.496714Z",
     "start_time": "2023-11-12T17:23:32.013275500Z"
    }
   },
   "id": "9db8c1ae2d5eb2b6"
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Sample by selection got dataframe with NaN values\n",
      "[ERROR] Sample by selection got dataframe with NaN values\n"
     ]
    }
   ],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=C.train_data, target_data=C.target_data, is_cat_boost=True)\n",
    "test_data = pre_process_test_data(test_data=C.test_data, is_cat_boost=True)\n",
    "\n",
    "C.train_data = train_data\n",
    "C.target_data = target_data\n",
    "C.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:23:34.715156200Z",
     "start_time": "2023-11-12T17:23:33.474698200Z"
    }
   },
   "id": "18b136e2dbb714cd"
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "data": {
      "text/plain": "Train: (34018, 49) [No. NaN: 0], Target: (34018, 1) [No. NaN: 0], Test: (720, 49) [No. NaN: 0]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (25875, 49) [No. NaN: 0], Target: (25875, 1) [No. NaN: 0], Test: (720, 49) [No. NaN: 0]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (21111, 49) [No. NaN: 0], Target: (21111, 1) [No. NaN: 0], Test: (720, 49) [No. NaN: 0]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:23:34.759161700Z",
     "start_time": "2023-11-12T17:23:34.686529300Z"
    }
   },
   "id": "2dd0b3962d03ffa4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbf5195d1813b58c"
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [],
   "source": [
    "def create_sinusoidal_features(df: pd.DataFrame, hour_col: str = 'hour', day_col: str = 'day_of_year') -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    dates = df.index.values\n",
    "    df['date_forecast'] = dates\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Create hour and day_of_year columns\n",
    "    df['hour'] = df['date_forecast'].dt.hour\n",
    "    df['day_of_year'] = df['date_forecast'].dt.dayofyear\n",
    "\n",
    "    # Extract week number using isocalendar()\n",
    "    df['week_of_year'] = df['date_forecast'].dt.isocalendar().week\n",
    "    df['month'] = df['date_forecast'].dt.month\n",
    "\n",
    "    # Constants for sinusoidal functions\n",
    "    hours_in_day = 24\n",
    "    days_in_year = 365.25  # Accounting for leap years\n",
    "    weeks_in_year = 52\n",
    "    months_in_year = 12\n",
    "\n",
    "    # Create sinusoidal features based on the hour of the day\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * (df[hour_col] - 18) / hours_in_day)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * (df[hour_col] - 18) / hours_in_day)\n",
    "\n",
    "    # Create sinusoidal features based on the day of the year\n",
    "    df['sin_day'] = np.sin(2 * np.pi * (df[day_col] - 355) / days_in_year)\n",
    "    df['cos_day'] = np.cos(2 * np.pi * (df[day_col] - 355) / days_in_year)\n",
    "\n",
    "    # Create sinusoidal features based on the week of the year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['week_of_year'] / weeks_in_year)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['week_of_year'] / weeks_in_year)\n",
    "\n",
    "    # Create sinusoidal features based on the month\n",
    "    df['sin_month'] = np.sin(2 * np.pi * df['month'] / months_in_year)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * df['month'] / months_in_year)\n",
    "\n",
    "    dates = df['date_forecast'].to_numpy()\n",
    "    df.index = dates\n",
    "    df.drop(columns=['hour', 'day_of_year', 'week_of_year', 'month', 'date_forecast'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_sinusoidal_features_for_location(location: LocationData) -> None:\n",
    "    train_data = location.train_data.copy()\n",
    "    test_data = location.test_data.copy()\n",
    "\n",
    "    train_data = create_sinusoidal_features(train_data)\n",
    "    test_data = create_sinusoidal_features(test_data)\n",
    "\n",
    "    train_data.sort_index(inplace=True)\n",
    "    test_data.sort_index(inplace=True)\n",
    "\n",
    "    location.train_data = train_data\n",
    "    location.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:23:34.802178400Z",
     "start_time": "2023-11-12T17:23:34.725271100Z"
    }
   },
   "id": "6f48677ca909d56f"
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [],
   "source": [
    "create_sinusoidal_features_for_location(A)\n",
    "create_sinusoidal_features_for_location(B)\n",
    "create_sinusoidal_features_for_location(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:23:34.985062500Z",
     "start_time": "2023-11-12T17:23:34.734158500Z"
    }
   },
   "id": "6d2159c768e2285"
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [
    {
     "data": {
      "text/plain": "Train: (34018, 57) [No. NaN: 0], Target: (34018, 1) [No. NaN: 0], Test: (720, 57) [No. NaN: 0]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (25875, 57) [No. NaN: 0], Target: (25875, 1) [No. NaN: 0], Test: (720, 57) [No. NaN: 0]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (21111, 57) [No. NaN: 0], Target: (21111, 1) [No. NaN: 0], Test: (720, 57) [No. NaN: 0]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:23:35.021582900Z",
     "start_time": "2023-11-12T17:23:34.876321900Z"
    }
   },
   "id": "f50b53df83c83aa6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining functions for CatBoost model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "871b8f994d7bd8e2"
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val):\n",
    "    predictions = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, predictions)\n",
    "    return mae\n",
    "\n",
    "\n",
    "def train_catboost_ensemble(location, N_MODELS=10):\n",
    "    train_data = location.train_data\n",
    "    target_data = location.target_data\n",
    "\n",
    "    total_length = len(train_data)\n",
    "    val_size = int(0.1 * total_length)\n",
    "\n",
    "    mae_list = []\n",
    "    models = []\n",
    "    depth_values = [7, 7, 8, 8, 8, 8, 9, 9, 9, 10]\n",
    "    learning_rate_values = [0.01, 0.04, 0.01, 0.02, 0.03, 0.04, 0.02, 0.03, 0.04, 0.03]\n",
    "    summer_evaluated = []\n",
    "    train_dates = train_data.index.values\n",
    "\n",
    "    for i in range(N_MODELS):\n",
    "        val_start_idx = int((i / N_MODELS) * (total_length - val_size))\n",
    "\n",
    "        X_train = pd.concat([train_data[:val_start_idx], train_data[val_start_idx + val_size:]], axis=0)\n",
    "        y_train = pd.concat([target_data[:val_start_idx], target_data[val_start_idx + val_size:]], axis=0)\n",
    "        X_val = train_data[val_start_idx:val_start_idx + val_size]\n",
    "        y_val = target_data[val_start_idx:val_start_idx + val_size]\n",
    "\n",
    "        date_at_middle_of_val = pd.to_datetime(X_train.index[val_start_idx + int(val_size / 2)])\n",
    "        evaluation_month = date_at_middle_of_val.month + 7\n",
    "        if evaluation_month > 12:\n",
    "            evaluation_month -= 12\n",
    "\n",
    "        summer_months = [5, 6, 7]\n",
    "        is_summer = evaluation_month in summer_months\n",
    "        summer_evaluated.append(is_summer)\n",
    "\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=20000,\n",
    "            depth=depth_values[i],\n",
    "            learning_rate=learning_rate_values[i],\n",
    "            loss_function='MAE',\n",
    "            od_type='Iter',\n",
    "            od_wait=100,\n",
    "            l2_leaf_reg=3,\n",
    "            random_seed=42,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # Fit the model using the validation set for early stopping\n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True, early_stopping_rounds=100)\n",
    "\n",
    "        # After training, evaluate the final model on the validation set\n",
    "        final_mae = evaluate_model(model, X_val, y_val)\n",
    "        print(f\"Model {i + 1} M[{evaluation_month}]: MAE:\\t{round(final_mae, 4)}\")\n",
    "\n",
    "        mae_list.append(final_mae)\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    print(f\"Average MAE:\\t{round(np.mean(mae_list), 4)}\")\n",
    "\n",
    "    return models, summer_evaluated\n",
    "\n",
    "\n",
    "def get_ensemble_predictions(models, test_data, summer_evaluated):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        predictions = model.predict(test_data)\n",
    "        if summer_evaluated[i]:\n",
    "            all_predictions.extend([\n",
    "                predictions,\n",
    "                predictions,\n",
    "                predictions,\n",
    "                predictions,\n",
    "            ])\n",
    "        else:\n",
    "            all_predictions.append(predictions)\n",
    "\n",
    "    ensemble_predictions = np.mean(all_predictions, axis=0)\n",
    "    return ensemble_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:23:35.021582900Z",
     "start_time": "2023-11-12T17:23:34.942082800Z"
    }
   },
   "id": "e04fc78b95151b8e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training CatBoost models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62597d58cb55938d"
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 M[8]: MAE:\t282.4645\n",
      "Model 2 M[12]: MAE:\t72.3829\n",
      "Model 3 M[4]: MAE:\t261.4866\n",
      "Model 4 M[8]: MAE:\t243.2635\n",
      "Model 5 M[12]: MAE:\t72.9029\n",
      "Model 6 M[5]: MAE:\t291.5898\n",
      "Model 7 M[9]: MAE:\t195.9941\n",
      "Model 8 M[1]: MAE:\t70.4181\n",
      "Model 9 M[5]: MAE:\t284.7359\n",
      "Model 10 M[10]: MAE:\t156.0276\n",
      "Average MAE:\t193.1266\n"
     ]
    }
   ],
   "source": [
    "A_models, A_summer_evaluated = train_catboost_ensemble(location=A)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:29:41.402215300Z",
     "start_time": "2023-11-12T17:23:34.954065600Z"
    }
   },
   "id": "9bd8e91049e19beb"
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 M[2]: MAE:\t24.6271\n",
      "Model 2 M[5]: MAE:\t64.3168\n",
      "Model 3 M[9]: MAE:\t27.3004\n",
      "Model 4 M[12]: MAE:\t9.6495\n",
      "Model 5 M[5]: MAE:\t53.8982\n",
      "Model 6 M[9]: MAE:\t20.7133\n",
      "Model 7 M[2]: MAE:\t22.5745\n",
      "Model 8 M[7]: MAE:\t53.643\n",
      "Model 9 M[6]: MAE:\t8.0122\n",
      "Model 10 M[10]: MAE:\t22.9655\n",
      "Average MAE:\t30.77\n"
     ]
    }
   ],
   "source": [
    "B_models, B_summer_evaluated = train_catboost_ensemble(location=B)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:32:47.896942900Z",
     "start_time": "2023-11-12T17:29:41.351210200Z"
    }
   },
   "id": "eb284a12468226b6"
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 M[9]: MAE:\t15.4571\n",
      "Model 2 M[12]: MAE:\t16.7236\n",
      "Model 3 M[3]: MAE:\t50.2088\n",
      "Model 4 M[6]: MAE:\t44.8549\n",
      "Model 5 M[10]: MAE:\t9.536\n",
      "Model 6 M[1]: MAE:\t19.952\n",
      "Model 7 M[3]: MAE:\t47.8173\n",
      "Model 8 M[6]: MAE:\t35.1647\n",
      "Model 9 M[6]: MAE:\t8.572\n",
      "Model 10 M[10]: MAE:\t18.0434\n",
      "Average MAE:\t26.633\n"
     ]
    }
   ],
   "source": [
    "C_models, C_summer_evaluated = train_catboost_ensemble(location=C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:35:55.198862900Z",
     "start_time": "2023-11-12T17:32:47.900900700Z"
    }
   },
   "id": "ae149c86170b5b16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CatBoost predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a05099bb51fcec9"
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [],
   "source": [
    "A_prediction = get_ensemble_predictions(A_models, A.test_data, A_summer_evaluated)\n",
    "B_prediction = get_ensemble_predictions(B_models, B.test_data, B_summer_evaluated)\n",
    "C_prediction = get_ensemble_predictions(C_models, C.test_data, C_summer_evaluated)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:35:55.405170Z",
     "start_time": "2023-11-12T17:35:55.202929900Z"
    }
   },
   "id": "9f0373b4e9113427"
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "prediction_list = [A_prediction, B_prediction, C_prediction]\n",
    "predictions = np.concatenate(prediction_list)\n",
    "catboost_predictions = predictions.clip(min=0, max=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:35:55.460659800Z",
     "start_time": "2023-11-12T17:35:55.404127900Z"
    }
   },
   "id": "f8b23491e45918e9"
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[218], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mprediction_list\u001B[49m\u001B[43m[\u001B[49m\u001B[43mautogloun_predictions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatboost_predictions\u001B[49m\u001B[43m]\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "prediction_list[autogloun_predictions, catboost_predictions]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:35:55.515465400Z",
     "start_time": "2023-11-12T17:35:55.420663500Z"
    }
   },
   "id": "5ead482e9a4e7e44"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b91d04668e2728d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17eac1d7b99002",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T17:35:55.479658Z"
    }
   },
   "outputs": [],
   "source": [
    "for prediction in prediction_list:\n",
    "    plt.figure(figsize=(30, 7))\n",
    "    plt.plot(catboost_predictions, 'b-')\n",
    "    plt.plot(autogloun_predictions, 'r-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c29779",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T17:35:55.483664Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction_list = [np.mean(prediction_list, axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Postprocessing: Removing negative values and setting night time values to zero"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c3e6b1b4455d05e"
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "def set_zero_in_time_range(data: pd.DataFrame, start_hour: int, end_hour: int):\n",
    "        time_column = TIME\n",
    "        if not pd.api.types.is_datetime64_any_dtype(data[time_column]):\n",
    "            data[time_column] = pd.to_datetime(data[time_column])\n",
    "\n",
    "        if start_hour <= end_hour:\n",
    "            mask = data[time_column].dt.hour.between(start_hour, end_hour, inclusive='left')\n",
    "        else:\n",
    "            mask = (data[time_column].dt.hour >= start_hour) | (data[time_column].dt.hour <= end_hour)\n",
    "\n",
    "        data.loc[mask, 'pv_measurement'] = 0\n",
    "        return data\n",
    "\n",
    "def set_zero_in_time(predictions: np.ndarray, start_hour: int, end_hour: int) -> np.ndarray:\n",
    "    locations: list[LocationData] = [A, B, C]\n",
    "    location_predictions: list[np.ndarray] = []\n",
    "\n",
    "    segment_size = len(predictions) // len(locations)\n",
    "\n",
    "    for i in range(len(locations)):\n",
    "        # Calculate the start and end indices for each segment\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = (i + 1) * segment_size\n",
    "\n",
    "        # Extract the segment from predictions and append to location_predictions\n",
    "        segment = predictions[start_idx:end_idx]\n",
    "        location_predictions.append(segment)\n",
    "        \n",
    "    data_list = []\n",
    "    for location, prediction in zip(locations, location_predictions):\n",
    "        test_dates = location.test_data.index.values\n",
    "        data = pd.DataFrame(data=prediction, index=test_dates, columns=[PV_MEASUREMENT])\n",
    "        data = set_zero_in_time_range(data, start_hour, end_hour)\n",
    "        data.append(data[PV_MEASUREMENT].to_numpy())\n",
    "        \n",
    "    return np.concatenate(data_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:50:34.935922800Z",
     "start_time": "2023-11-12T17:50:34.893771900Z"
    }
   },
   "id": "5449a8e8b4473476"
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [],
   "source": [
    "# predictions = set_zero_in_time(predictions, 21, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:50:39.861818600Z",
     "start_time": "2023-11-12T17:50:39.848378500Z"
    }
   },
   "id": "a4d84293cef3537c"
  },
  {
   "cell_type": "markdown",
   "id": "b1cd360cbb63a5fc",
   "metadata": {},
   "source": [
    "# Saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "69d4e0f1c8748630",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:50:39.892872900Z",
     "start_time": "2023-11-12T17:50:39.866819Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "\n",
    "def create_csv(*prediction_list: np.ndarray) -> None:\n",
    "    prediction_path: str = os.path.join(\n",
    "        DATA_DIR,\n",
    "        \"predictions\",\n",
    "        time.strftime(\"%Y%m%d-%H%M%S\") + \".csv\",\n",
    "    )\n",
    "\n",
    "    output: np.ndarray = np.concatenate(prediction_list)\n",
    "    output = output.clip(min=0, max=None)\n",
    "    indexes = np.arange(0, len(output), 1, dtype=int)\n",
    "\n",
    "    data = {\n",
    "        \"id\": indexes,\n",
    "        \"prediction\": output\n",
    "    }\n",
    "\n",
    "    dataframe: pd.DataFrame = pd.DataFrame(data)\n",
    "    dataframe.to_csv(prediction_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "9ad4562075a901b4",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-12T17:50:46.756365200Z",
     "start_time": "2023-11-12T17:50:46.647534100Z"
    }
   },
   "outputs": [],
   "source": [
    "create_csv(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5a90fb813388f7d9"
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
