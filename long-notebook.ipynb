{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:01:04.633372Z",
     "start_time": "2023-11-12T17:01:04.577106Z"
    }
   },
   "outputs": [],
   "source": [
    "# External imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.stl._stl import STL\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore, skew, kurtosis, probplot\n",
    "from pandas.plotting import autocorrelation_plot, lag_plot\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from scipy.signal import periodogram\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import os\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import clone\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_predict, RandomizedSearchCV, train_test_split\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from tqdm.notebook import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "from flaml import AutoML\n",
    "import joblib\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:01:05.388136Z",
     "start_time": "2023-11-12T17:01:05.303237Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Long Notebook, Eirik Varnes (StudID: 543576) , Jathavaan Shankarr (StudID:544755) [77] SindreFinnesDetEksternData"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting rules for libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "# Warnings\n",
    "\n",
    "# Pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:01:09.216887Z",
     "start_time": "2023-11-12T17:01:09.122958Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:01:10.772491Z",
     "start_time": "2023-11-12T17:01:10.706279Z"
    }
   },
   "outputs": [],
   "source": [
    "# Features to remove\n",
    "DATE_CALC: str = \"date_calc\"\n",
    "DATE_FORECAST: str = \"date_forecast\"\n",
    "ABSOLUTE_HUMIDITY: str = 'absolute_humidity_2m:gm3'\n",
    "AIR_DENSITY: str = 'air_density_2m:kgm3'\n",
    "CEILING_HEIGHT: str = 'ceiling_height_agl:m'\n",
    "CLEAR_SKY_ENERGY: str = 'clear_sky_energy_1h:J'\n",
    "CLEAR_SKY_RAD: str = 'clear_sky_rad:W'\n",
    "CLOUD_BASE: str = 'cloud_base_agl:m'\n",
    "DEW_OR_RIME: str = 'dew_or_rime:idx'\n",
    "DEW_POINT: str = 'dew_point_2m:K'\n",
    "DIFFUSE_RAD: str = 'diffuse_rad:W'\n",
    "DIFFUSE_RAD_1H: str = 'diffuse_rad_1h:J'\n",
    "DIRECT_RAD: str = 'direct_rad:W'\n",
    "DIRECT_RAD_1H: str = 'direct_rad_1h:J'\n",
    "EFFECTIVE_CLOUD_COVER: str = 'effective_cloud_cover:p'\n",
    "ELEVATION: str = 'elevation:m'\n",
    "FRESH_SNOW_12H: str = 'fresh_snow_12h:cm'\n",
    "FRESH_SNOW_1H: str = 'fresh_snow_1h:cm'\n",
    "FRESH_SNOW_24H: str = 'fresh_snow_24h:cm'\n",
    "FRESH_SNOW_3H: str = 'fresh_snow_3h:cm'\n",
    "FRESH_SNOW_6H: str = 'fresh_snow_6h:cm'\n",
    "IS_DAY: str = 'is_day:idx'\n",
    "IS_IN_SHADOW: str = 'is_in_shadow:idx'\n",
    "MSL_PRESSURE: str = 'msl_pressure:hPa'\n",
    "PRECIP_5MIN: str = 'precip_5min:mm'\n",
    "PRECIP_TYPE_5MIN: str = 'precip_type_5min:idx'\n",
    "PRESSURE_100M: str = 'pressure_100m:hPa'\n",
    "PRESSURE_50M: str = 'pressure_50m:hPa'\n",
    "PROB_RIME: str = 'prob_rime:p'\n",
    "RAIN_WATER: str = 'rain_water:kgm2'\n",
    "RELATIVE_HUMIDITY: str = 'relative_humidity_1000hPa:p'\n",
    "SFC_PRESSURE: str = 'sfc_pressure:hPa'\n",
    "SNOW_DENSITY: str = 'snow_density:kgm3'\n",
    "SNOW_DEPTH: str = 'snow_depth:cm'\n",
    "SNOW_DRIFT: str = 'snow_drift:idx'\n",
    "SNOW_MELT_10MIN: str = 'snow_melt_10min:mm'\n",
    "SNOW_WATER: str = 'snow_water:kgm2'\n",
    "SUN_AZIMUTH: str = 'sun_azimuth:d'\n",
    "SUN_ELEVATION: str = 'sun_elevation:d'\n",
    "SUPER_COOLED_LIQUID_WATER: str = 'super_cooled_liquid_water:kgm2'\n",
    "T_1000HPA: str = 't_1000hPa:K'\n",
    "TOTAL_CLOUD_COVER: str = 'total_cloud_cover:p'\n",
    "VISIBILITY: str = 'visibility:m'\n",
    "WIND_SPEED_10M: str = 'wind_speed_10m:ms'\n",
    "WIND_SPEED_U_10M: str = 'wind_speed_u_10m:ms'\n",
    "WIND_SPEED_V_10M: str = 'wind_speed_v_10m:ms'\n",
    "WIND_SPEED_W_1000HPA: str = 'wind_speed_w_1000hPa:ms'\n",
    "\n",
    "TIME = 'time'\n",
    "PV_MEASUREMENT = 'pv_measurement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:01:13.480504Z",
     "start_time": "2023-11-12T17:01:13.234819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def read_parquet(filepath: str) -> pd.DataFrame:\n",
    "    dataframe: pd.DataFrame = pd.read_parquet(filepath)\n",
    "    if DATE_CALC in dataframe.columns:\n",
    "        dataframe = dataframe.drop(columns=[DATE_CALC])\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def concat_observed_and_estimated_train_data(\n",
    "        X_observed: pd.DataFrame,\n",
    "        X_estimated: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    X_observed: pd.DataFrame = X_observed.copy()\n",
    "    X_estimated: pd.DataFrame = X_estimated.copy()\n",
    "\n",
    "    assert X_observed.shape[1] == X_estimated.shape[1]\n",
    "\n",
    "    concatenated_dataframe: pd.DataFrame = pd.concat(\n",
    "        objs=[X_observed, X_estimated],\n",
    "        axis=0\n",
    "    ).sort_values(\n",
    "        by=DATE_FORECAST,\n",
    "        ascending=True\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return concatenated_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "# Data transfer object for location data\n",
    "class LocationData:\n",
    "    def __init__(self, train_data, target_data, test_data) -> None:\n",
    "        self.train_data = train_data\n",
    "        self.target_data = target_data\n",
    "        self.test_data = test_data\n",
    "        \n",
    "        self.original_train_data = train_data\n",
    "        self.original_target_data = target_data\n",
    "        self.original_test_data = test_data\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Train: {self.train_data.shape} [No. NaN: {self.train_data.isna().sum().sum()}], Target: {self.target_data.shape} [No. NaN: {self.target_data.isna().sum().sum()}], Test: {self.test_data.shape} [No. NaN: {self.test_data.isna().sum().sum()}]\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:01:14.691895Z",
     "start_time": "2023-11-12T17:01:14.605468Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/A/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/A/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/A/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/A/X_test_estimated.parquet\")\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:01:15.665958Z",
     "start_time": "2023-11-12T17:01:15.450303Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "A = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:01:16.009730Z",
     "start_time": "2023-11-12T17:01:15.941086Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/B/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/B/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/B/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/B/X_test_estimated.parquet\")\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")\n",
    "\n",
    "B = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:01:17.034613Z",
     "start_time": "2023-11-12T17:01:16.807323Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "observed_train = read_parquet(\"data/C/X_train_observed.parquet\")\n",
    "estimated_train = read_parquet(\"data/C/X_train_estimated.parquet\")\n",
    "target_data = read_parquet(\"data/C/train_targets.parquet\")\n",
    "test_data = read_parquet(\"data/C/X_test_estimated.parquet\")\n",
    "\n",
    "train_data = concat_observed_and_estimated_train_data(\n",
    "    X_observed=observed_train,\n",
    "    X_estimated=estimated_train\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:01:17.363206Z",
     "start_time": "2023-11-12T17:01:17.239479Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "C = LocationData(\n",
    "    train_data=train_data,\n",
    "    target_data=target_data,\n",
    "    test_data=test_data\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:01:18.070830Z",
     "start_time": "2023-11-12T17:01:17.952617Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "Train: (136245, 46) [No. NaN: 168040], Target: (34085, 2) [No. NaN: 0], Test: (2880, 46) [No. NaN: 3971]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (134505, 46) [No. NaN: 158811], Target: (32848, 2) [No. NaN: 4], Test: (2880, 46) [No. NaN: 3912]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train: (134401, 46) [No. NaN: 157326], Target: (32155, 2) [No. NaN: 6060], Test: (2880, 46) [No. NaN: 4104]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:01:18.716832Z",
     "start_time": "2023-11-12T17:01:18.573994Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "Function to display info about dataframe, used for debugging\n",
    "## Defining functions for preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "def display_info(df: pd.DataFrame, show_nan_dataframe: bool = False) -> None:\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(f\"Expected dataframe, got {type(df)}\")\n",
    "\n",
    "    obj_type = type(df)\n",
    "    shape = df.shape\n",
    "    nan_sum = df.isna().sum().sum()\n",
    "    nan_indexes = np.where(df.isna().any(axis=1))[0]\n",
    "    nan_df = df.iloc[nan_indexes]\n",
    "    print(f\"Type: {obj_type}, Shape: {shape}, NaN sum: {nan_sum}\")\n",
    "    if show_nan_dataframe:\n",
    "        display(nan_df)\n",
    "    else:\n",
    "        display(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:01:20.247312Z",
     "start_time": "2023-11-12T17:01:20.113332Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to remove features and replace `NaN` values with zero"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "def remove_feature(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    dataframe = dataframe.drop(columns=[*features])\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def replace_nan_with_zero(dataframe: pd.DataFrame, feature: str) -> pd.DataFrame:\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe[[feature]] = dataframe[[feature]].fillna(0)\n",
    "    return dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:01:24.263731Z",
     "start_time": "2023-11-12T17:01:24.161110Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to resample data by categorizing them\n",
    "- Features to average\n",
    "- Features to pick a value\n",
    "- Features to change redefine `NaN`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sample_by_selection(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "    if reduced_dataframe.isna().sum().sum() > 0:\n",
    "        print(\"[ERROR] Sample by selection got dataframe with NaN values\")\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.last()\n",
    "\n",
    "    \"\"\"\n",
    "    # Dropping NaN values that were introduced by resampling\n",
    "    reduced_dataframe = reduced_dataframe.dropna(axis=0, how='any')\n",
    "    \"\"\"\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_by_mean(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "    if reduced_dataframe.isna().sum().sum() > 0:\n",
    "        print(\"[ERROR] Sample by mean got dataframe with NaN values\")\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.mean()\n",
    "\n",
    "    \"\"\"\n",
    "    # Dropping NaN values that were introduced by resampling\n",
    "    reduced_dataframe = reduced_dataframe.dropna(axis=0, how='any')\n",
    "    \"\"\"\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_by_sum(dataframe: pd.DataFrame, *features: str) -> pd.DataFrame:\n",
    "    reduced_dataframe = dataframe[[*features]]\n",
    "    if reduced_dataframe.isna().sum().sum() > 0:\n",
    "        print(\"[ERROR] Sample by mean got dataframe with NaN values\")\n",
    "\n",
    "    resampler = reduced_dataframe.resample('H')\n",
    "    reduced_dataframe = resampler.sum()\n",
    "\n",
    "    \"\"\"\n",
    "    # Dropping NaN values that were introduced by resampling\n",
    "    reduced_dataframe = reduced_dataframe.dropna(axis=0, how='any')\n",
    "    \"\"\"\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def sample_cloud_base(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    cloud_base_frame = dataframe[[CLOUD_BASE]]\n",
    "\n",
    "    filled_frame = cloud_base_frame.copy()\n",
    "    filled_frame = filled_frame.fillna(-100_000)\n",
    "\n",
    "    resampler = filled_frame.resample('H')\n",
    "    cloud_base_frame = resampler.mean()\n",
    "\n",
    "    cloud_bases = cloud_base_frame[CLOUD_BASE].to_numpy()\n",
    "    negative_indexes = np.where(cloud_bases < 0)[0]\n",
    "    cloud_bases[negative_indexes] = -666\n",
    "    cloud_base_frame[CLOUD_BASE] = cloud_bases\n",
    "\n",
    "    # Dropping NaN values that were introduced by resampling\n",
    "\n",
    "    reduced_dataframe = cloud_base_frame.copy()\n",
    "    \"\"\"\n",
    "    reduced_dataframe = reduced_dataframe.dropna(axis=0, how='any')\n",
    "\n",
    "    if reduced_dataframe.isna().sum().sum() > 0:\n",
    "        print(\"[ERROR] Cloud base still has NaN values after resampling\")\n",
    "    \"\"\"\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "\n",
    "def merge_frames(*dataframes: pd.DataFrame) -> pd.DataFrame:\n",
    "    indexes = [df.index for df in dataframes]\n",
    "\n",
    "    intersecting_indexes = list(set(indexes[0]).intersection(*indexes))\n",
    "    filtered_dataframes = [df.loc[intersecting_indexes] for df in dataframes]\n",
    "\n",
    "    return pd.concat(filtered_dataframes, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions to synchronze dates in weather data and target data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def synchronize_timestamps(train_data: pd.DataFrame, target_data: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    if not isinstance(train_data.index, pd.DatetimeIndex):\n",
    "        raise IndexError(\"Train data does not have DateTimeIndex\")\n",
    "\n",
    "    if not isinstance(target_data.index, pd.DatetimeIndex):\n",
    "        raise IndexError(\"Target data does not have DateTimeIndex\")\n",
    "\n",
    "    train_data.sort_index(inplace=True)\n",
    "    target_data.sort_index(inplace=True)\n",
    "\n",
    "    train_dates = train_data.index.values\n",
    "    target_dates = target_data.index.values\n",
    "\n",
    "    # Finding intersecting dates and filtering dataframes\n",
    "    intersecting_dates = np.intersect1d(train_dates, target_dates)\n",
    "    train_data = train_data.loc[intersecting_dates]\n",
    "    target_data = target_data.loc[intersecting_dates]\n",
    "\n",
    "    return train_data, target_data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dateset contains many constant values for some features, any row with constant values is removed. If there is are at least 24 rows with constant values, the rows are removed.\n",
    "- -666 in ceiling height should not be removed as this is added in earlier "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_constant_measurements(\n",
    "        train_data: pd.DataFrame,\n",
    "        target_data: pd.DataFrame,\n",
    "        THRESHOLD: int = 24\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    indexes = []\n",
    "    count = 0\n",
    "    previous_value = None\n",
    "    start_index = None\n",
    "\n",
    "    # Setting date indexes as columns and resetting index\n",
    "    train_data[DATE_FORECAST] = train_data.index.values\n",
    "    target_data[TIME] = target_data.index.values\n",
    "\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    target_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Assuming y is a DataFrame and has a column for pv_measurement values\n",
    "    pv_measurements = target_data[PV_MEASUREMENT].to_numpy()\n",
    "\n",
    "    # Identifying indexes of consecutive constant values\n",
    "    for index, value in enumerate(pv_measurements):\n",
    "        if value == previous_value:\n",
    "            count += 1\n",
    "            if count == 1:\n",
    "                start_index = index - 1\n",
    "            if count >= THRESHOLD and value > -0.1:\n",
    "                indexes.extend(range(start_index, index + 1))\n",
    "        else:\n",
    "            count = 0\n",
    "        previous_value = value\n",
    "\n",
    "    # Assuming y has a column for dates and X has a similar column to filter on\n",
    "    dates = target_data.loc[indexes, TIME].unique()\n",
    "\n",
    "    filtered_train_data = train_data[~train_data[DATE_FORECAST].isin(dates)]\n",
    "    filtered_target_data = target_data[~target_data[TIME].isin(dates)]\n",
    "\n",
    "    # Switching back to date as index\n",
    "    filtered_train_data = filtered_train_data.set_index(DATE_FORECAST)\n",
    "    filtered_target_data = filtered_target_data.set_index(TIME)\n",
    "\n",
    "    return filtered_train_data, filtered_target_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adding lag features, this has to come here because of the resampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_lag_features(dataframe: pd.DataFrame, features: list[str], lags: list[int]) -> tuple[pd.DataFrame, list[str]]:\n",
    "    dataframe = dataframe.copy()\n",
    "    lag_feature_names: list[str] = []\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            lag_feature_name = f\"{feature}_lag_{lag}\"\n",
    "            dataframe[lag_feature_name] = dataframe[feature].shift(lag)\n",
    "            lag_feature_names.append(lag_feature_name)\n",
    "\n",
    "    return dataframe, lag_feature_names\n",
    "\n",
    "\n",
    "def add_lead_features(dataframe: pd.DataFrame, features: list[str], lags: list[int]) -> tuple[pd.DataFrame, list[str]]:\n",
    "    dataframe = dataframe.copy()\n",
    "    lead_feature_names: list[str] = []\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            lead_feature_name = f\"{feature}_lead_{lag}\"\n",
    "            dataframe[lead_feature_name] = dataframe[feature].shift(-lag)\n",
    "            lead_feature_names.append(lead_feature_name)\n",
    "\n",
    "    return dataframe, lead_feature_names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pipeline for preprocessing that is common for both train and test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def common_pre_processing(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    dataframe = remove_feature(dataframe, SNOW_DRIFT, CEILING_HEIGHT, SNOW_DENSITY)\n",
    "    dataframe = dataframe.set_index(DATE_FORECAST)\n",
    "\n",
    "    feature_to_lag_and_lead: str = DIRECT_RAD\n",
    "    # Adding lag and lead features\n",
    "    dataframe, lag_features = add_lag_features(dataframe, [feature_to_lag_and_lead], [1, 2, 3])\n",
    "    dataframe, lead_features = add_lead_features(dataframe, [feature_to_lag_and_lead], [1, 2, 3])\n",
    "\n",
    "    # Resample data to hours\n",
    "    selection_features = [\n",
    "        CLEAR_SKY_ENERGY,  # Maybe move\n",
    "        DIRECT_RAD_1H,  # MAYBE MOVE\n",
    "        DIFFUSE_RAD_1H,  # Maybe move\n",
    "        FRESH_SNOW_1H,\n",
    "        FRESH_SNOW_3H,\n",
    "        FRESH_SNOW_6H,\n",
    "        FRESH_SNOW_12H,\n",
    "        FRESH_SNOW_24H,\n",
    "        DEW_OR_RIME,\n",
    "        IS_DAY,\n",
    "        IS_IN_SHADOW,\n",
    "        PRECIP_TYPE_5MIN,\n",
    "        ELEVATION,\n",
    "        *lag_features,\n",
    "        *lead_features,\n",
    "    ]\n",
    "\n",
    "    mean_features = [\n",
    "        ABSOLUTE_HUMIDITY,\n",
    "        CLEAR_SKY_RAD,\n",
    "        DIFFUSE_RAD,\n",
    "        DIRECT_RAD,\n",
    "        AIR_DENSITY,\n",
    "        DEW_POINT,\n",
    "        MSL_PRESSURE,\n",
    "        PRESSURE_100M,\n",
    "        PRESSURE_50M,\n",
    "        SFC_PRESSURE,\n",
    "        RELATIVE_HUMIDITY,\n",
    "        T_1000HPA,\n",
    "        SUN_AZIMUTH,\n",
    "        SUN_ELEVATION,\n",
    "        EFFECTIVE_CLOUD_COVER,\n",
    "        TOTAL_CLOUD_COVER,\n",
    "        VISIBILITY,\n",
    "        WIND_SPEED_10M,\n",
    "        WIND_SPEED_U_10M,\n",
    "        WIND_SPEED_V_10M,\n",
    "        WIND_SPEED_W_1000HPA,\n",
    "        SNOW_MELT_10MIN,\n",
    "        PROB_RIME,\n",
    "        SUPER_COOLED_LIQUID_WATER,\n",
    "        SNOW_DEPTH,\n",
    "    ]\n",
    "\n",
    "    sum_features = [\n",
    "        SNOW_WATER,\n",
    "        PRECIP_5MIN,\n",
    "        RAIN_WATER,\n",
    "    ]\n",
    "\n",
    "    selection_df = sample_by_selection(dataframe, *selection_features)\n",
    "    mean_df = sample_by_mean(dataframe, *mean_features)\n",
    "    sum_df = sample_by_sum(dataframe, *sum_features)\n",
    "    ceiling_height_df = sample_cloud_base(dataframe)\n",
    "    dataframe = merge_frames(selection_df, mean_df, ceiling_height_df, sum_df)\n",
    "\n",
    "    dataframe = dataframe.dropna(axis=0, how='any')\n",
    "\n",
    "    return dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing pipelines for train and test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pre_process_train_data(train_data: pd.DataFrame, target_data: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_data = common_pre_processing(dataframe=train_data)\n",
    "\n",
    "    # Setting time as index for target data\n",
    "    target_data[TIME] = pd.to_datetime(target_data[TIME])\n",
    "    target_data = target_data.set_index(TIME)\n",
    "\n",
    "    # Preprocessing that affects both train and target data\n",
    "    train_data, target_data = synchronize_timestamps(train_data=train_data, target_data=target_data)\n",
    "    train_data, target_data = remove_constant_measurements(train_data=train_data, target_data=target_data)\n",
    "\n",
    "    return train_data, target_data\n",
    "\n",
    "\n",
    "def pre_process_test_data(test_data: pd.DataFrame, columns: np.ndarray) -> pd.DataFrame:\n",
    "    test_data = common_pre_processing(dataframe=test_data)\n",
    "    if np.any(test_data.columns != columns):\n",
    "        raise ValueError(\"Test data does not have the same columns as train data\")\n",
    "    return test_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running preprocessing\n",
    "Updating data objects for each location"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=A.train_data, target_data=A.target_data)\n",
    "test_data = pre_process_test_data(test_data=A.test_data, columns=train_data.columns)\n",
    "A.train_data = train_data\n",
    "A.target_data = target_data\n",
    "A.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=B.train_data, target_data=B.target_data)\n",
    "test_data = pre_process_test_data(test_data=B.test_data, columns=train_data.columns)\n",
    "B.train_data = train_data\n",
    "B.target_data = target_data\n",
    "B.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, target_data = pre_process_train_data(train_data=C.train_data, target_data=C.target_data)\n",
    "test_data = pre_process_test_data(test_data=C.test_data, columns=train_data.columns)\n",
    "C.train_data = train_data\n",
    "C.target_data = target_data\n",
    "C.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Extended preprocessing\n",
    "As shown in the location summaries, we can see that there are many `NaN` values in the target data for location `B` and  some for location `C`. This needs to be handles as we loose valuable data if we remove all rows with `NaN` values. This is done by using data from the other location to fill in the missing values. We can safley assume that location `B` and `C` is the same location based on correlation between the features and measurements. They also tend to follow the same trends in the data.\n",
    "Defining defining functions to impute `NaN` values using data from other locations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def impute_target_data(\n",
    "        to_df: pd.DataFrame,\n",
    "        from_df: pd.DataFrame,\n",
    "        SCALE_FACTOR: int = 1,\n",
    "        INTERCEPT: float = 0\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Assigning variables to index column and pv_measurement column in each dataframe\n",
    "    to_dates = to_df.index.values\n",
    "    to_pv_measurements = to_df[PV_MEASUREMENT].to_numpy()\n",
    "    from_dates = from_df.index.values\n",
    "    from_pv_measurements = from_df[PV_MEASUREMENT].to_numpy()\n",
    "\n",
    "    # Finding indexes of NaN values that we want to impute\n",
    "    nan_to_impute_indexes = np.where(np.isnan(to_pv_measurements))[0]\n",
    "    nan_to_impute_dates = to_dates[nan_to_impute_indexes]\n",
    "\n",
    "    # Finding the indexes of the dates we want to impute in the from dataframe (source)\n",
    "    source_df_date_indexes = []\n",
    "    for nan_date in nan_to_impute_dates:\n",
    "        source_nan_index = np.where(from_dates == nan_date)[0]\n",
    "        if len(source_nan_index) > 0:\n",
    "            source_df_date_indexes.append(source_nan_index[0])\n",
    "\n",
    "    source_df_date_indexes = np.array(source_df_date_indexes, dtype=np.int64)\n",
    "    source_dates = from_dates[source_df_date_indexes]\n",
    "    source_pv_measurements = from_pv_measurements[source_df_date_indexes]\n",
    "    source_pv_measurements = source_pv_measurements * SCALE_FACTOR + INTERCEPT\n",
    "\n",
    "    # Finding indexes of NaN values in from data\n",
    "    to_df.loc[source_dates, PV_MEASUREMENT] = source_pv_measurements\n",
    "\n",
    "    return to_df, from_df\n",
    "\n",
    "\n",
    "def impute_missing_dates(\n",
    "        from_location: LocationData,\n",
    "        to_location: LocationData,\n",
    "        SCALE_FACTOR: int = 1,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    from_train = from_location.train_data\n",
    "    from_target = from_location.target_data\n",
    "    to_train = to_location.train_data\n",
    "    to_target = to_location.target_data\n",
    "\n",
    "    # Missing dates \n",
    "    missing_dates = np.setdiff1d(from_target.index.values, to_target.index.values)\n",
    "\n",
    "    # Defining date ranges for both dataframes\n",
    "    from_start_date = from_target.index.values[0]\n",
    "    from_end_date = from_target.index.values[-1]\n",
    "\n",
    "    to_start_date = to_target.index.values[0]\n",
    "    to_end_date = to_target.index.values[-1]\n",
    "\n",
    "    from_dates = np.arange(from_start_date, from_end_date + np.timedelta64(1, 'h'), dtype='datetime64[h]')\n",
    "    to_dates = np.arange(to_start_date, to_end_date + np.timedelta64(1, 'h'), dtype='datetime64[h]')\n",
    "\n",
    "    to_target_frame_with_full_date_range = pd.DataFrame(index=pd.DatetimeIndex(to_dates), columns=to_target.columns)\n",
    "    to_target_frame_with_full_date_range.loc[to_target.index.values] = to_target.values\n",
    "    to_target_frame_with_full_date_range.loc[missing_dates] = from_target.loc[missing_dates].values * SCALE_FACTOR\n",
    "\n",
    "    to_train_frame_with_fuill_date_range = pd.DataFrame(index=pd.DatetimeIndex(to_dates), columns=to_train.columns)\n",
    "    to_train_frame_with_fuill_date_range.loc[to_train.index.values] = to_train.values\n",
    "    to_train_frame_with_fuill_date_range.loc[missing_dates] = from_train.loc[missing_dates].values\n",
    "\n",
    "    return to_train_frame_with_fuill_date_range, to_target_frame_with_full_date_range, from_train, from_target"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pipeline for imputation of target data for B and C"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def target_data_imputation(B: LocationData, C: LocationData) -> None:\n",
    "    target_B = B.target_data.copy()\n",
    "    target_C = C.target_data.copy()\n",
    "\n",
    "    target_B, target_C = impute_target_data(to_df=target_B, from_df=target_C,\n",
    "                                            SCALE_FACTOR=1.10387868)  # 1.10387868 , 1.2\n",
    "    target_C, target_B = impute_target_data(to_df=target_C, from_df=target_B, SCALE_FACTOR=0.82386204)  # 0.82386204\n",
    "\n",
    "    \"\"\"\n",
    "    train_B, target_B, train_C, target_C = impute_missing_dates(to_location=B, from_location=C, SCALE_FACTOR=1.2)\n",
    "    train_C, target_C, train_B, target_B = impute_missing_dates(to_location=C, from_location=B, SCALE_FACTOR=0.5)\n",
    "    \"\"\"\n",
    "\n",
    "    # Dropping rows with NaN values, this is done as we have imputed all NaN values and any remaining NaN values are due to missing data in both locations\n",
    "    target_B.dropna(axis=0, how='any', inplace=True)\n",
    "    target_C.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "    train_B, target_B = synchronize_timestamps(train_data=B.train_data, target_data=target_B)\n",
    "    train_C, target_C = synchronize_timestamps(train_data=C.train_data, target_data=target_C)\n",
    "\n",
    "    B.train_data = train_B\n",
    "    B.target_data = target_B\n",
    "    C.target_data = target_C\n",
    "    C.train_data = train_C\n",
    "\n",
    "\n",
    "def drop_nan_values_from_target_data(location: LocationData) -> None:\n",
    "    target_data = location.target_data.copy()\n",
    "    target_data.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "    train_data = location.train_data.copy()\n",
    "    train_data, target_data = synchronize_timestamps(train_data, target_data)\n",
    "\n",
    "    location.train_data = train_data\n",
    "    location.target_data = target_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Target value imputation by borrowing data beteween B and C"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#target_data_imputation(B, C)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "drop_nan_values_from_target_data(A)\n",
    "drop_nan_values_from_target_data(B)\n",
    "drop_nan_values_from_target_data(C)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Synchronizing timestamps between train and target data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering\n",
    "## Seasonality captured by sine and cosine\n",
    "Function to create sinusoidal features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_sinusoidal_features(df: pd.DataFrame, hour_col: str = 'hour', day_col: str = 'day_of_year') -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    dates = df.index.values\n",
    "    df['date_forecast'] = dates\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Create hour and day_of_year columns\n",
    "    df['hour'] = df['date_forecast'].dt.hour\n",
    "    df['day_of_year'] = df['date_forecast'].dt.dayofyear\n",
    "\n",
    "    # Extract week number using isocalendar()\n",
    "    df['week_of_year'] = df['date_forecast'].dt.isocalendar().week\n",
    "    df['month'] = df['date_forecast'].dt.month\n",
    "\n",
    "    # Constants for sinusoidal functions\n",
    "    hours_in_day = 24\n",
    "    days_in_year = 365.25  # Accounting for leap years\n",
    "    weeks_in_year = 52\n",
    "    months_in_year = 12\n",
    "\n",
    "    # Create sinusoidal features based on the hour of the day\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * (df[hour_col] - 18) / hours_in_day)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * (df[hour_col] - 18) / hours_in_day)\n",
    "\n",
    "    # Create sinusoidal features based on the day of the year\n",
    "    df['sin_day'] = np.sin(2 * np.pi * (df[day_col] - 355) / days_in_year)\n",
    "    df['cos_day'] = np.cos(2 * np.pi * (df[day_col] - 355) / days_in_year)\n",
    "\n",
    "    # Create sinusoidal features based on the week of the year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['week_of_year'] / weeks_in_year)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['week_of_year'] / weeks_in_year)\n",
    "\n",
    "    # Create sinusoidal features based on the month\n",
    "    df['sin_month'] = np.sin(2 * np.pi * df['month'] / months_in_year)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * df['month'] / months_in_year)\n",
    "\n",
    "    dates = df['date_forecast'].to_numpy()\n",
    "    df.index = dates\n",
    "    df.drop(columns=['hour', 'day_of_year', 'week_of_year', 'month', 'date_forecast'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_sinusoidal_features_for_location(location: LocationData) -> None:\n",
    "    train_data = location.train_data.copy()\n",
    "    test_data = location.test_data.copy()\n",
    "\n",
    "    train_data = create_sinusoidal_features(train_data)\n",
    "    test_data = create_sinusoidal_features(test_data)\n",
    "\n",
    "    train_data.sort_index(inplace=True)\n",
    "    test_data.sort_index(inplace=True)\n",
    "\n",
    "    location.train_data = train_data\n",
    "    location.test_data = test_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding seasonality\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "create_sinusoidal_features_for_location(A)\n",
    "create_sinusoidal_features_for_location(B)\n",
    "create_sinusoidal_features_for_location(C)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(A)\n",
    "display(B)\n",
    "display(C)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scaling start of ceiling height "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "\n",
    "def match_quantiles(series_before, series_after):\n",
    "    quantiles_after = np.percentile(series_after, np.arange(0, 101, 1))\n",
    "\n",
    "    def find_quantile_match(value, quantiles):\n",
    "        percentile = percentileofscore(series_before, value, kind='rank')\n",
    "        return np.percentile(quantiles, percentile)\n",
    "\n",
    "    return series_before.apply(lambda x: find_quantile_match(x, quantiles_after))\n",
    "\n",
    "\n",
    "def scale_ceiling_height(location, index):\n",
    "    train_data = location.train_data.copy()\n",
    "    meter_data = train_data.iloc[index:]\n",
    "\n",
    "    good = train_data.iloc[index:][CEILING_HEIGHT]\n",
    "    bad = train_data.iloc[:index][CEILING_HEIGHT]\n",
    "\n",
    "    matched_series = match_quantiles(good, bad)\n",
    "\n",
    "    train_data.loc[:index, CEILING_HEIGHT] = matched_series\n",
    "    scaled_data = train_data.iloc[:index]\n",
    "\n",
    "    merged_data = pd.concat([scaled_data, meter_data], axis=0)\n",
    "    merged_data.sort_index(inplace=True)\n",
    "\n",
    "    location.train_data = merged_data\n",
    "\n",
    "\n",
    "def scale_ceiling_height_by_multiplication(location: LocationData, index: int) -> None:\n",
    "    feet_data = location.train_data.iloc[:index][CEILING_HEIGHT]\n",
    "    meter_data = location.train_data.iloc[index:][CEILING_HEIGHT]\n",
    "\n",
    "    neg_feet_indexes = np.where(feet_data < 0)[0]\n",
    "    feet_data_in_meter = feet_data * 0\n",
    "    feet_data_in_meter[neg_feet_indexes] = -666\n",
    "\n",
    "    merged_ceiling_height = pd.concat([feet_data_in_meter, meter_data], axis=0)\n",
    "    merged_ceiling_height.sort_index(inplace=True)\n",
    "\n",
    "    location.train_data[CEILING_HEIGHT] = merged_ceiling_height"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_by_feature(location: LocationData, feature: str) -> None:\n",
    "    train_data = location.train_data.copy()\n",
    "\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    train_data = train_data[[feature]]\n",
    "\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    plt.plot(train_data, label='Train')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Outlier detection and removal (Not used) "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_outliers_IQR(\n",
    "            train_data,\n",
    "            target_data,\n",
    "            column,\n",
    "            daily_seasonality=24,\n",
    "            monthly_seasonality=730,\n",
    "            upper_threshold=3,\n",
    "            lower_threshold=3\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        result_daily = seasonal_decompose(train_data[column], model='additive', period=daily_seasonality)\n",
    "        daily_adjusted = result_daily.resid.dropna()\n",
    "\n",
    "        result_monthly = seasonal_decompose(daily_adjusted, model='additive', period=monthly_seasonality)\n",
    "        monthly_adjusted = result_monthly.resid.dropna()\n",
    "\n",
    "        rolling_mean = monthly_adjusted.rolling(window=daily_seasonality, center=True).mean()\n",
    "        rolling_std = monthly_adjusted.rolling(window=daily_seasonality, center=True).std()\n",
    "\n",
    "        upper_bound = rolling_mean + upper_threshold * rolling_std\n",
    "        lower_bound = rolling_mean - lower_threshold * rolling_std\n",
    "\n",
    "        outliers = ((monthly_adjusted > upper_bound) | (monthly_adjusted < lower_bound)).reindex(\n",
    "            train_data.index,\n",
    "            fill_value=False\n",
    "        )\n",
    "\n",
    "        cleaned_train_data = train_data[~outliers]\n",
    "        cleaned_target_data = target_data[~outliers]\n",
    "\n",
    "        return cleaned_train_data, cleaned_target_data\n",
    "\n",
    "    \n",
    "def remove_outliers_LOF(train_data, target_data,\n",
    "                            n_neighbors=2,\n",
    "                            contamination='auto',\n",
    "                            drop_columns=['time']):\n",
    "        features_for_lof = target_data.drop(drop_columns, axis=1)\n",
    "\n",
    "        lof = LocalOutlierFactor(n_neighbors=n_neighbors, novelty=False, contamination=contamination)\n",
    "\n",
    "        lof_labels = lof.fit_predict(features_for_lof)\n",
    "\n",
    "        outliers_mask = lof_labels == -1\n",
    "        cleaned_target_data = target_data[~outliers_mask]\n",
    "        cleaned_train_data = train_data[~outliers_mask]\n",
    "\n",
    "        return cleaned_train_data, cleaned_target_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 EDA "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Weather Data and Solar Energy Production Analysis\n",
    "\n",
    "The dataset consists of different weather phenomenas and solar energy production data. Weather data, in various units, include both instantaneous and cumulative values for every 15 min. The train data consists of observed and estimated weather conditions across two distinct, non-overlapping periods. We have data on energy production from a solar panel installation, collected from different locations during approximately the same period.\n",
    "\n",
    "# Domain Knowledge: Weather Data and Energy Production\n",
    "\n",
    "1. **Temperature**: \n",
    "   - Temperature directly impacts photo voltaic cell efficiency, and therefore energy production.\n",
    "   - Solar panels usually perform better at moderate temperatures.\n",
    "\n",
    "2. **Sunlight and Solar Radiation**: \n",
    "   - Important for solar energy production.\n",
    "   - Includes direct sunlight and diffused sky radiation.\n",
    "   - Affected by cloud cover blocking the solar panels.\n",
    "\n",
    "3. **Wind**: \n",
    "   - Cooling effect can increase efficiency, but strong winds may damage installations.\n",
    "   - High wind speeds can cause dust and debris to accumulate on panels, reducing efficiency.\n",
    "   - High wind speed cause more change in other weather factors. \n",
    "\n",
    "4. **Precipitation**: \n",
    "   - Rainfall can clean solar panels, enhancing efficiency.\n",
    "   - Heavy snowfall can obstruct panels, reducing production.\n",
    "\n",
    "5. **Humidity**: \n",
    "   - High humidity can lead to material degradation and connection issues.\n",
    "   - High humidity is often related to cloud cover, which reduces solar radiation.\n",
    "\n",
    "6. **Atmospheric Pressure**: \n",
    "   - Indicates weather pattern changes, indirectly affecting solar production.\n",
    "\n",
    "7. **Cloud Cover**: \n",
    "   - Impacts the amount of solar radiation received.\n",
    "   - Can diffuse or block sunlight.\n",
    "   - May increase radiative forcing, which can increase temperatures.\n",
    "\n",
    "8. **Air Quality**: \n",
    "   - Particulates and pollutants reduce solar radiation efficiency by blocking sunlight from cells.\n",
    "\n",
    "9. **Seasonal Variations**: \n",
    "   - Sun angle changes affect daylight hours and solar radiation intensity throughout the year.\n",
    "\n",
    "10. **Geographical Differences**: \n",
    "    - Location affects sunlight amount due to latitude, altitude, and local climates.\n",
    "\n",
    "\n",
    "## Considerations and Remarks for Data Analysis\n",
    "- **Data Synchronization:** Align weather data timestamps with energy production timestamps.\n",
    "- **Unit Consistency:** Keep units in mind for meaningful comparisons.\n",
    "- **Seasonal Analysis:** Consider seasonal variations in both weather patterns and solar energy production.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.1 General Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def describe_numerical_data(df):\n",
    "    \n",
    "    description = df.describe(percentiles=[.25, .5, .75], include='all').transpose()\n",
    "    \n",
    "    # Adding additional statistics\n",
    "    description['#NaN'] = df.isna().sum()\n",
    "    description['#Unique'] = df.nunique()\n",
    "    \n",
    "    # Renaming the percentiles for clarity\n",
    "    description = description.rename(columns={'25%': 'Q1 (25th Percentile)', '50%': 'Median (50th Percentile)', '75%': 'Q3 (75th Percentile)'})\n",
    "    \n",
    "    return description\n",
    "\n",
    "train_data_numerical_A = A.original_train_data.iloc[:, 1:-1]\n",
    "train_data_numerical_B = B.original_train_data.iloc[:, 1:-1]\n",
    "train_data_numerical_C = C.original_train_data.iloc[:, 1:-1]\n",
    "\n",
    "target_data_numerical_A = A.original_target_data.iloc[:, 1:]\n",
    "target_data_numerical_B = B.original_target_data.iloc[:, 1:]\n",
    "target_data_numerical_C = C.original_target_data.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1.1 Location A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_description_A = describe_numerical_data(train_data_numerical_A)\n",
    "train_data_description_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_data_description_A = describe_numerical_data(target_data_numerical_A)\n",
    "target_data_description_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1.2 Location B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_description_B = describe_numerical_data(train_data_numerical_B)\n",
    "data_description_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_data_description_B = describe_numerical_data(target_data_numerical_B)\n",
    "target_data_description_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1.3 Location C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_description_C = describe_numerical_data(train_data_numerical_C)\n",
    "data_description_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_data_description_C = describe_numerical_data(target_data_numerical_C)\n",
    "target_data_description_C"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Summary of general statistics\n",
    "Based on general statistics, we can see that the weather data consists of both scalar and categorical values. The magnitude of the various weather data elements is relatively similar across locations. Generally, the weather data appears to be very similar across all locations. However, the PV measurements are quite different. The magnitude at location A is 5 times greater than at B and C, and there is also some difference between B and C. The number of unique measurements varies significantly, with A having many unique measurements, B fewer, and C proportionally few. It is also observed that snow density, cloud base, and ceiling height have NaN values in all datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.3 Individual feature analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3.1 PV Measurement (Target Value)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_target_value(data, features, location = None):\n",
    "    for feature in features:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(data['time'], data[feature], label=feature)\n",
    "        plt.title(f\"Time Series of {feature} - Location {location}\")\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "def plot_individual_time_series(data, features, location = None):\n",
    "    for feature in features:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(data['date_forecast'], data[feature], label=feature)\n",
    "        plt.title(f\"Time Series of {feature} - Location {location}\")\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "def analyze_target_feature_with_log(target_series, feature_name):\n",
    "    target_series_no_na = target_series.dropna()\n",
    "\n",
    "    # Adding a small constant to avoid log(0)\n",
    "    target_series_log_no_na = np.log(target_series_no_na + 1)\n",
    "\n",
    "    # Basic Statistics\n",
    "    stats = target_series.describe()\n",
    "    stats['#NaN'] = target_series.isna().sum()\n",
    "    stats['#Unique'] = target_series.nunique()\n",
    "\n",
    "    # Visualizations\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Histogram of original data\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.histplot(target_series_no_na, bins=30, kde=True)\n",
    "    plt.title(f'Distribution of {feature_name} Values')\n",
    "\n",
    "    # Boxplot of original data\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.boxplot(x=target_series_no_na)\n",
    "    plt.title(f'Boxplot of {feature_name} Values')\n",
    "\n",
    "    # Histogram of log-transformed data with log-normal fit\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.histplot(target_series_log_no_na, bins=30, kde=True, color='skyblue')\n",
    "    # ... remaining code ...\n",
    "    plt.title(f'Log-Transformed Distribution of {feature_name}\\n with Log-Normal Fit')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return stats\n",
    "\n",
    "def decompose_time_series_short(time_series, period, title):\n",
    "    if not isinstance(time_series, pd.Series):\n",
    "        time_series = pd.Series(time_series)\n",
    "\n",
    "    # Remove any missing values from the time series\n",
    "    time_series = time_series.dropna()\n",
    "\n",
    "    # Perform STL decomposition\n",
    "    stl = STL(time_series, period=period, seasonal=13, seasonal_deg=0, trend_deg=0, robust=True)\n",
    "    result = stl.fit()\n",
    "\n",
    "    # Extract the components\n",
    "    trend = result.trend\n",
    "    seasonal = result.seasonal\n",
    "    residual = result.resid\n",
    "\n",
    "    # Plotting the components\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.subplot(411)\n",
    "    plt.plot(time_series, label='Original', color='blue')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'Time Series Decomposition {title}')\n",
    "\n",
    "    plt.subplot(412)\n",
    "    plt.plot(trend, label='Trend', color='red')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.subplot(413)\n",
    "    plt.plot(seasonal, label='Seasonal', color='green')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.subplot(414)\n",
    "    plt.plot(residual, label='Residual', color='purple')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    components = {\n",
    "        'original': time_series,\n",
    "        'trend': trend,\n",
    "        'seasonal': seasonal,\n",
    "        'residual': residual\n",
    "    }\n",
    "    \n",
    "    return components\n",
    "\n",
    "def plot_feature_distributions(data, features, nrows=3, ncols=2, figsize=(14, 12), color='skyblue', log_scale=False):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    # Initialize the subplot figure\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Loop through the features and plot their distributions\n",
    "    for i, feature in enumerate(features):\n",
    "        # Transform the data if log_scale is True\n",
    "        axes[i].set_title(f'Distribution of Feature: {feature}', fontsize=12)\n",
    "\n",
    "        if log_scale:\n",
    "            data_to_plot = np.log1p(data[feature])\n",
    "            title_suffix = ' (Log Scale)'\n",
    "        else:\n",
    "            data_to_plot = data[feature]\n",
    "            title_suffix = ''\n",
    "            \n",
    "        sns.histplot(data_to_plot, kde=True, ax=axes[i], color=color, edgecolor='black')\n",
    "        axes[i].set_title(f'Distribution of {feature}{title_suffix}', fontsize=12)\n",
    "        axes[i].set_xlabel('')\n",
    "        axes[i].set_ylabel('')\n",
    "\n",
    "    # Adjust layout for better readability\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_distribution_properties(data, features):\n",
    "    for feature in features:\n",
    "        feature_data = data[feature].dropna()\n",
    "        skewness = skew(feature_data)\n",
    "        kurt = kurtosis(feature_data)\n",
    "        print(f\"{feature}: Skewness = {skewness:.2f}, Kurtosis = {kurt:.2f}\")\n",
    "\n",
    "def plot_outliers_boxplot(data, features, location):\n",
    "    num_features = len(features)\n",
    "    fig, axes = plt.subplots(num_features, 1, figsize=(10, 5 * num_features), squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        sns.boxplot(x=data[feature], ax=axes[i])\n",
    "        axes[i].set_title(f'Boxplot of {feature} - Location {location}')\n",
    "        axes[i].set_xlabel('Value')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def count_outliers_iqr(data, features):\n",
    "     outlier_counts = {}\n",
    "     for feature in features:\n",
    "         Q1 = np.percentile(data[feature], 25)\n",
    "         Q3 = np.percentile(data[feature], 75)\n",
    "         IQR = Q3 - Q1\n",
    "         outlier_step = 1.5 * IQR\n",
    "         outliers = data[(data[feature] < Q1 - outlier_step) | (data[feature] > Q3 + outlier_step)]\n",
    "         outlier_counts[feature] = len(outliers)\n",
    "         \n",
    "         # Print the results in a nicely formatted string\n",
    "         print(f\"Feature '{feature}' has {len(outliers)} outliers\")\n",
    "     \n",
    "     return outlier_counts\n",
    " \n",
    "def descriptive_statistics(data, features):\n",
    "    return data[features].describe()\n",
    "\n",
    "def plot_time_series(data, features, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for feature in features:\n",
    "        plt.plot(data['date_forecast'], data[feature], label=feature)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def correlation_analysis(data, features):\n",
    "    correlation_matrix = data[features].corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "    plt.title('Correlation Analysis')\n",
    "    plt.show()\n",
    "    return correlation_matrix\n",
    "\n",
    "def missing_values_analysis(data, features):\n",
    "    missing_values = data[features].isnull().sum()\n",
    "    \n",
    "    # Print the results in a nicely formatted string\n",
    "    print(\"Missing Values Analysis:\")\n",
    "    print(\"-\" * 30)  # Just a separator for better readability\n",
    "    for feature, missing_count in missing_values.items():\n",
    "        print(f\"{feature}: {missing_count} missing values\")\n",
    "    \n",
    "    return missing_values\n",
    "\n",
    "def apply_decomposition(dataframe, feature_group, period, decompose_function=decompose_time_series_short):\n",
    "    decomposed_components = {}\n",
    "    \n",
    "    for feature in feature_group:\n",
    "        if feature in dataframe.columns:\n",
    "            time_series = dataframe[feature]\n",
    "            title = f'Decomposition for {feature}'\n",
    "            components = decompose_function(time_series, period, title = title)\n",
    "            decomposed_components[feature] = components\n",
    "        else:\n",
    "            print(f\"Feature {feature} not found in DataFrame\")\n",
    "            \n",
    "    return decomposed_components\n",
    "\n",
    "def plot_autocorrelation(data, feature):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    autocorrelation_plot(data[feature].dropna())\n",
    "    plt.title(f'Autocorrelation Plot for {feature}')\n",
    "    plt.show()\n",
    "\n",
    "def plot_partial_autocorrelation(data, feature):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    plot_pacf(data[feature].dropna(), ax=ax)\n",
    "    plt.title(f'Partial Autocorrelation Plot for {feature}')\n",
    "    plt.show()\n",
    "\n",
    "def plot_density(data, feature):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.kdeplot(data[feature].dropna(), fill=True)\n",
    "    plt.title(f'Density Plot for {feature}')\n",
    "    plt.show()\n",
    "\n",
    "def plot_lag(data, feature):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    lag_plot(data[feature])\n",
    "    plt.title(f'Lag Plot for {feature}')\n",
    "    plt.show()\n",
    "\n",
    "def plot_rolling_statistics(data, feature, window=12):\n",
    "    rolling_mean = data[feature].rolling(window=window).mean()\n",
    "    rolling_std = data[feature].rolling(window=window).std()\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(data[feature], label='Original')\n",
    "    plt.plot(rolling_mean, label='Rolling Mean')\n",
    "    plt.plot(rolling_std, label='Rolling Std Dev')\n",
    "    plt.title(f'Rolling Mean & Standard Deviation for {feature}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_qq(data, feature):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    probplot(data[feature].dropna(), dist=\"norm\", plot=plt)\n",
    "    plt.title(f'Q-Q Plot for {feature}')\n",
    "    plt.show()\n",
    "\n",
    "def plot_fourier_transform(data, feature):\n",
    "    # Fourier Transform for finding dominant cycles\n",
    "    ft = np.fft.fft(data[feature].dropna())\n",
    "    frequencies = np.fft.fftfreq(len(ft))\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.stem(frequencies, np.abs(ft), 'b', markerfmt=\" \", basefmt=\"-b\")\n",
    "    plt.title('Fourier Transform - Magnitude')\n",
    "    plt.show()\n",
    "\n",
    "def plot_ecdf(data, feature):\n",
    "    sorted_var = np.sort(data[feature].dropna())\n",
    "    yvals = np.arange(len(sorted_var))/float(len(sorted_var)-1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sorted_var, yvals)\n",
    "    plt.title(f'ECDF for {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.show()\n",
    "\n",
    "def plot_periodogram(data, feature):\n",
    "    f, Pxx = periodogram(data[feature].dropna())\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.semilogy(f, Pxx)\n",
    "    plt.ylim([1e-4, Pxx.max() + 0.05])\n",
    "    plt.title('Periodogram')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Power Spectrum')\n",
    "    plt.show()\n",
    "\n",
    "def plot_dendrogram(data, features):\n",
    "    # Generate the linkage matrix\n",
    "    Z = linkage(data[features].dropna(), 'ward')\n",
    "    \n",
    "    # Plot the dendrogram\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    dendrogram(Z, truncate_mode='lastp', p=12, show_leaf_counts=False, leaf_rotation=90., leaf_font_size=12., show_contracted=True)\n",
    "    plt.title('Hierarchical Clustering Dendrogram')\n",
    "    plt.xlabel('Cluster Size')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.show()\n",
    "    \n",
    "# Auxiliary function to plot partial autocorrelation\n",
    "def plot_partial_autocorrelation(series):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plot_pacf(series, lags=20)\n",
    "    plt.title('Partial Autocorrelation Plot')\n",
    "    plt.show()\n",
    "\n",
    "def detect_anomalies_z_score(data, features, threshold=3):\n",
    "    for feature in features:\n",
    "        # Calculate the Z-score\n",
    "        data[f'{feature}_z_score'] = np.abs(zscore(data[feature]))\n",
    "\n",
    "        # Identify anomalies\n",
    "        data[f'{feature}_anomaly'] = data[f'{feature}_z_score'] > threshold\n",
    "\n",
    "        # Print the number of anomalies detected\n",
    "        anomalies = data[f'{feature}_anomaly'].sum()\n",
    "        print(f\"Feature '{feature}' has {anomalies} anomalies\")\n",
    "\n",
    "    return data\n",
    "    \n",
    "def perform_comprehensive_analysis(df, features):\n",
    "    print(descriptive_statistics(df, features))\n",
    "    count_outliers_iqr(df, features)\n",
    "    detect_anomalies_z_score(df, features)\n",
    "    missing_values_analysis(df, features)\n",
    "    try:\n",
    "        plot_target_value(df, features)\n",
    "    except:\n",
    "        try: \n",
    "            plot_time_series(df, features)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    apply_decomposition(df, features, period=24)\n",
    "    calculate_distribution_properties(df, features)\n",
    "    #plot_dendrogram(df, features)\n",
    "    for feature in features:\n",
    "        analyze_target_feature_with_log(df[feature], feature)\n",
    "    #for feature in features:\n",
    "    #    plot_autocorrelation(df, feature)\n",
    "    #for feature in features:\n",
    "    #    plot_partial_autocorrelation(df[feature])\n",
    "    for feature in features:\n",
    "        plot_lag(df, feature)\n",
    "    for feature in features:\n",
    "        plot_rolling_statistics(df, feature, window=12)\n",
    "    #for feature in features:\n",
    "    #    plot_qq(df, feature)\n",
    "    #for feature in features:\n",
    "    #    plot_fourier_transform(df, feature)\n",
    "    #for feature in features:\n",
    "    #    plot_ecdf(df, feature)\n",
    "    #for feature in features:\n",
    "    #    plot_periodogram(df, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3.1.1 Location A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perform_comprehensive_analysis(A.original_target_data, ['pv_measurement'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Location B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perform_comprehensive_analysis(B.original_target_data, ['pv_measurement'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Location C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perform_comprehensive_analysis(C.original_target_data, ['pv_measurement'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary of PV Measurement\n",
    "The first thing we notice when analyzing PV measurements is that parts of the data are characterized by constant static values. This is the case at locations B and C. Not surprisingly, there are clear trends and seasons that follow the strength of the sun throughout the year. We observe that there are mostly recorded zero values, which is expected as there is no sunlight, and thus no energy production, at night. The standard deviation and the average value follow the same trend as the measurements. There is a noticeable lag trend showing that energy production is influenced by previous energy production, which is a natural consequence of gradual weather changes and also highly dependent on previous weather conditions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.3.2 Train Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3.2.1 Grouping Features By Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def analyze_highly_correlated_features(X: pd.DataFrame, threshold: float = 0.9) -> List[List[str]]:\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        raise TypeError(\"X must be a pandas dataframe\")\n",
    "\n",
    "    if not isinstance(threshold, (int, float)):\n",
    "        raise TypeError(\"threshold must be a numeric value\")\n",
    "\n",
    "    corr_matrix = X.corr().abs()\n",
    "\n",
    "    # Get the upper triangle of the correlation matrix to avoid duplicate pairs\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Keep track of features that have already been grouped\n",
    "    grouped_features = set()\n",
    "\n",
    "    # List to store groups of correlated features\n",
    "    correlated_feature_groups = []\n",
    "\n",
    "    # Iterate through the columns of the upper triangle\n",
    "    for column in upper_triangle.columns:\n",
    "        # Find features that are highly correlated with the current column\n",
    "        correlated_features = upper_triangle.index[upper_triangle[column] > threshold].tolist()\n",
    "        if correlated_features and column not in grouped_features:\n",
    "            # Add the current column to the group\n",
    "            correlated_features.append(column)\n",
    "            # Sort and add the group to the list of groups\n",
    "            correlated_feature_group = sorted(correlated_features)\n",
    "            correlated_feature_groups.append(correlated_feature_group)\n",
    "            # Update the set of grouped features\n",
    "            grouped_features.update(correlated_feature_group)\n",
    "\n",
    "    # Deduplicate the list of groups\n",
    "    unique_correlated_feature_groups = []\n",
    "    for group in correlated_feature_groups:\n",
    "        if group not in unique_correlated_feature_groups:\n",
    "            unique_correlated_feature_groups.append(group)\n",
    "\n",
    "    return unique_correlated_feature_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3.2.2 Location A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correlation_matrix_A = train_data_numerical_A.corr()\n",
    "print(correlation_matrix_A)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix_A, annot=False, fmt=\".1f\", cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix for Dataset A')\n",
    "plt.show()\n",
    "correlated_feature_groups_A = analyze_highly_correlated_features(train_data_numerical_A)\n",
    "correlated_feature_groups_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3.2.3 Location B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correlation_matrix_B = train_data_numerical_B.corr()\n",
    "print(correlation_matrix_B)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix_B, annot=False, fmt=\".1f\", cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix for Dataset A')\n",
    "plt.show()\n",
    "correlated_feature_groups_B = analyze_highly_correlated_features(train_data_numerical_B)\n",
    "correlated_feature_groups_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3.2.4 Location C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correlation_matrix_C = train_data_numerical_C.corr()\n",
    "print(correlation_matrix_C)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix_C, annot=False, fmt=\".1f\", cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix for Dataset A')\n",
    "plt.show()\n",
    "correlated_feature_groups_C = analyze_highly_correlated_features(train_data_numerical_C)\n",
    "correlated_feature_groups_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3.3 Comparing correlated data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare_correlated_feature_groups(groups_A, groups_B, groups_C):\n",
    "    # Converting lists of lists to lists of sets for easier comparison\n",
    "    set_groups_A = [set(group) for group in groups_A]\n",
    "    set_groups_B = [set(group) for group in groups_B]\n",
    "    set_groups_C = [set(group) for group in groups_C]\n",
    "    \n",
    "    # Function to find unique and common correlated feature groups between two sets of groups\n",
    "    def find_differences_and_commons(set_groups_1, set_groups_2):\n",
    "        unique_to_1 = [group for group in set_groups_1 if group not in set_groups_2]\n",
    "        unique_to_2 = [group for group in set_groups_2 if group not in set_groups_1]\n",
    "        common_groups = [group for group in set_groups_1 if group in set_groups_2]\n",
    "        return unique_to_1, unique_to_2, common_groups\n",
    "    \n",
    "    # Finding unique and common correlated feature groups between datasets A, B, and C\n",
    "    unique_A_not_B, unique_B_not_A, common_A_B = find_differences_and_commons(set_groups_A, set_groups_B)\n",
    "    unique_A_not_C, unique_C_not_A, common_A_C = find_differences_and_commons(set_groups_A, set_groups_C)\n",
    "    unique_B_not_C, unique_C_not_B, common_B_C = find_differences_and_commons(set_groups_B, set_groups_C)\n",
    "    \n",
    "    # Calculating similarity scores\n",
    "    similarity_score_A_B = len(common_A_B) / (len(set_groups_A) + len(set_groups_B) - len(common_A_B))\n",
    "    similarity_score_A_C = len(common_A_C) / (len(set_groups_A) + len(set_groups_C) - len(common_A_C))\n",
    "    similarity_score_B_C = len(common_B_C) / (len(set_groups_B) + len(set_groups_C) - len(common_B_C))\n",
    "    \n",
    "    # Preparing the results\n",
    "    results = {\n",
    "        \"A vs B\": {\n",
    "            \"Unique to A\": unique_A_not_B,\n",
    "            \"Unique to B\": unique_B_not_A,\n",
    "            \"Common\": common_A_B,\n",
    "            \"Similarity Score\": similarity_score_A_B\n",
    "        },\n",
    "        \"A vs C\": {\n",
    "            \"Unique to A\": unique_A_not_C,\n",
    "            \"Unique to C\": unique_C_not_A,\n",
    "            \"Common\": common_A_C,\n",
    "            \"Similarity Score\": similarity_score_A_C\n",
    "        },\n",
    "        \"B vs C\": {\n",
    "            \"Unique to B\": unique_B_not_C,\n",
    "            \"Unique to C\": unique_C_not_B,\n",
    "            \"Common\": common_B_C,\n",
    "            \"Similarity Score\": similarity_score_B_C\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "correlated_feature_comparison = compare_correlated_feature_groups(correlated_feature_groups_A, correlated_feature_groups_B, correlated_feature_groups_C)\n",
    "\n",
    "def print_correlated_feature_comparison(results):\n",
    "    for comparison, data in results.items():\n",
    "        dataset_1, dataset_2 = comparison.split(\" vs \")\n",
    "        \n",
    "        print(f\"### {comparison}:\\n\")\n",
    "        \n",
    "        print(f\"Unique to {dataset_1}:\")\n",
    "        for group in data[f\"Unique to {dataset_1}\"]:\n",
    "            print(f\"  - {sorted(list(group))}\")\n",
    "        print()\n",
    "        \n",
    "        print(f\"Unique to {dataset_2}:\")\n",
    "        for group in data[f\"Unique to {dataset_2}\"]:\n",
    "            print(f\"  - {sorted(list(group))}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"Common groups:\")\n",
    "        for group in data[\"Common\"]:\n",
    "            print(f\"  - {sorted(list(group))}\")\n",
    "        print()\n",
    "        \n",
    "        print(f\"Similarity Score: {data['Similarity Score']:.2f}\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_correlated_feature_comparison(correlated_feature_comparison)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3.4 Groups defined by correlation and intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "radiation_and_energy = [\n",
    "    'clear_sky_energy_1h:J',\n",
    "    'clear_sky_rad:W',\n",
    "    'diffuse_rad:W',\n",
    "    'diffuse_rad_1h:J',\n",
    "    'direct_rad:W',\n",
    "    'direct_rad_1h:J'\n",
    "]\n",
    "\n",
    "atmospheric_conditions = [\n",
    "    'absolute_humidity_2m:gm3',\n",
    "    'air_density_2m:kgm3',\n",
    "    'dew_point_2m:K',\n",
    "    'msl_pressure:hPa',\n",
    "    'pressure_100m:hPa',\n",
    "    'pressure_50m:hPa',\n",
    "    'sfc_pressure:hPa',\n",
    "    'relative_humidity_1000hPa:p',\n",
    "    't_1000hPa:K',\n",
    "]\n",
    "\n",
    "time_and_date = [\n",
    "    'is_day:idx',\n",
    "    'is_in_shadow:idx',\n",
    "    'sun_azimuth:d',\n",
    "    'sun_elevation:d'\n",
    "]\n",
    "\n",
    "cloud_cover = [\n",
    "    'effective_cloud_cover:p',\n",
    "    'cloud_base_agl:m',\n",
    "    'total_cloud_cover:p',\n",
    "    'visibility:m',\n",
    "    'ceiling_height_agl:m',\n",
    "\n",
    "]\n",
    "\n",
    "wind_and_air_movement = [\n",
    "    'wind_speed_10m:ms',\n",
    "    'wind_speed_u_10m:ms',\n",
    "    'wind_speed_v_10m:ms',\n",
    "    'wind_speed_w_1000hPa:ms'\n",
    "]\n",
    "\n",
    "snow_features = [\n",
    "    'fresh_snow_12h:cm',\n",
    "    'fresh_snow_1h:cm',\n",
    "    'fresh_snow_24h:cm',\n",
    "    'fresh_snow_3h:cm',\n",
    "    'fresh_snow_6h:cm',\n",
    "    'snow_density:kgm3',\n",
    "    'snow_depth:cm',\n",
    "    'snow_drift:idx',\n",
    "    'snow_melt_10min:mm',\n",
    "    'snow_water:kgm2'\n",
    "]\n",
    "\n",
    "miscellaneous = [\n",
    "    'precip_5min:mm',\n",
    "    'precip_type_5min:idx',\n",
    "    'prob_rime:p',\n",
    "    'rain_water:kgm2',\n",
    "    'super_cooled_liquid_water:kgm2',\n",
    "    'dew_or_rime:idx',\n",
    "    'elevation:m'\n",
    "]\n",
    "\n",
    "feature_groups = {\n",
    "    'radiation_and_energy': radiation_and_energy,\n",
    "    'atmospheric_conditions': atmospheric_conditions,\n",
    "    'time_and_date': time_and_date,\n",
    "    'cloud_cover': cloud_cover,\n",
    "    'wind_and_air_movement': wind_and_air_movement,\n",
    "    'snow_features': snow_features,\n",
    "    'miscellaneous': miscellaneous\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Location A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\nLocation A - Correlation Analysis:\")\n",
    "correlation_analysis(A.original_train_data, radiation_and_energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Location B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\nLocation B - Correlation Analysis:\")\n",
    "correlation_analysis(B.original_train_data, radiation_and_energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:35:52.302233Z",
     "start_time": "2023-11-03T09:35:52.020093Z"
    },
    "collapsed": false
   },
   "source": [
    "### Location C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\nLocation C - Correlation Analysis:\")\n",
    "correlation_analysis(C.original_train_data, radiation_and_energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Correlation Analysis Summary\n",
    "We observe that the weather data are highly correlated with each other, and based on the weather phenomena they represent, clear groupings of the various data emerge. The weather phenomena follow the same correlations across the datasets, and mostly the same groupings of correlated data are formed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.5 Perform analysis on every feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perform_analysis_by_feature_groups(feature_groups, datasets):\n",
    "    for feature_name, features in feature_groups.items():\n",
    "        print(f\"\\nAnalyzing feature group: {feature_name}\")\n",
    "        for dataset_name, data in datasets.items():\n",
    "            print(f\"\\nAnalyzing dataset: {dataset_name}\")\n",
    "            perform_comprehensive_analysis(data, features)\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'Train_data_A': A.original_train_data,\n",
    "    'Train_data_B': B.original_train_data,\n",
    "    'Train_data_C': C.original_train_data\n",
    "}\n",
    "\n",
    "perform_analysis_by_feature_groups(feature_groups, datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.6 Feature Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Functions \n",
    "def prepare_dataframe(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X_copy = X.copy()\n",
    "    if TIME in X_copy.columns:\n",
    "        X_copy = X_copy.drop(columns=[TIME])\n",
    "    return X_copy\n",
    "\n",
    "def analyze_low_variance_features(X: pd.DataFrame, threshold: float = 0.001) -> list:\n",
    "    X = prepare_dataframe(X)\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        raise TypeError(\"X must be a pandas dataframe\")\n",
    "\n",
    "    if not isinstance(threshold, (int, float)):\n",
    "        raise TypeError(\"threshold must be a numeric value\")\n",
    "    X = X.copy()\n",
    "    variances = X.var()\n",
    "    low_variance_features = variances[variances < threshold].index.tolist()\n",
    "\n",
    "    return low_variance_features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_variance_A = analyze_low_variance_features(X = train_data_numerical_A)\n",
    "feature_variance_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_variance_B = analyze_low_variance_features(X = train_data_numerical_B)\n",
    "feature_variance_B"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_variance_C = analyze_low_variance_features(X = train_data_numerical_C)\n",
    "feature_variance_C"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that it is the same features that have low variance across the datasets. This is not unexpected, as the trend indicates that the weather data are very consistent across locations A, B, and C."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_pv_measurement_to_time(data):\n",
    "    \"\"\"\n",
    "    Analyzes the average pv_measurement values, including hourly and monthly averages.\n",
    "    Provides visualizations for patterns. Handles time data in either a column or index.\n",
    "    \"\"\"\n",
    "    # Normalize -0 to 0\n",
    "    data['pv_measurement'] = data['pv_measurement'].replace(-0.0, 0.0)\n",
    "    \n",
    "    # Check if 'time' column exists, else use index\n",
    "    if 'time' in data.columns:\n",
    "        data['hour'] = data['time'].dt.hour\n",
    "        data['month'] = data['time'].dt.month\n",
    "    else:  # Use index if 'time' column is not present\n",
    "        data['hour'] = data.index.hour\n",
    "        data['month'] = data.index.month\n",
    "\n",
    "    # Calculate averages and plot\n",
    "    hourly_avg = data.groupby('hour')['pv_measurement'].mean()\n",
    "    monthly_hourly_avg = data.groupby(['month', 'hour'])['pv_measurement'].mean().unstack()\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    sns.heatmap(monthly_hourly_avg, cmap=\"YlGnBu\", linewidths=.5, annot=True, fmt=\".0f\")\n",
    "    plt.title('Monthly Average PV Measurement by Hour')\n",
    "    plt.xlabel('Hour of the Day')\n",
    "    plt.ylabel('Month')\n",
    "    plt.show()\n",
    "\n",
    "    return hourly_avg, monthly_hourly_avg\n",
    "\n",
    "def analyze_zero_percentage_pv_measurement_to_time(data):\n",
    "    \"\"\"\n",
    "    Analyzes the percentage of zero pv_measurement values by hour and month.\n",
    "    Handles time data in either a column or index.\n",
    "    \"\"\"\n",
    "    # Normalize -0 to 0\n",
    "    data['pv_measurement'] = data['pv_measurement'].replace(-0.0, 0.0)\n",
    "\n",
    "    # Check if 'time' column exists, else use index\n",
    "    if 'time' in data.columns:\n",
    "        data['hour'] = data['time'].dt.hour\n",
    "        data['month'] = data['time'].dt.month\n",
    "    else:  # Use index if 'time' column is not present\n",
    "        data['hour'] = data.index.hour\n",
    "        data['month'] = data.index.month\n",
    "\n",
    "    # Calculate percentages and plot\n",
    "    zero_count = data[data['pv_measurement'] == 0].groupby(['month', 'hour']).size().unstack(fill_value=0)\n",
    "    total_count = data.groupby(['month', 'hour']).size().unstack(fill_value=0)\n",
    "    zero_percentage = (zero_count / total_count) * 100\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    sns.heatmap(zero_percentage, cmap=\"viridis\", linewidths=.5, annot=True, fmt=\".1f\")\n",
    "    plt.title('Percentage of Zero PV Measurement by Hour and Month')\n",
    "    plt.xlabel('Hour of the Day')\n",
    "    plt.ylabel('Month')\n",
    "    plt.show()\n",
    "\n",
    "    return zero_percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.7 Hourly PV per hour each month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Location A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hourly_avg_analysis, monthly_hourly_avg_analysis = analyze_pv_measurement_to_time(A.original_target_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Location B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hourly_avg_analysis, monthly_hourly_avg_analysis = analyze_pv_measurement_to_time(B.original_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Location C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hourly_avg_analysis, monthly_hourly_avg_analysis = analyze_pv_measurement_to_time(C.original_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_percentage = analyze_zero_percentage_pv_measurement_to_time(A.original_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_percentage = analyze_zero_percentage_pv_measurement_to_time(B.original_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_percentage = analyze_zero_percentage_pv_measurement_to_time(C.original_target_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We observe that the average PV measurements follow a natural trend where they are low at night, and the periods of low measurements align with the seasons. They are strongest in the middle of the day during summer and weakest in winter and at night. The percentage of measurements that are zero follows the same trend, but we clearly see that B and C differ from A and what is expected. This is most likely a result of the presence of constant static values. It is also noted that in June, there is a value of 0.8% zero readings throughout the day at location A, which is unexpected but may stem from the same cause."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Afther preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hourly_avg_analysis, monthly_hourly_avg_analysis = analyze_pv_measurement_to_time(A.target_data)\n",
    "zero_percentage = analyze_zero_percentage_pv_measurement_to_time(A.target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hourly_avg_analysis, monthly_hourly_avg_analysis = analyze_pv_measurement_to_time(B.target_data)\n",
    "zero_percentage = analyze_zero_percentage_pv_measurement_to_time(B.target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hourly_avg_analysis, monthly_hourly_avg_analysis = analyze_pv_measurement_to_time(C.target_data)\n",
    "zero_percentage = analyze_zero_percentage_pv_measurement_to_time(C.target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is a significant improvement in the hourly values after preprocessing. There are much fewer unexpected values, and even though the data does not exhibit exactly the desired behavior, there is a significant improvement compared to how one would expect this type of data to appear."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "perform_comprehensive_analysis(A.target_data, ['pv_measurement'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "perform_comprehensive_analysis(B.target_data, ['pv_measurement'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "perform_comprehensive_analysis(C.target_data, ['pv_measurement'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The graphs look odd, but this is a result of having removed rows with constant values, which leads to a time jump that we plot over. However, the statistics over the data are much more as expected and smoother after preprocessing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.6 Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def feature_importance_random_forest(\n",
    "            X: pd.DataFrame,\n",
    "            y: pd.DataFrame,\n",
    "            target_column: PV_MEASUREMENT,\n",
    "            n_estimators: int = 100,\n",
    "            random_state: int = 42\n",
    "    ) -> list:\n",
    "        X = prepare_dataframe(X)\n",
    "        y = y.copy()\n",
    "\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators, random_state=random_state)\n",
    "        model.fit(X, y)\n",
    "\n",
    "        importances = model.feature_importances_\n",
    "\n",
    "        importance_series = pd.Series(\n",
    "            importances, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "        return importance_series\n",
    "\n",
    "\n",
    "def compute_correlation_and_matching(data1: pd.DataFrame, feature1: str,\n",
    "                                         data2: pd.DataFrame, feature2: str,\n",
    "                                         date_column: str) -> tuple:\n",
    "\n",
    "        # Drop rows from data1 and data2 that don't have matching dates in both datasets\n",
    "        common_dates = set(data1[date_column]).intersection(\n",
    "            set(data2[date_column]))\n",
    "        data1_filtered = data1[data1[date_column].isin(common_dates)]\n",
    "        data2_filtered = data2[data2[date_column].isin(common_dates)]\n",
    "\n",
    "        # Compute the correlation\n",
    "        correlation = data1_filtered[feature1].corr(data2_filtered[feature2])\n",
    "\n",
    "        # Compute the mean squared error\n",
    "        mse = mean_squared_error(\n",
    "            data1_filtered[feature1], data2_filtered[feature2])\n",
    "\n",
    "        # Compute max possible error (using range of data1_filtered values as reference)\n",
    "        max_possible_error = np.var(data1_filtered[feature1])\n",
    "\n",
    "        # Compute the matching scale\n",
    "        matching_scale = 1 - (mse / max_possible_error)\n",
    "\n",
    "        return correlation, matching_scale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features_A = feature_importance_random_forest(A.train_data, A.target_data, PV_MEASUREMENT)\n",
    "important_features_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "important_features_B = feature_importance_random_forest(B.train_data, B.target_data, PV_MEASUREMENT)\n",
    "important_features_B"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "important_features_C = feature_importance_random_forest(C.train_data, C.target_data, PV_MEASUREMENT)\n",
    "important_features_C"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By training a random forest model on the data, we can gain an understanding of which features provide the most 'knowledge' value from the data in relation to the target variable PV measurement. It appears that it is mostly the same features, or features of the same category, that have a significant impact on PV measurements. Even though this may not necessarily be the model we will use as a predictor, it serves as a good starting point for identifying which features are important. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Model Selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient Boosting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gradient Boosting Model Overview\n",
    "\n",
    "## Basics of Gradient Boosting Model\n",
    "Gradient Boosting is used for both regression and classification tasks. It builds the model stage by stage, with each stage adding a decision tree that corrects the errors made by the previous trees. \n",
    "\n",
    "## Key Points about Gradient Boosting\n",
    "- **Type:** Supervised learning technique.\n",
    "- **Suitable for:** Regression and Classification.\n",
    "- **Core Concept:** Builds models sequentially, each correcting its predecessor.\n",
    "- **Strength:** Reduces both bias and variance, leading to accurate predictions.\n",
    "- **Overfitting:** Might be prune to overfitting.\n",
    "- **Scalability:** Handles large datasets effectively.\n",
    "\n",
    "## Implementation in Code\n",
    "The `GradientBoostingModel` class encapsulates the Gradient Boosting Regressor. It initializes with  hyperparameters and includes methods for training and prediction. The `GradientBoostingPipeline` extends `IPipeline`, ensuring data validation and preprocessing. It prepares data by dropping NaNs and non-intersecting columns, and handles training and prediction tasks. The pipeline emphasizes proper data alignment and clean-up to feed into the model effectively. The hyperparameter tuning part, using `RandomizedSearchCV`, aims to find the optimal model parameters, thus enhancing model performance.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class IPipeline(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def prepare_data_for_model(\n",
    "            self,\n",
    "            X: pd.DataFrame,\n",
    "            X_test: pd.DataFrame,\n",
    "            y: pd.DataFrame,\n",
    "            y_test: pd.DataFrame = None\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _validate_data(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def run(self, X_train: pd.DataFrame, y_train: pd.DataFrame, X_test: pd.DataFrame) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class GradientBoostingModel:\n",
    "    __model: 'GradientBoostingRegressor'\n",
    "    __predictions: np.ndarray\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.model = GradientBoostingRegressor(\n",
    "            n_estimators=GradientBoostingHyperparameters.N_ESTIMATORS.value,\n",
    "            learning_rate=GradientBoostingHyperparameters.LEARNING_RATE.value,\n",
    "            max_depth=GradientBoostingHyperparameters.MAX_DEPTH.value,\n",
    "            random_state=GradientBoostingHyperparameters.RANDOM_STATE.value,\n",
    "        )\n",
    "\n",
    "        self.predictions = np.array([])\n",
    "\n",
    "    @property\n",
    "    def model(self) -> 'GradientBoostingRegressor':\n",
    "        return self.__model\n",
    "\n",
    "    @model.setter\n",
    "    def model(self, model: 'GradientBoostingRegressor') -> None:\n",
    "        self.__model = model\n",
    "\n",
    "    @property\n",
    "    def predictions(self) -> np.ndarray:\n",
    "        return self.__predictions\n",
    "\n",
    "    @predictions.setter\n",
    "    def predictions(self, predictions: np.ndarray) -> None:\n",
    "        predictions = np.clip(predictions, 0, None)\n",
    "        self.__predictions = predictions\n",
    "\n",
    "    def train(self, X_train: pd.DataFrame, y_train: pd.Series) -> None:\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> predictions:\n",
    "        predictions: np.ndarray = self.model.predict(X)\n",
    "        self.predictions = predictions\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        X_test: pd.DataFrame,\n",
    "        y_test: pd.Series\n",
    "    ) -> tuple[Union[float, np.ndarray], Union[float, np.ndarray]]:\n",
    "        y_pred = self.predict(X_test)\n",
    "        mse: Union[float, np.ndarray] = mean_squared_error(\n",
    "            y_ture=y_test.to_numpy(),\n",
    "            y_pred=y_pred.to_numpy()\n",
    "        )\n",
    "\n",
    "        r2: Union[float, np.ndarray] = r2_score(y_test, y_pred)\n",
    "        return mse, r2\n",
    "\n",
    "    def feature_importances(self) -> np.ndarray:\n",
    "        return self.model.feature_importances_\n",
    "\n",
    "\n",
    "class GradientBoostingPipeline(IPipeline):\n",
    "    def __init__(self) -> None:\n",
    "        self.model = GradientBoostingModel()\n",
    "\n",
    "    def _validate_data(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
    "        assert X.shape[0] == y.shape[0], 'X and y must have the same number of rows'\n",
    "        assert X.shape[1] > 0, 'X must have at least one column'\n",
    "        assert isinstance(y, pd.Series), 'y must be a pandas series'\n",
    "        assert X.isna().sum().sum() == 0, 'X must not have any missing values'\n",
    "\n",
    "    def prepare_data_for_model(\n",
    "            self,\n",
    "            X: pd.DataFrame,\n",
    "            X_test: pd.DataFrame,\n",
    "            y: pd.DataFrame,\n",
    "            y_test: pd.DataFrame = None\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "        # X, y = self.drop_rows_with_nan_in_target_data(X=X, y=y)\n",
    "\n",
    "        if y_test is not None:\n",
    "            y_test: pd.Series = self._drop_target_data_time_column(y=y_test)\n",
    "\n",
    "        if 'date_forecast' in X.columns:\n",
    "            X = X.drop(columns=[TrainFeature.DATE.value])\n",
    "\n",
    "        if 'date_forecast' in X_test.columns:\n",
    "            X_test = X_test.drop(columns=[TrainFeature.DATE.value])\n",
    "\n",
    "        if isinstance(y, pd.DataFrame) and TargetFeature.DATE.value in y.columns:\n",
    "            y = y.drop(columns=[TargetFeature.DATE.value])\n",
    "\n",
    "        X, X_test = PipelineUtils.remove_non_intersecting_columns(\n",
    "            df1=X, df2=X_test\n",
    "        )\n",
    "        y: pd.Series = self._drop_target_data_time_column(y=y)\n",
    "        return X, X_test, y, y_test\n",
    "\n",
    "    def drop_rows_with_nan_in_target_data(self, X: pd.DataFrame, y: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        y = y.dropna()\n",
    "        X = X.loc[y.index]\n",
    "        return X, y\n",
    "\n",
    "    def _drop_target_data_time_column(self, y: pd.DataFrame) -> pd.Series:\n",
    "        if isinstance(y, pd.DataFrame) and TargetFeature.DATE.value in y.columns:\n",
    "            y.drop(columns=TargetFeature.DATE.value, axis=1, inplace=True)\n",
    "\n",
    "        return y.squeeze()\n",
    "\n",
    "    def run(self, train_data: pd.DataFrame, train_target_data: pd.DataFrame, test_data: pd.DataFrame) -> None:\n",
    "        print(\"Running Gradient Boosting Pipeline\")\n",
    "        train_data: pd.DataFrame = train_data.copy()\n",
    "        train_target_data: pd.DataFrame = train_target_data.copy()\n",
    "        test_data: pd.DataFrame = test_data.copy()\n",
    "\n",
    "        train_data, test_data, train_target_data, test_target_data = self.prepare_data_for_model(\n",
    "            X=train_data,\n",
    "            X_test=test_data,\n",
    "            y=train_target_data\n",
    "        )\n",
    "\n",
    "        self._validate_data(X=train_data, y=train_target_data)\n",
    "        print(\"Training model...\")\n",
    "        self.model.train(X_train=train_data, y_train=train_target_data)\n",
    "        print(\"Predicting...\")\n",
    "        self.model.predict(X=test_data)\n",
    "\n",
    "    # TODO: use this to find hyperparameters\n",
    "    def hyperparameter_tuning(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
    "        param_distributions = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 1],\n",
    "            'max_depth': [3, 4, 5],\n",
    "        }\n",
    "\n",
    "        randomized_search = RandomizedSearchCV(\n",
    "            self.model.model,\n",
    "            param_distributions=param_distributions,\n",
    "            n_iter=1,  # You can adjust this\n",
    "            cv=3,\n",
    "            n_jobs=-1,  # Use all CPU core\n",
    "            sscoring=make_scorer(\n",
    "                mean_squared_error,\n",
    "                greater_is_better=False\n",
    "            ),\n",
    "            return_train_score=True,\n",
    "            random_state=42\n",
    "        )  # for reproducibility)\n",
    "\n",
    "        randomized_search.fit(X_train, y_train)\n",
    "\n",
    "        # Update the model with the best hyperparameters\n",
    "        self.model.model = randomized_search.best_estimator_\n",
    "\n",
    "        print(f\"Best parameters: {randomized_search.best_params_}\")\n",
    "        print(f\"Best cross-validation score: {-randomized_search.best_score_}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gradient_boosting: 'GradientBoostingPipeline' = GradientBoostingPipeline()\n",
    "gradient_boosting.run(\n",
    "    X_train=train_data,\n",
    "    y_train=target_data,\n",
    "    X_test=test_data\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Stacking\n",
    "\n",
    "Model Stacking is an technique used for regression and classification tasks. It combines multiple predictive models to improve performance. Using a meta-model to integrate predictions from base models. The meta-model is trained on the predictions of the base models. The base models are trained on the original dataset. The meta-model and base models can be any machine learning model. \n",
    "\n",
    "## Key Points about Model Stacking\n",
    "- **Type:** Supervised learning technique.\n",
    "- **Suitable for:** Regression and Classification.\n",
    "- **Core Concept:** Combines multiple models predictions using a meta-model.\n",
    "- **Strength:** Increases prediction accuracy, leveraging strengths of various models.\n",
    "- **Scalability:** Efficiently handles large datasets and complex problems.\n",
    "\n",
    "## Implementation in Code\n",
    "The code implements model stacking with different models as base learners (Decision Tree, Random Forest, LGBM, CatBoost, XGB, and Gradient Boosting regressors) and LGBMRegressor as the meta-model. Each base model is configured with specific hyperparameters, and the meta-model is trained on the predictions of these base models.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "KAGGLE = True\n",
    "SELF_MADE_SPLIT = False\n",
    "HP_TUNING = False\n",
    "TUNE_HP = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    DecisionTreeRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    LGBMRegressor(),\n",
    "    CatBoostRegressor(),\n",
    "    XGBRegressor(),\n",
    "    GradientBoostingRegressor()\n",
    "]\n",
    "if TUNE_HP:\n",
    "    for location in location_data:\n",
    "        stack = ModelStacker(None, None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining the models\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SEED = 42  # Example SEED value, replace with your actual seed if different\n",
    "\n",
    "base_models = [\n",
    "    DecisionTreeRegressor(\n",
    "        criterion='squared_error',\n",
    "        max_depth=5,\n",
    "        min_samples_split=2,  # Reduced for faster training\n",
    "        min_samples_leaf=1,  # Reduced for faster training\n",
    "        random_state=SEED\n",
    "    ),\n",
    "    RandomForestRegressor(\n",
    "        n_estimators=30,  # Reduced for faster training\n",
    "        max_depth=15,  # Reduced for faster training\n",
    "        n_jobs=-1,\n",
    "        random_state=SEED\n",
    "    ),\n",
    "    LGBMRegressor(\n",
    "        num_leaves=10,\n",
    "        max_depth=5,\n",
    "        verbose=-1,\n",
    "        metric='squared_error',\n",
    "        n_jobs=4,\n",
    "        n_estimators=100,  # Reduced for faster training\n",
    "        colsample_bytree=0.95,\n",
    "        subsample=0.9,\n",
    "        learning_rate=0.01,  # Adjusted for faster training\n",
    "        random_state=SEED\n",
    "    ),\n",
    "    CatBoostRegressor(\n",
    "        n_estimators=500,  # Reduced for faster training\n",
    "        learning_rate=0.1,  # Adjusted for faster training\n",
    "        thread_count=-1,\n",
    "        depth=7,\n",
    "        silent=True,\n",
    "        bagging_temperature=0.2,\n",
    "        random_seed=SEED\n",
    "    ),\n",
    "    XGBRegressor(\n",
    "        eta=0.05,  # Adjusted for faster training\n",
    "        max_depth=10,  # Reduced for faster training\n",
    "        min_child_weight=1,  # Reduced for faster training\n",
    "        n_estimators=200,  # Reduced for faster training\n",
    "        n_jobs=-1,\n",
    "        random_state=SEED\n",
    "    ),\n",
    "    GradientBoostingRegressor(\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=100,  # Reduced for faster training\n",
    "        subsample=0.8,  # Reduced for faster training\n",
    "        criterion='friedman_mse',\n",
    "        min_samples_split=2,  # Reduced for faster training\n",
    "        min_samples_leaf=1,  # Reduced for faster training\n",
    "        max_depth=5,  # Reduced for faster training\n",
    "        random_state=SEED\n",
    "    )\n",
    "]\n",
    "\n",
    "meta_model = LGBMRegressor(\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    random_state=SEED,\n",
    "    verbose=-1,\n",
    "    metric='mse',\n",
    "    n_jobs=4,\n",
    "    n_estimators=100,\n",
    "    colsample_bytree=1,\n",
    "    subsample=1,\n",
    "    learning_rate=0.1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Location A\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if KAGGLE:\n",
    "    train_data = remove_timestamp(A.train_data)\n",
    "    target_data = remove_timestamp(A.target_data)\n",
    "    test_data = remove_timestamp(A.test_data)\n",
    "    test_target_data = None\n",
    "elif SELF_MADE_SPLIT:\n",
    "    train_data = remove_timestamp(A.random_train_data)\n",
    "    target_data = remove_timestamp(A.random_train_target_data)\n",
    "    test_data = remove_timestamp(A.random_test_data_with_gaps)\n",
    "    random_test_target_data = remove_timestamp(A.random_test_target_data_with_gaps)\n",
    "else:\n",
    "    train_data = remove_timestamp(A.sklearn_train_data)\n",
    "    target_data = remove_timestamp(A.sklearn_target_data)\n",
    "    test_data = remove_timestamp(A.sklearn_test_data)\n",
    "    test_target_data = remove_timestamp(A.sklearn_test_target_data)\n",
    "\n",
    "target_data = target_data[TargetFeature.PV_MEASUREMENT.value].to_numpy()\n",
    "\n",
    "A_stack = ModelStacker(\n",
    "    base_models=base_models,\n",
    "    meta_model=meta_model\n",
    ")\n",
    "\n",
    "A_stack.fit(X=train_data, y=target_data)\n",
    "prediction_A = A_stack.predict(test_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Location B"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if KAGGLE:\n",
    "    train_data = remove_timestamp(B.train_data)\n",
    "    target_data = remove_timestamp(B.target_data)\n",
    "    test_data = remove_timestamp(B.test_data)\n",
    "    test_target_data = None\n",
    "elif SELF_MADE_SPLIT:\n",
    "    train_data = remove_timestamp(B.random_train_data)\n",
    "    target_data = remove_timestamp(B.random_train_target_data)\n",
    "    test_data = remove_timestamp(B.random_test_data_with_gaps)\n",
    "    random_test_target_data = remove_timestamp(B.random_test_target_data_with_gaps)\n",
    "else:\n",
    "    train_data = remove_timestamp(B.sklearn_train_data)\n",
    "    target_data = remove_timestamp(B.sklearn_target_data)\n",
    "    test_data = remove_timestamp(B.sklearn_test_data)\n",
    "    test_target_data = remove_timestamp(B.sklearn_test_target_data)\n",
    "\n",
    "target_data = target_data[TargetFeature.PV_MEASUREMENT.value].to_numpy()\n",
    "\n",
    "B_stack = ModelStacker(\n",
    "    base_models=base_models,\n",
    "    meta_model=meta_model\n",
    ")\n",
    "\n",
    "B_stack.fit(X=train_data, y=target_data)\n",
    "prediction_B = B_stack.predict(test_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Location C"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if KAGGLE:\n",
    "    train_data = remove_timestamp(C.train_data)\n",
    "    target_data = remove_timestamp(C.target_data)\n",
    "    test_data = remove_timestamp(C.test_data)\n",
    "    test_target_data = None\n",
    "elif SELF_MADE_SPLIT:\n",
    "    train_data = remove_timestamp(C.random_train_data)\n",
    "    target_data = remove_timestamp(C.random_train_target_data)\n",
    "    test_data = remove_timestamp(C.random_test_data_with_gaps)\n",
    "    random_test_target_data = remove_timestamp(B.random_test_target_data_with_gaps)\n",
    "else:\n",
    "    train_data = remove_timestamp(C.sklearn_train_data)\n",
    "    target_data = remove_timestamp(C.sklearn_target_data)\n",
    "    test_data = remove_timestamp(C.sklearn_test_data)\n",
    "    test_target_data = remove_timestamp(C.sklearn_test_target_data)\n",
    "\n",
    "target_data = target_data[TargetFeature.PV_MEASUREMENT.value].to_numpy()\n",
    "\n",
    "C_stack = ModelStacker(\n",
    "    base_models=base_models,\n",
    "    meta_model=meta_model\n",
    ")\n",
    "\n",
    "C_stack.fit(X=train_data, y=target_data)\n",
    "prediction_C = C_stack.predict(test_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_prediction = pd.read_csv(\"notebooks/best_pred.csv\")[\"prediction\"].values\n",
    "if KAGGLE:\n",
    "    best_pred_A = best_prediction[:720]\n",
    "    best_pred_B = best_prediction[720:1440]\n",
    "    best_pred_C = best_prediction[1440:]\n",
    "    title = 'KAGGLE'\n",
    "    comp_label = 'best prediction'\n",
    "elif SELF_MADE_SPLIT:\n",
    "    best_pred_A = A.random_test_target_data_with_gaps[TargetFeature.PV_MEASUREMENT.value].to_numpy()\n",
    "    best_pred_B = B.random_test_target_data_with_gaps[TargetFeature.PV_MEASUREMENT.value].to_numpy()\n",
    "    best_pred_C = C.random_test_target_data_with_gaps[TargetFeature.PV_MEASUREMENT.value].to_numpy()\n",
    "    title = 'SELF MADE SPLIT'\n",
    "    comp_label = 'random test target data'\n",
    "else:\n",
    "    best_pred_A = A.sklearn_test_target_data[TargetFeature.PV_MEASUREMENT.value].to_numpy()\n",
    "    best_pred_B = B.sklearn_test_target_data[TargetFeature.PV_MEASUREMENT.value].to_numpy()\n",
    "    best_pred_C = C.sklearn_test_target_data[TargetFeature.PV_MEASUREMENT.value].to_numpy()\n",
    "    title = 'SKLEARN SPLIT'\n",
    "    comp_label = 'sklearn test target data'\n",
    "\n",
    "MAE_A = mean_absolute_error(prediction_A, best_pred_A)\n",
    "MAE_B = mean_absolute_error(prediction_B, best_pred_B)\n",
    "MAE_C = mean_absolute_error(prediction_C, best_pred_C)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "line_width = 1.\n",
    "fig, axs = plt.subplots(3, figsize=(30, 20))\n",
    "axs[0].plot(prediction_A, 'r-', label='prediction', linewidth=line_width)\n",
    "axs[0].plot(best_pred_A, 'b-', label=comp_label, linewidth=line_width)\n",
    "axs[0].set_title(f\"[{title}] Prediction vs best prediction - MAE: {MAE_A}\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(prediction_B, 'r-', label='prediction', linewidth=line_width)\n",
    "axs[1].plot(best_pred_B, 'b-', label=comp_label, linewidth=line_width)\n",
    "axs[1].set_title(f\"[{title}] Prediction vs best prediction - MAE: {MAE_B}\")\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].plot(prediction_C, 'r-', label='prediction', linewidth=line_width)\n",
    "axs[2].plot(best_pred_C, 'b-', label=comp_label, linewidth=line_width)\n",
    "axs[2].set_title(f\"[{title}] Prediction vs best prediction - MAE: {MAE_C}\")\n",
    "axs[2].legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FLAML AutoML \n",
    "\n",
    "Fast and Lightweight AutoML (FLAML) is an automated machine learning framework that require minimal computational resources. FLAML automatically determines the best model and hyperparameters for a given dataset.\n",
    "\n",
    "## Key Points about FLAML AutoML\n",
    "- **Purpose:** Automated selection and tuning of machine learning models.\n",
    "- **Efficiency:** Optimized for low computational resource usage.\n",
    "- **Adaptability:** Works well across various types of data and ML tasks.\n",
    "- **User-Friendly:** Requires minimal input from the user to start the model selection process.\n",
    "- **Performance:** Competitively high accuracy with less resource consumption.\n",
    "- **Flexibility:** Compatible with popular machine learning frameworks.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "automl = AutoML()\n",
    "automl_settings = {\n",
    "    \"time_budget\": 12000,  \n",
    "    \"task\": \"regression\",  \n",
    "}\n",
    "\n",
    "train_data_A_for_flaml = train_data_A[test_data_A.columns.intersection(train_data_A.columns)]\n",
    "        \n",
    "train_data_A_for_flaml = train_data_A_for_flaml.copy().drop(columns=['date_forecast'])\n",
    "test_data_A_for_flaml = test_data_A.copy().drop(columns=['date_forecast'])\n",
    "\n",
    "train_data_A_for_flaml.columns = ['feature_' + str(i) for i in range(len(train_data_A_for_flaml.columns))]\n",
    "target_data_A.columns = ['target_' + str(i) for i in range(len(target_data_A.columns))]\n",
    "test_data_A_for_flaml.columns = ['feature_' + str(i) for i in range(len(test_data_A_for_flaml.columns))]\n",
    "\n",
    "test_data_A_for_flaml = test_data_A_for_flaml.drop(columns=['feature_11'])\n",
    "\n",
    "automl.fit(X_train=train_data_A_for_flaml, y_train=target_data_A['target_1'], **automl_settings)\n",
    "new_model = automl.model\n",
    "joblib.dump(new_model, 'new_model_A.pkl')\n",
    "\n",
    "#loaded_model_A = joblib.load('/Users/eirikvarnes/ML/Modeler/FLAML/best_model_A.pkl')\n",
    "predictions_A: np.ndarray = new_model.predict(test_data_A_for_flaml)\n",
    "clipped_predictions_A = np.clip(predictions_A, 0, None)\n",
    "\n",
    "clipped_predictions_A"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_predictions = np.concatenate([clipped_predictions_A, clipped_predictions_B, clipped_predictions_C])\n",
    "combined_predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# H2o AutoML \n",
    "A autoML that is more advanced than FLAML, but also more resource intensive."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def H2O_train(location):\n",
    "    h2o.init()\n",
    "    train_data = location.train_data.copy()\n",
    "    target_data = location.target_data.copy()\n",
    "    \n",
    "    merged_data = pd.merge(left=train_data, right=target_data, left_index=True, right_index=True)\n",
    "    \n",
    "    h2o_data = h2o.H2OFrame(merged_data)\n",
    "    \n",
    "    # Identify predictors and response\n",
    "    x = h2o_data.columns\n",
    "    y = \"pv_measurement\"\n",
    "    x.remove(y)    \n",
    "    \n",
    "    aml_A = H2OAutoML(\n",
    "        max_runtime_secs = 7200, \n",
    "        seed = 42, \n",
    "        sort_metric = \"MAE\"\n",
    "    )\n",
    "    \n",
    "    aml_A.train(x = x, y = y, training_frame = h2o_data)\n",
    "    \n",
    "    h2o.save_model(model=aml_A.leader, path=f\"/Users/eirikvarnes/ML/Modeler/H2O/endgame\", force=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CatBoost \n",
    "\n",
    "CatBoost is a library for gradient boosting on decision trees.\n",
    "\n",
    "## Key Points about CatBoost\n",
    "- **Type:** Supervised learning algorithm.\n",
    "- **Suitable for:** Classification and Regression.\n",
    "- **Core Concept:** Utilizes gradient boosting on decision trees, excelling with categorical data.\n",
    "- **Strength:** Delivers high performance with minimal need for data preprocessing, reduces overfitting.\n",
    "- **Handling Categorical Data:** Effectively processes categorical features without extensive preprocessing.\n",
    "- **Speed and Scalability:** Fast training and prediction, scales well with large datasets.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tuning "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def grid_search_hyper_parameter_tuning(location: LocationData) -> None:\n",
    "    catboost_model = CatBoostRegressor(\n",
    "        iterations=20000,\n",
    "        loss_function='MAE',\n",
    "        od_type='Iter',\n",
    "        od_wait=100,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        'depth': [6, 8, 10],\n",
    "        'learning_rate': [0.1, 0.01, 0.001]\n",
    "    }\n",
    "\n",
    "    mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "    grid_search = GridSearchCV(estimator=catboost_model, param_grid=param_grid, scoring=mae_scorer, cv=3, verbose=1)\n",
    "    X_train_split, X_validation_split, y_train_split, y_validation_split = train_test_split(\n",
    "        location.train_data,\n",
    "        location.target_data,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    grid_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "    print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "    print(f\"Best MAE score from grid search: {-grid_search.best_score_}\")\n",
    "\n",
    "\n",
    "def random_search_hyper_parameter_tuning(location: LocationData) -> None:\n",
    "    catboost_model = CatBoostRegressor(\n",
    "        iterations=20000,\n",
    "        loss_function='MAE',\n",
    "        od_type='Iter',\n",
    "        od_wait=100,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    param_distributions = {\n",
    "        'depth': [6, 8, 10, 12],\n",
    "        'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n",
    "    }\n",
    "\n",
    "    mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=catboost_model,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=1,\n",
    "        scoring=mae_scorer,\n",
    "        cv=3,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    X_train_split, X_validation_split, y_train_split, y_validation_split = train_test_split(\n",
    "        location.train_data,\n",
    "        location.target_data,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    random_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "    print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "    print(f\"Best MAE score from random search: {-random_search.best_score_}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val):\n",
    "    predictions = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, predictions)\n",
    "    return mae"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Without eval"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_catboost(location, params):\n",
    "    N_MODELS = 1\n",
    "    validation_size = 0.2\n",
    "\n",
    "    params['verbose'] = False\n",
    "    params['loss_function'] = 'MAE'\n",
    "    params['eval_metric'] = 'MAE'\n",
    "    params['random_seed'] = 42\n",
    "    params['iterations'] = 20_000\n",
    "    params['per_float_feature_quantization'] = ['20:border_count=1024']\n",
    "    # params['has_time'] = True\n",
    "    # params['bootstrap_type'] = 'Bayesian'\n",
    "    # params['boosting_type'] = 'Plain'\n",
    "    # params['bagging_temperature'] = 3.65\n",
    "\n",
    "    train_data = location.train_data\n",
    "    target_data = location.target_data\n",
    "\n",
    "    # Calculate the index for splitting the data\n",
    "    split_index = int((1 - validation_size) * len(train_data))\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, y_train = train_data.iloc[:split_index, :], target_data.iloc[:split_index]\n",
    "    X_val, y_val = train_data.iloc[split_index:, :], target_data.iloc[split_index:]\n",
    "\n",
    "    models = []\n",
    "    \"\"\"\n",
    "    depth_values = np.random.randint(6, 12, size=N_MODELS)\n",
    "    learning_rate_values = 10 ** -np.random.uniform(1, 2, size=N_MODELS)\n",
    "    learning_rates = params['learning_rate']\n",
    "    depths = params['depth']\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(N_MODELS):\n",
    "        \"\"\"\n",
    "        params['depth'] = depth_values[i]\n",
    "        params['learning_rate'] = learning_rate_values[i]\n",
    "        \"\"\"\n",
    "        model = CatBoostRegressor(**params)\n",
    "\n",
    "        # Fit the model using the validation set for early stopping\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            use_best_model=True,\n",
    "            early_stopping_rounds=100\n",
    "        )\n",
    "\n",
    "        # After training, evaluate the final model on the validation set\n",
    "        final_mae = evaluate_model(model, X_val, y_val)\n",
    "        print(f\"Final MAE for model {i + 1}: {final_mae}\")\n",
    "\n",
    "        # Store the model\n",
    "        models.append(model)\n",
    "\n",
    "    return models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# With eval"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_catboost_ensemble(location, N_MODELS=10):\n",
    "    train_data = location.train_data\n",
    "    target_data = location.target_data\n",
    "\n",
    "    total_length = len(train_data)\n",
    "    val_size = int(0.1 * total_length)\n",
    "\n",
    "    mae_list = []\n",
    "    models = []\n",
    "    depth_values = [7, 7, 8, 8, 8, 8, 9, 9, 9, 10]\n",
    "    learning_rate_values = [0.01, 0.04, 0.01, 0.02, 0.03, 0.04, 0.02, 0.03, 0.04, 0.03]\n",
    "    summer_evaluated = []\n",
    "    train_dates = train_data.index.values\n",
    "\n",
    "    for i in range(N_MODELS):\n",
    "        val_start_idx = int((i / N_MODELS) * (total_length - val_size))\n",
    "\n",
    "        X_train = pd.concat([train_data[:val_start_idx], train_data[val_start_idx + val_size:]], axis=0)\n",
    "        y_train = pd.concat([target_data[:val_start_idx], target_data[val_start_idx + val_size:]], axis=0)\n",
    "        X_val = train_data[val_start_idx:val_start_idx + val_size]\n",
    "        y_val = target_data[val_start_idx:val_start_idx + val_size]\n",
    "\n",
    "        date_at_middle_of_val = pd.to_datetime(X_train.index[val_start_idx + int(val_size / 2)])\n",
    "        evaluation_month = date_at_middle_of_val.month + 7\n",
    "        if evaluation_month > 12:\n",
    "            evaluation_month -= 12\n",
    "\n",
    "        summer_months = [5, 6, 7]\n",
    "        is_summer = evaluation_month in summer_months\n",
    "        summer_evaluated.append(is_summer)\n",
    "\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=20000,\n",
    "            depth=depth_values[i],\n",
    "            learning_rate=learning_rate_values[i],\n",
    "            loss_function='MAE',\n",
    "            od_type='Iter',\n",
    "            od_wait=100,\n",
    "            l2_leaf_reg=3,\n",
    "            random_seed=42,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # print(f\"Model nr: {i + 1}\")\n",
    "        # print(f\"Evaluation month: {evaluation_month}\")\n",
    "        # print(f\"Train data index: {[0, val_start_idx - 1]} and {[val_start_idx + val_size, total_length - 1]}\")\n",
    "        # print(f\"Validation data index {[val_start_idx, val_start_idx + val_size - 1]}\")\n",
    "\n",
    "        # Fit the model using the validation set for early stopping\n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True, early_stopping_rounds=100)\n",
    "\n",
    "        # After training, evaluate the final model on the validation set\n",
    "        final_mae = evaluate_model(model, X_val, y_val)\n",
    "        # print(f\"Final MAE for model {i + 1}: {final_mae}\")\n",
    "        print(f\"Model {i + 1} M[{evaluation_month}]: MAE:\\t{round(final_mae, 4)}\")\n",
    "\n",
    "        mae_list.append(final_mae)\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    print(f\"Average MAE:\\t{round(np.mean(mae_list), 4)}\")\n",
    "\n",
    "    return models, summer_evaluated\n",
    "\n",
    "\n",
    "def get_ensemble_predictions(models, test_data, summer_evaluated):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        predictions = model.predict(test_data)\n",
    "        if summer_evaluated[i]:\n",
    "            all_predictions.extend([\n",
    "                predictions,\n",
    "                predictions,\n",
    "                predictions,\n",
    "                predictions,\n",
    "            ])\n",
    "        else:\n",
    "            all_predictions.append(predictions)\n",
    "\n",
    "    ensemble_predictions = np.mean(all_predictions, axis=0)\n",
    "    return ensemble_predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "def hei() -> None\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
